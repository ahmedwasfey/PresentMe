{"name": "E:\\graduation\\PresentMe\\GraduationProjectPresentation\\wwwroot\\uploads\\33aa5a8a-4139-4e31-afb8-920dc14ee391.pdf", "metadata": {"source": "CRF", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "authors": ["Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova"], "emails": ["jacobdevlin@google.com", "mingweichang@google.com", "kentonl@google.com", "kristout@google.com"], "sections": [{"heading": null, "text": "BERT achieves new state-of-the-art results on eleven natural language processing tasks. it pushes the GLUE score to 80.5% (7.7% point absolute improvement)"}, {"heading": "1 Introduction", "text": "language model pre-training has been shown to be effective for improving many tasks. these include sentence-level tasks such as natural language inference and paraphrasing. we propose BERT: Bidirectional Encoder Representations from Transformers."}, {"heading": "2 Related Work", "text": "in this section, we briefly review the most widely-used pre-training general language representations."}, {"heading": "2.1 Unsupervised Feature-based Approaches", "text": "pre-trained word embeddings are an integral part of modern NLP systems. cloze task can be used to improve the robustness of text generation models."}, {"heading": "2.2 Unsupervised Fine-tuning Approaches", "text": "the first works in this direction only pre-trained word embedding parameters from unlabeled text. the advantage of these approaches is that few parameters need to be learned from scratch. OpenAI GPT achieved previously state-of-the-art results on many sentencelevel tasks from the GLUE benchmark."}, {"heading": "2.3 Transfer Learning from Supervised Data", "text": "computer vision research has demonstrated the importance of transfer learning from large pre-trained models."}, {"heading": "3 BERT", "text": "model architecture is a multi-layer bidirectional Transformer encoder based on the original implementation described in Vaswani et al. (2017)."}, {"heading": "3.1 Pre-training BERT", "text": "the final model achieves 97%-98% accuracy on next sentence prediction (NSP) this task can be trivially generated from any monolingual corpus."}, {"heading": "3.2 Fine-tuning BERT", "text": "the self-attention mechanism in the Transformer allows BERT to model many downstream tasks. for applications involving text pairs, a common pattern is to independently encode text before applying bidirectional cross attention. compared to pre-training, fine-tuning is relatively inexpensive."}, {"heading": "4 Experiments", "text": "in this section, we present BERT fine-tuning results on 11 NLP tasks."}, {"heading": "4.1 GLUE", "text": "Detailed descriptions of GLUE datasets are included in Appendix B. We use a batch size of 32 and fine-tune for 3 epochs over the data. BERTBASE and BERTLARGE outperform all systems on all tasks, obtaining 4.5% and 7.0% respective average accuracy improvement over prior state of the art."}, {"heading": "4.2 SQuAD v1.1", "text": "a single GLUE evaluation server submitted for each of BERTBASE and BERTLARGE. the task is to predict the answer text span in the passage."}, {"heading": "4.3 SQuAD v2.0", "text": "the SQuAD 2.0 task allows for the possibility that no short answer exists in the provided paragraph. we treat questions that do not have an answer as having a span with start and end at the [CLS] token. for prediction, we compare the score of the no-answer span to the best non-null span."}, {"heading": "4.4 SWAG", "text": "the dataset contains 113k sentence-pair completion examples that evaluate grounded commonsense inference."}, {"heading": "5 Ablation Studies", "text": "in this section, we perform ablation experiments over a number of facets of BERT."}, {"heading": "5.1 Effect of Pre-training Tasks", "text": "a bidirectional model is trained using the \u201cmasked LM\u201d (MLM) but without the \"next sentence prediction\" (NSP) task. the left-only constraint was also applied at fine-tuning, because removing it introduced an pre-train/fine tune mismatch that degraded downstream performance."}, {"heading": "5.2 Effect of Model Size", "text": "in this section, we explore the effect of model size on fine-tuning task accuracy. we trained a number of BERT models with differing numbers of layers, hidden units, and attention heads."}, {"heading": "5.3 Feature-based Approach with BERT", "text": "feature-based approach uses fixed features extracted from the pre-trained model. not all tasks can be easily represented by a Transformer encoder architecture, argues dr. robert mcdonald - et al."}, {"heading": "6 Conclusion", "text": "recent empirical improvements due to transfer learning with language models. rich, unsupervised pre-training is an integral part of many language understanding systems."}, {"heading": "Appendix for \u201cBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\u201d", "text": "we present additional ablation studies for BERT including:\u2013 Effect of Number of Training Steps; and \u2013 Ablation for Different Masking Proce-dures."}, {"heading": "A Additional Details for BERT", "text": "compared to standard langauge model training, the masked LM only make predictions on 15% of tokens in each batch."}, {"heading": "B Detailed Experimental Setup", "text": "the GLUE benchmark includes the following datasets. given a pair of sentences, the goal is to predict whether the second sentence is an entailment, contradiction, or neutral with respect to the first one."}, {"heading": "C Additional Ablation Studies", "text": "BERTBASE achieves almost 1.0% additional accuracy on MNLI when trained on 1M steps. in terms of absolute accuracy, the MLM model begins to outperform the LTR model almost immediately."}], "references": [{"title": "Contextual string embeddings for sequence labeling", "author": ["Alan Akbik", "Duncan Blythe", "Roland Vollgraf."], "venue": "Proceedings of the 27th International Conference on Computational Linguistics, pages 1638\u20131649.", "citeRegEx": "Akbik et al\\.,? 2018", "shortCiteRegEx": "Akbik et al\\.", "year": 2018}, {"title": "Character-level language modeling with deeper self-attention", "author": ["Rami Al-Rfou", "Dokook Choe", "Noah Constant", "Mandy Guo", "Llion Jones."], "venue": "arXiv preprint arXiv:1808.04444.", "citeRegEx": "Al.Rfou et al\\.,? 2018", "shortCiteRegEx": "Al.Rfou et al\\.", "year": 2018}, {"title": "A framework for learning predictive structures from multiple tasks and unlabeled data", "author": ["Rie Kubota Ando", "Tong Zhang."], "venue": "Journal of Machine Learning Research, 6(Nov):1817\u20131853.", "citeRegEx": "Ando and Zhang.,? 2005", "shortCiteRegEx": "Ando and Zhang.", "year": 2005}, {"title": "The fifth PASCAL recognizing textual entailment challenge", "author": ["Luisa Bentivogli", "Bernardo Magnini", "Ido Dagan", "Hoa Trang Dang", "Danilo Giampiccolo."], "venue": "TAC. NIST.", "citeRegEx": "Bentivogli et al\\.,? 2009", "shortCiteRegEx": "Bentivogli et al\\.", "year": 2009}, {"title": "Domain adaptation with structural correspondence learning", "author": ["John Blitzer", "Ryan McDonald", "Fernando Pereira."], "venue": "Proceedings of the 2006 conference on empirical methods in natural language processing, pages 120\u2013128. Association for Computa-", "citeRegEx": "Blitzer et al\\.,? 2006", "shortCiteRegEx": "Blitzer et al\\.", "year": 2006}, {"title": "A large annotated corpus for learning natural language inference", "author": ["Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning."], "venue": "EMNLP. Association for Computational Linguistics.", "citeRegEx": "Bowman et al\\.,? 2015", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Class-based n-gram models of natural language", "author": ["Peter F Brown", "Peter V Desouza", "Robert L Mercer", "Vincent J Della Pietra", "Jenifer C Lai."], "venue": "Computational linguistics, 18(4):467\u2013479.", "citeRegEx": "Brown et al\\.,? 1992", "shortCiteRegEx": "Brown et al\\.", "year": 1992}, {"title": "Semeval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation", "author": ["Daniel Cer", "Mona Diab", "Eneko Agirre", "Inigo LopezGazpio", "Lucia Specia."], "venue": "Proceedings of the 11th International Workshop on Semantic", "citeRegEx": "Cer et al\\.,? 2017", "shortCiteRegEx": "Cer et al\\.", "year": 2017}, {"title": "One billion word benchmark for measuring progress in statistical language modeling", "author": ["Ciprian Chelba", "Tomas Mikolov", "Mike Schuster", "Qi Ge", "Thorsten Brants", "Phillipp Koehn", "Tony Robinson."], "venue": "arXiv preprint arXiv:1312.3005.", "citeRegEx": "Chelba et al\\.,? 2013", "shortCiteRegEx": "Chelba et al\\.", "year": 2013}, {"title": "Simple and effective multi-paragraph reading comprehension", "author": ["Christopher Clark", "Matt Gardner."], "venue": "ACL.", "citeRegEx": "Clark and Gardner.,? 2018", "shortCiteRegEx": "Clark and Gardner.", "year": 2018}, {"title": "Semi-supervised sequence modeling with cross-view training", "author": ["Kevin Clark", "Minh-Thang Luong", "Christopher D Manning", "Quoc Le."], "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1914\u2013", "citeRegEx": "Clark et al\\.,? 2018", "shortCiteRegEx": "Clark et al\\.", "year": 2018}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston."], "venue": "Proceedings of the 25th international conference on Machine learning, pages 160\u2013167. ACM.", "citeRegEx": "Collobert and Weston.,? 2008", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "Supervised learning of universal sentence representations from natural language inference data", "author": ["Alexis Conneau", "Douwe Kiela", "Holger Schwenk", "Lo\u0131\u0308c Barrault", "Antoine Bordes"], "venue": "In Proceedings of the 2017 Conference on Empirical Methods in Nat-", "citeRegEx": "Conneau et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Conneau et al\\.", "year": 2017}, {"title": "Semi-supervised sequence learning", "author": ["Andrew M Dai", "Quoc V Le."], "venue": "Advances in neural information processing systems, pages 3079\u20133087.", "citeRegEx": "Dai and Le.,? 2015", "shortCiteRegEx": "Dai and Le.", "year": 2015}, {"title": "ImageNet: A Large-Scale Hierarchical Image Database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. FeiFei."], "venue": "CVPR09.", "citeRegEx": "Deng et al\\.,? 2009", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Automatically constructing a corpus of sentential paraphrases", "author": ["William B Dolan", "Chris Brockett."], "venue": "Proceedings of the Third International Workshop on Paraphrasing (IWP2005).", "citeRegEx": "Dolan and Brockett.,? 2005", "shortCiteRegEx": "Dolan and Brockett.", "year": 2005}, {"title": "Maskgan: Better text generation via filling in the", "author": ["William Fedus", "Ian Goodfellow", "Andrew M Dai."], "venue": "arXiv preprint arXiv:1801.07736.", "citeRegEx": "Fedus et al\\.,? 2018", "shortCiteRegEx": "Fedus et al\\.", "year": 2018}, {"title": "Bridging nonlinearities and stochastic regularizers with gaussian error linear units", "author": ["Dan Hendrycks", "Kevin Gimpel."], "venue": "CoRR, abs/1606.08415.", "citeRegEx": "Hendrycks and Gimpel.,? 2016", "shortCiteRegEx": "Hendrycks and Gimpel.", "year": 2016}, {"title": "Learning distributed representations of sentences from unlabelled data", "author": ["Felix Hill", "Kyunghyun Cho", "Anna Korhonen."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human", "citeRegEx": "Hill et al\\.,? 2016", "shortCiteRegEx": "Hill et al\\.", "year": 2016}, {"title": "Universal language model fine-tuning for text classification", "author": ["Jeremy Howard", "Sebastian Ruder."], "venue": "ACL. Association for Computational Linguistics.", "citeRegEx": "Howard and Ruder.,? 2018", "shortCiteRegEx": "Howard and Ruder.", "year": 2018}, {"title": "Reinforced mnemonic reader for machine reading comprehension", "author": ["Minghao Hu", "Yuxing Peng", "Zhen Huang", "Xipeng Qiu", "Furu Wei", "Ming Zhou."], "venue": "IJCAI.", "citeRegEx": "Hu et al\\.,? 2018", "shortCiteRegEx": "Hu et al\\.", "year": 2018}, {"title": "Discourse-based objectives for fast unsupervised sentence representation learning", "author": ["Yacine Jernite", "Samuel R. Bowman", "David Sontag."], "venue": "CoRR, abs/1705.00557.", "citeRegEx": "Jernite et al\\.,? 2017", "shortCiteRegEx": "Jernite et al\\.", "year": 2017}, {"title": "Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension", "author": ["Mandar Joshi", "Eunsol Choi", "Daniel S Weld", "Luke Zettlemoyer."], "venue": "ACL.", "citeRegEx": "Joshi et al\\.,? 2017", "shortCiteRegEx": "Joshi et al\\.", "year": 2017}, {"title": "Skip-thought vectors", "author": ["Ryan Kiros", "Yukun Zhu", "Ruslan R Salakhutdinov", "Richard Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler."], "venue": "Advances in neural information processing systems, pages 3294\u20133302.", "citeRegEx": "Kiros et al\\.,? 2015", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc Le", "Tomas Mikolov."], "venue": "International Conference on Machine Learning, pages 1188\u20131196.", "citeRegEx": "Le and Mikolov.,? 2014", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "The winograd schema challenge", "author": ["Hector J Levesque", "Ernest Davis", "Leora Morgenstern."], "venue": "Aaai spring symposium: Logical formalizations of commonsense reasoning, volume 46, page 47.", "citeRegEx": "Levesque et al\\.,? 2011", "shortCiteRegEx": "Levesque et al\\.", "year": 2011}, {"title": "An efficient framework for learning sentence representations", "author": ["Lajanugen Logeswaran", "Honglak Lee."], "venue": "International Conference on Learning Representations.", "citeRegEx": "Logeswaran and Lee.,? 2018", "shortCiteRegEx": "Logeswaran and Lee.", "year": 2018}, {"title": "Learned in translation: Contextualized word vectors", "author": ["Bryan McCann", "James Bradbury", "Caiming Xiong", "Richard Socher."], "venue": "NIPS.", "citeRegEx": "McCann et al\\.,? 2017", "shortCiteRegEx": "McCann et al\\.", "year": 2017}, {"title": "context2vec: Learning generic context embedding with bidirectional LSTM", "author": ["Oren Melamud", "Jacob Goldberger", "Ido Dagan."], "venue": "CoNLL.", "citeRegEx": "Melamud et al\\.,? 2016", "shortCiteRegEx": "Melamud et al\\.", "year": 2016}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in Neural Information Processing Systems 26, pages 3111\u20133119. Curran Associates,", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A scalable hierarchical distributed language model", "author": ["Andriy Mnih", "Geoffrey E Hinton."], "venue": "D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing Systems 21, pages 1081\u20131088. Curran As-", "citeRegEx": "Mnih and Hinton.,? 2009", "shortCiteRegEx": "Mnih and Hinton.", "year": 2009}, {"title": "A decomposable attention model for natural language inference", "author": ["Ankur P Parikh", "Oscar T\u00e4ckstr\u00f6m", "Dipanjan Das", "Jakob Uszkoreit."], "venue": "EMNLP.", "citeRegEx": "Parikh et al\\.,? 2016", "shortCiteRegEx": "Parikh et al\\.", "year": 2016}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "Empirical Methods in Natural Language Processing (EMNLP), pages 1532\u2013 1543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Semi-supervised sequence tagging with bidirectional language models", "author": ["Matthew Peters", "Waleed Ammar", "Chandra Bhagavatula", "Russell Power."], "venue": "ACL.", "citeRegEx": "Peters et al\\.,? 2017", "shortCiteRegEx": "Peters et al\\.", "year": 2017}, {"title": "Deep contextualized word representations", "author": ["Matthew Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer."], "venue": "NAACL.", "citeRegEx": "Peters et al\\.,? 2018a", "shortCiteRegEx": "Peters et al\\.", "year": 2018}, {"title": "Dissecting contextual word embeddings: Architecture and representation", "author": ["Matthew Peters", "Mark Neumann", "Luke Zettlemoyer", "Wen-tau Yih."], "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages", "citeRegEx": "Peters et al\\.,? 2018b", "shortCiteRegEx": "Peters et al\\.", "year": 2018}, {"title": "Improving language understanding with unsupervised learning", "author": ["Alec Radford", "Karthik Narasimhan", "Tim Salimans", "Ilya Sutskever."], "venue": "Technical report, OpenAI.", "citeRegEx": "Radford et al\\.,? 2018", "shortCiteRegEx": "Radford et al\\.", "year": 2018}, {"title": "Squad: 100,000+ questions for machine comprehension of text", "author": ["Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383\u20132392.", "citeRegEx": "Rajpurkar et al\\.,? 2016", "shortCiteRegEx": "Rajpurkar et al\\.", "year": 2016}, {"title": "Bidirectional attention flow for machine comprehension", "author": ["Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi."], "venue": "ICLR.", "citeRegEx": "Seo et al\\.,? 2017", "shortCiteRegEx": "Seo et al\\.", "year": 2017}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D Manning", "Andrew Ng", "Christopher Potts."], "venue": "Proceedings of the 2013 conference on", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "U-net: Machine reading comprehension with unanswerable questions", "author": ["Fu Sun", "Linyang Li", "Xipeng Qiu", "Yang Liu."], "venue": "arXiv preprint arXiv:1810.06638.", "citeRegEx": "Sun et al\\.,? 2018", "shortCiteRegEx": "Sun et al\\.", "year": 2018}, {"title": "Cloze procedure: A new tool for measuring readability", "author": ["Wilson L Taylor."], "venue": "Journalism Bulletin, 30(4):415\u2013433.", "citeRegEx": "Taylor.,? 1953", "shortCiteRegEx": "Taylor.", "year": 1953}, {"title": "Introduction to the conll-2003 shared task: Language-independent named entity recognition", "author": ["Erik F Tjong Kim Sang", "Fien De Meulder."], "venue": "CoNLL.", "citeRegEx": "Sang and Meulder.,? 2003", "shortCiteRegEx": "Sang and Meulder.", "year": 2003}, {"title": "Word representations: A simple and general method for semi-supervised learning", "author": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio."], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL \u201910, pages 384\u2013394.", "citeRegEx": "Turian et al\\.,? 2010", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Attention is all you need", "author": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Lukasz Kaiser", "Illia Polosukhin."], "venue": "Advances in Neural Information Processing Systems, pages 6000\u20136010.", "citeRegEx": "Vaswani et al\\.,? 2017", "shortCiteRegEx": "Vaswani et al\\.", "year": 2017}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Pascal Vincent", "Hugo Larochelle", "Yoshua Bengio", "Pierre-Antoine Manzagol."], "venue": "Proceedings of the 25th international conference on Machine learning, pages 1096\u20131103.", "citeRegEx": "Vincent et al\\.,? 2008", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}, {"title": "2018a. Glue: A multi-task benchmark and analysis platform", "author": ["Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel Bowman"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2018\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2018}, {"title": "Multigranularity hierarchical attention fusion networks for reading comprehension and question answering", "author": ["Wei Wang", "Ming Yan", "Chen Wu."], "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1:", "citeRegEx": "Wang et al\\.,? 2018b", "shortCiteRegEx": "Wang et al\\.", "year": 2018}, {"title": "Neural network acceptability judgments", "author": ["Alex Warstadt", "Amanpreet Singh", "Samuel R Bowman."], "venue": "arXiv preprint arXiv:1805.12471.", "citeRegEx": "Warstadt et al\\.,? 2018", "shortCiteRegEx": "Warstadt et al\\.", "year": 2018}, {"title": "A broad-coverage challenge corpus for sentence understanding through inference", "author": ["Adina Williams", "Nikita Nangia", "Samuel R Bowman."], "venue": "NAACL.", "citeRegEx": "Williams et al\\.,? 2018", "shortCiteRegEx": "Williams et al\\.", "year": 2018}, {"title": "How transferable are features in deep neural networks", "author": ["Jason Yosinski", "Jeff Clune", "Yoshua Bengio", "Hod Lipson"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Yosinski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yosinski et al\\.", "year": 2014}, {"title": "QANet: Combining local convolution with global self-attention for reading comprehension", "author": ["Adams Wei Yu", "David Dohan", "Minh-Thang Luong", "Rui Zhao", "Kai Chen", "Mohammad Norouzi", "Quoc V Le."], "venue": "ICLR.", "citeRegEx": "Yu et al\\.,? 2018", "shortCiteRegEx": "Yu et al\\.", "year": 2018}, {"title": "Swag: A large-scale adversarial dataset for grounded commonsense inference", "author": ["Rowan Zellers", "Yonatan Bisk", "Roy Schwartz", "Yejin Choi."], "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Zellers et al\\.,? 2018", "shortCiteRegEx": "Zellers et al\\.", "year": 2018}, {"title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books", "author": ["Yukun Zhu", "Ryan Kiros", "Rich Zemel", "Ruslan Salakhutdinov", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler."], "venue": "Proceedings of the IEEE", "citeRegEx": "Zhu et al\\.,? 2015", "shortCiteRegEx": "Zhu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 34, "context": "Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.", "startOffset": 45, "endOffset": 89}, {"referenceID": 36, "context": "Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.", "startOffset": 45, "endOffset": 89}, {"referenceID": 13, "context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks (Dai and Le, 2015; Peters et al., 2018a; Radford et al., 2018; Howard and Ruder, 2018).", "startOffset": 112, "endOffset": 198}, {"referenceID": 34, "context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks (Dai and Le, 2015; Peters et al., 2018a; Radford et al., 2018; Howard and Ruder, 2018).", "startOffset": 112, "endOffset": 198}, {"referenceID": 36, "context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks (Dai and Le, 2015; Peters et al., 2018a; Radford et al., 2018; Howard and Ruder, 2018).", "startOffset": 112, "endOffset": 198}, {"referenceID": 19, "context": "Language model pre-training has been shown to be effective for improving many natural language processing tasks (Dai and Le, 2015; Peters et al., 2018a; Radford et al., 2018; Howard and Ruder, 2018).", "startOffset": 112, "endOffset": 198}, {"referenceID": 5, "context": "These include sentence-level tasks such as natural language inference (Bowman et al., 2015; Williams et al., 2018) and paraphrasing (Dolan and Brockett, 2005), which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level (Tjong Kim Sang and De Meulder, 2003; Rajpurkar et al.", "startOffset": 70, "endOffset": 114}, {"referenceID": 49, "context": "These include sentence-level tasks such as natural language inference (Bowman et al., 2015; Williams et al., 2018) and paraphrasing (Dolan and Brockett, 2005), which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level (Tjong Kim Sang and De Meulder, 2003; Rajpurkar et al.", "startOffset": 70, "endOffset": 114}, {"referenceID": 15, "context": ", 2018) and paraphrasing (Dolan and Brockett, 2005), which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level (Tjong Kim Sang and De Meulder, 2003; Rajpurkar et al.", "startOffset": 25, "endOffset": 51}, {"referenceID": 37, "context": ", 2018) and paraphrasing (Dolan and Brockett, 2005), which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level (Tjong Kim Sang and De Meulder, 2003; Rajpurkar et al., 2016).", "startOffset": 304, "endOffset": 365}, {"referenceID": 34, "context": "The feature-based approach, such as ELMo (Peters et al., 2018a), uses task-specific architectures that include the pre-trained representations as additional features.", "startOffset": 41, "endOffset": 63}, {"referenceID": 36, "context": "The fine-tuning approach, such as the Generative Pre-trained Transformer (OpenAI GPT) (Radford et al., 2018), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.", "startOffset": 86, "endOffset": 108}, {"referenceID": 44, "context": "For example, in OpenAI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer (Vaswani et al., 2017).", "startOffset": 174, "endOffset": 196}, {"referenceID": 41, "context": "BERT alleviates the previously mentioned unidirectionality constraint by using a \u201cmasked language model\u201d (MLM) pre-training objective, inspired by the Cloze task (Taylor, 1953).", "startOffset": 162, "endOffset": 176}, {"referenceID": 6, "context": "Learning widely applicable representations of words has been an active area of research for decades, including non-neural (Brown et al., 1992; Ando and Zhang, 2005; Blitzer et al., 2006) and neural (Mikolov et al.", "startOffset": 122, "endOffset": 186}, {"referenceID": 2, "context": "Learning widely applicable representations of words has been an active area of research for decades, including non-neural (Brown et al., 1992; Ando and Zhang, 2005; Blitzer et al., 2006) and neural (Mikolov et al.", "startOffset": 122, "endOffset": 186}, {"referenceID": 4, "context": "Learning widely applicable representations of words has been an active area of research for decades, including non-neural (Brown et al., 1992; Ando and Zhang, 2005; Blitzer et al., 2006) and neural (Mikolov et al.", "startOffset": 122, "endOffset": 186}, {"referenceID": 43, "context": "Pre-trained word embeddings are an integral part of modern NLP systems, offering significant improvements over embeddings learned from scratch (Turian et al., 2010).", "startOffset": 143, "endOffset": 164}, {"referenceID": 30, "context": "To pretrain word embedding vectors, left-to-right language modeling objectives have been used (Mnih and Hinton, 2009), as well as objectives to discriminate correct from incorrect words in left and right context (Mikolov et al.", "startOffset": 94, "endOffset": 117}, {"referenceID": 29, "context": "To pretrain word embedding vectors, left-to-right language modeling objectives have been used (Mnih and Hinton, 2009), as well as objectives to discriminate correct from incorrect words in left and right context (Mikolov et al., 2013).", "startOffset": 212, "endOffset": 234}, {"referenceID": 23, "context": "These approaches have been generalized to coarser granularities, such as sentence embeddings (Kiros et al., 2015; Logeswaran and Lee, 2018) or paragraph embeddings (Le and Mikolov, 2014).", "startOffset": 93, "endOffset": 139}, {"referenceID": 26, "context": "These approaches have been generalized to coarser granularities, such as sentence embeddings (Kiros et al., 2015; Logeswaran and Lee, 2018) or paragraph embeddings (Le and Mikolov, 2014).", "startOffset": 93, "endOffset": 139}, {"referenceID": 24, "context": ", 2015; Logeswaran and Lee, 2018) or paragraph embeddings (Le and Mikolov, 2014).", "startOffset": 58, "endOffset": 80}, {"referenceID": 21, "context": "To train sentence representations, prior work has used objectives to rank candidate next sentences (Jernite et al., 2017; Logeswaran and Lee, 2018), left-to-right generation of next sentence words given a representation of the previous sentence (Kiros et al.", "startOffset": 99, "endOffset": 147}, {"referenceID": 26, "context": "To train sentence representations, prior work has used objectives to rank candidate next sentences (Jernite et al., 2017; Logeswaran and Lee, 2018), left-to-right generation of next sentence words given a representation of the previous sentence (Kiros et al.", "startOffset": 99, "endOffset": 147}, {"referenceID": 23, "context": ", 2017; Logeswaran and Lee, 2018), left-to-right generation of next sentence words given a representation of the previous sentence (Kiros et al., 2015), or denoising autoencoder derived objectives (Hill et al.", "startOffset": 131, "endOffset": 151}, {"referenceID": 18, "context": ", 2015), or denoising autoencoder derived objectives (Hill et al., 2016).", "startOffset": 53, "endOffset": 72}, {"referenceID": 34, "context": "When integrating contextual word embeddings with existing task-specific architectures, ELMo advances the state of the art for several major NLP benchmarks (Peters et al., 2018a) including question answering (Rajpurkar et al.", "startOffset": 155, "endOffset": 177}, {"referenceID": 37, "context": ", 2018a) including question answering (Rajpurkar et al., 2016), sentiment analysis (Socher et al.", "startOffset": 38, "endOffset": 62}, {"referenceID": 39, "context": ", 2016), sentiment analysis (Socher et al., 2013), and named entity recognition (Tjong Kim Sang and De Meulder, 2003).", "startOffset": 28, "endOffset": 49}, {"referenceID": 11, "context": "As with the feature-based approaches, the first works in this direction only pre-trained word embedding parameters from unlabeled text (Collobert and Weston, 2008).", "startOffset": 135, "endOffset": 163}, {"referenceID": 13, "context": "More recently, sentence or document encoders which produce contextual token representations have been pre-trained from unlabeled text and fine-tuned for a supervised downstream task (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018).", "startOffset": 182, "endOffset": 246}, {"referenceID": 19, "context": "More recently, sentence or document encoders which produce contextual token representations have been pre-trained from unlabeled text and fine-tuned for a supervised downstream task (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018).", "startOffset": 182, "endOffset": 246}, {"referenceID": 36, "context": "More recently, sentence or document encoders which produce contextual token representations have been pre-trained from unlabeled text and fine-tuned for a supervised downstream task (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018).", "startOffset": 182, "endOffset": 246}, {"referenceID": 36, "context": "At least partly due to this advantage, OpenAI GPT (Radford et al., 2018) achieved previously state-of-the-art results on many sentencelevel tasks from the GLUE benchmark (Wang et al.", "startOffset": 50, "endOffset": 72}, {"referenceID": 19, "context": "ing and auto-encoder objectives have been used for pre-training such models (Howard and Ruder, 2018; Radford et al., 2018; Dai and Le, 2015).", "startOffset": 76, "endOffset": 140}, {"referenceID": 36, "context": "ing and auto-encoder objectives have been used for pre-training such models (Howard and Ruder, 2018; Radford et al., 2018; Dai and Le, 2015).", "startOffset": 76, "endOffset": 140}, {"referenceID": 13, "context": "ing and auto-encoder objectives have been used for pre-training such models (Howard and Ruder, 2018; Radford et al., 2018; Dai and Le, 2015).", "startOffset": 76, "endOffset": 140}, {"referenceID": 12, "context": "There has also been work showing effective transfer from supervised tasks with large datasets, such as natural language inference (Conneau et al., 2017) and machine translation (McCann et al.", "startOffset": 130, "endOffset": 152}, {"referenceID": 14, "context": "Computer vision research has also demonstrated the importance of transfer learning from large pre-trained models, where an effective recipe is to fine-tune models pre-trained with ImageNet (Deng et al., 2009; Yosinski et al., 2014).", "startOffset": 189, "endOffset": 231}, {"referenceID": 50, "context": "Computer vision research has also demonstrated the importance of transfer learning from large pre-trained models, where an effective recipe is to fine-tune models pre-trained with ImageNet (Deng et al., 2009; Yosinski et al., 2014).", "startOffset": 189, "endOffset": 231}, {"referenceID": 41, "context": "We refer to this procedure as a \u201cmasked LM\u201d (MLM), although it is often referred to as a Cloze task in the literature (Taylor, 1953).", "startOffset": 118, "endOffset": 132}, {"referenceID": 45, "context": "In contrast to denoising auto-encoders (Vincent et al., 2008), we only predict the masked words rather than reconstructing the entire input.", "startOffset": 39, "endOffset": 61}, {"referenceID": 53, "context": "For the pre-training corpus we use the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words).", "startOffset": 64, "endOffset": 82}, {"referenceID": 8, "context": "It is critical to use a document-level corpus rather than a shuffled sentence-level corpus such as the Billion Word Benchmark (Chelba et al., 2013) in order to extract long contiguous sequences.", "startOffset": 126, "endOffset": 147}, {"referenceID": 37, "context": "1) is a collection of 100k crowdsourced question/answer pairs (Rajpurkar et al., 2016).", "startOffset": 62, "endOffset": 86}, {"referenceID": 38, "context": "Table 2 shows top leaderboard entries as well as results from top published systems (Seo et al., 2017; Clark and Gardner, 2018; Peters et al., 2018a; Hu et al., 2018).", "startOffset": 84, "endOffset": 166}, {"referenceID": 9, "context": "Table 2 shows top leaderboard entries as well as results from top published systems (Seo et al., 2017; Clark and Gardner, 2018; Peters et al., 2018a; Hu et al., 2018).", "startOffset": 84, "endOffset": 166}, {"referenceID": 34, "context": "Table 2 shows top leaderboard entries as well as results from top published systems (Seo et al., 2017; Clark and Gardner, 2018; Peters et al., 2018a; Hu et al., 2018).", "startOffset": 84, "endOffset": 166}, {"referenceID": 20, "context": "Table 2 shows top leaderboard entries as well as results from top published systems (Seo et al., 2017; Clark and Gardner, 2018; Peters et al., 2018a; Hu et al., 2018).", "startOffset": 84, "endOffset": 166}, {"referenceID": 22, "context": "We therefore use modest data augmentation in our system by first fine-tuning on TriviaQA (Joshi et al., 2017) befor fine-tuning on SQuAD.", "startOffset": 89, "endOffset": 109}, {"referenceID": 40, "context": "The results compared to prior leaderboard entries and top published work (Sun et al., 2018; Wang et al., 2018b) are shown in Table 3, excluding systems that use BERT as one of their components.", "startOffset": 73, "endOffset": 111}, {"referenceID": 47, "context": "The results compared to prior leaderboard entries and top published work (Sun et al., 2018; Wang et al., 2018b) are shown in Table 3, excluding systems that use BERT as one of their components.", "startOffset": 73, "endOffset": 111}, {"referenceID": 52, "context": "The Situations With Adversarial Generations (SWAG) dataset contains 113k sentence-pair completion examples that evaluate grounded commonsense inference (Zellers et al., 2018).", "startOffset": 152, "endOffset": 174}, {"referenceID": 1, "context": "(2017) is (L=6, H=1024, A=16) with 100M parameters for the encoder, and the largest Transformer we have found in the literature is (L=64, H=512, A=2) with 235M parameters (Al-Rfou et al., 2018).", "startOffset": 171, "endOffset": 193}], "year": 2019, "abstractText": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).", "creator": "LaTeX with hyperref package"}}