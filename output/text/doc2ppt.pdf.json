{
  "name" : "doc2ppt.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "DOC2PPT: Automatic Presentation Slides Generation from Scientific Documents",
    "authors" : [ "Tsu-Jui Fu", "William Yang Wang", "Daniel McDuff", "Yale Song" ],
    "emails" : [ "tsu-juifu@cs.ucsb.edu", "william@cs.ucsb.edu", "damcduff@microsoft.com", "yalesong@microsoft.com" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Creating presentations is often a work of art. It requires skills to abstract complex concepts and conveys them in a concise and visually pleasing manner. Consider the steps involved in creating presentation slides based on a white paper or manuscript: One needs to 1) establish a storyline that will connect with the audience, 2) identify essential sections and components that support the main message, 3) delineate the structure of that content, e.g., the ordering/length of the sections, 4) summarize the content in a concise form, e.g., punchy bullet points, 5) gather figures that help communicate the message accurately and engagingly, and 6) arrange these elements (e.g., text, figures, and graphs) in a logical and aesthetically pleasing manner on each slide.\nCan machines emulate this laborious process by learning from the plethora of example manuscripts and slide decks created by human experts? We argue that this is an\nProject webpage will be released.\narea where AI can enhance humans’ productivity, e.g., by drafting slides for humans to build upon. This would open up new opportunities to human-AI collaboration, e.g., one could quickly generate a slide deck by revising the draft or simply generate slide decks of many papers and skim them through to digest a lot of material quickly.\nHowever, building such a system poses unique challenges in vision and language understanding. Both the input (a manuscript) and output (a slide deck) contain tightly coupled visual and textual elements; thus, it requires multimodal reasoning. Further, there are significant differences in the presentation: compared to manuscripts, slides tend to be more concise (e.g., containing bullet points rather than full sentences), structured (e.g., each slide has a fixed screen real estate and delivers one or few messages), and visualcentric (e.g., figures are first-class citizens, the visual layout plays an important role, etc.).\nExisting literature only partially addresses some of the\n1\nar X\niv :2\n10 1.\n11 79\n6v 2\n[ cs\n.C V\n] 1\n4 Fe\nb 20\n21\nchallenges above. Document summarization [13, 16] aims to find a concise text summary of the input, but it does not deal with images/figures and lacks multimodal understanding. Cross-modal retrieval [26, 38] focuses on finding a multimodal embedding space but does not produce summarized outputs. Multimodal summarization [68] deals with both (summarizing documents with text and figures), but it lacks the ability to produce structured output (as in slides). Furthermore, none of the above addresses the challenge of finding an optimal visual layout of each slide. While assessing visual aesthetics have been investigated [49], exiting work focuses on photographic metrics for images that would not translate to slides. These aspects make ours a unique task in the vision-and-language literature.\nIn this paper, we introduce DOC2PPT, a novel task of creating presentation slides from documents. As this is a new task with no existing benchmark, we release code, and a new dataset of 5,873 paired scientific documents and associated presentation slide decks (for a total of about 70K pages and 100K slides, respectively). We present a series of automatic data processing steps to extract useful learning signals from documents and slides. We also introduce new quantitative metrics designed to measure the quality of the generated slides.\nTo tackle this task, we present a hierarchical recurrent sequence-to-sequence architecture that “reads” the input document and “summarizes” it into a structured slide deck. We exploit the inherent structure within documents and slides by performing inference at the section-level (for documents) and at the slide-level (for slides). To make our model end-to-end trainable, we explicitly encode section/slide embeddings and use them to learn a policy that determines when to proceed to the next section/slide. Further, we learn the policy in a hierarchical manner so that the network decides which actions to take by considering the structural context, e.g., a decision to create a new slide will depend on the section the model is currently summarizing and the previous slides that it has generated thus far.\nTo account for the concise nature of text in slides (e.g., bullet points), we incorporate a paraphrasing module that converts document-style full sentences to slide-style phrases/clauses. We show that this module drastically improves the quality of the generated textual content for the slides. In addition, we introduce a text-image matching objective that encourages related text-image pairs to appear on the same slide. We demonstrate that this objective substantially improves figure placement in slides. Lastly, we explore both template-based and learning-based solutions for slide layout design and compare them both quantitatively and qualitatively.\nTo summarize, our main contributions include: 1) Introducing a novel task, dataset, and evaluation metrics for automatic slide generation; 2) Proposing a hierarchi-\ncal sequence-to-sequence approach that summarizes a document in a structure output format suitable for slide presentation; 3) Evaluating our approach both quantitatively, using our proposed metrics, and qualitatively based on human evaluation. The task of generating presentation slides presents numerous challenges. We hope that our work will enable researchers to advance the state-of-the-art in the vision-and-language domain."
    }, {
      "heading" : "2. Related Work",
      "text" : "Vision and Language. Joint modeling of vision-andlanguage has been studied from different angles. Image/video captioning [59, 66, 40, 62], visual question answering [36, 5, 6], visually-grounded dialogue generation [20] and visual navigation [60] are all tasks that involve learning relationships between visual imagery and text. Despite this large body of work, there remain many vision and language tasks that have not been addressed, e.g., multimodal document generation such as ours. As argued above, our task brings a new suite of challenges to vision-andlanguage understanding.\nDocument Summarization. This task has been tackled from two angles: abstractive [16, 55, 14, 45, 22, 67, 9, 54, 43, 52] and extractive [8, 50, 44, 12, 65, 13, 64]. Our DOC2PPT task involves both abstractive and extractive summarization because it requires a model to extract the key content from a document and paraphrase it into a concise form. A task closely related to ours is scientific document summarization [23, 47, 34, 51], but to date that work has only focused on producing text summaries, while we focus on generating multimedia slides. Furthermore, existing datasets in this domain (such as TalkSumm [39] and ScisummNet [63]) are rather small with only about 1K documents each. We propose a large dataset of 5,873 pairs of high-quality scientific documents and slide decks.\nVisual-Semantic Embedding Our task involves generating slides with relevant text and figures. Learning textimage similarity has been studied in the visual-semantic embedding (VSE) literature [38, 37, 58, 25, 32, 27, 56]. However, unlike the VSE setting where text instances are known in advance, ours requires simultaneously generating text and retrieving the related images at the same time.\nMultimodal Summarization This task aims to summarize a document with text and figures into a summary that also contains text and figures. MultiModal Summarization with Multimodal Output (MSMO) [68, 69] applies an attention mechanism to generate a textual summary with related images for news articles. Similarly, our task involves summarizing multimodal documents, but it also involves putting the summary in a structured format such as slides."
    }, {
      "heading" : "3. Approach",
      "text" : "The goal of DOC2PPT is to generate a slide deck from a multimodal document with text and figures.1 As shown in Fig. 1, the task involves “reading” a document (i.e., encoding sentences and images) and summarizing it, paraphrasing the summarized sentences into a concise format suitable for slide presentation, and placing the chosen text and figures to appropriate locations in the output slides.\nOverview Given the multi-objective nature of the task, we design our network with modularized components that are jointly trained in an end-to-end fashion. Fig. 2 shows an overview of our network that includes these modules:\n• A Document Reader (DR) encodes sentences and figures in a document. • A Progress Tracker (PT) maintains pointers to the input (i.e., which section is currently being processed) and the output (i.e., which slide is currently being generated) and determines when to proceed to the next section/slide based on the progress so far. • An Object Placer (OP) decides which object from the current section (sentence or figure) to put on the current slide. It also predicts the location and the size of each object to be placed on the slide. • A Paraphraser (PAR) takes the selected sentence and rewrites it in a concise form before putting it on a slide.\n1In this work, figures include images, graphs, charts, and tables.\nNotation A document D is organized into sections S = {Si}i∈NinS and figures F = {F in q }q∈MinF . Each section Si contains sentences T ini = {T ini,k}k∈Nini , and each figure Fq = {Iq, Cq} contains an image Iq and a caption Cq . We do not assign figures to any particular section because multiple sections can reference the same figure. A slide deck O = {Oj}j∈NoutO contains a number of slides, each containing sentences T outj = {T outj,k }k∈Noutj and figures Foutj = {F outj,k }k∈Moutj . We encode the position and the size of each object on a slide in a bounding box format using an auxiliary layout variable Lj,k, which includes four real-valued numbers {lx, ly, lw, lh} encoding the x-y offsets (top-left corner), the width and height of a bounding box."
    }, {
      "heading" : "3.1. Model",
      "text" : "Document Reader (DR) We extract sentence and figure embeddings from an input document and project them to a shared embedding space so that the Object Placer treats both textual and visual elements as an object coming from a joint multimodal distribution.\nFor each section Si, we use RoBERTa [46] to encode each of the sentences T ini,k, and then use a bidirectional GRU [17] to extract contextualized sentence embeddings Xini,k:\nBini,k = RoBERTa(T in i,k),\nXini,k = Bi-GRU(B in i,0, ..., B in i,Nini −1 ) k ,\n(1)\nSimilarly, for each figure F inq = {Iinq , Cinq }, we apply ResNet-152 [30] to extract the image embedding of Iinq and RoBERTa for the caption embedding of Cinq . We then concatenate them as the figure embedding V inq :\nV inq = [ResNet(F in q ),RoBERTa(C in q )]. (2)\nNext, we project Xini,k and V in q to a shared embedding\nspace using a two-layer multilayer perceptron (MLP):\nEtxti,k = MLP txt(Xini,k), E fig q = MLP fig(V inq ). (3)\nFinally, we combine Etxti and E fig as the section em-\nbedding Eseci of Si:\nEseci = {Etxti,k , Efigq }k∈Nini ,q∈MinF (4)\nWe include all figures F in each section embedding Eseci because each section can reference any of the figures.\nProgress Tracker (PT) We define the PT as a state machine operating in a hierarchically-structured space with sections ([SEC]), slides ([SLIDE]), and objects ([OBJ]). This is to reflect the structure of documents and slides, i.e., each section of a document can have multiple corresponding slides, and each slide can contain multiple objects.\nThe PT maintains pointers to the current section i and the current slide j, and learns a policy to proceed to the next section/slide as it generates slides. For simplicity, we initialize i = j = 0, i.e., the output slides will follow the natural order of sections in an input document.\nWe construct PT as a three-layer hierarchical RNN with (PTsec,PTslide,PTobj), where each RNN encodes the latent space for each level in a section-slide-object hierarchy. This is a natural choice to encode our prior knowledge about the hierarchical structure; in Section 5, we empirically compare this to a “flattened” version of RNN that encodes the section-slide-object structure using a single latent space.\nFirst, PTsec takes as input the head-tail contextualized sentence embeddings from the DR, which encodes the overall information of the current section Si:\nhseci = PT sec(hseci−1, [X in i,1, X in i,Nini ]), (5)\nWe use GRU [15] for PTsec and initialize hsec0 to the contextualized sentence embeddings of the first section, i.e., hsec0 = [X in 0,1, X\nin 0,Nin0 −1 ].\nBased on the section state hseci , PT slide models the\nsection-to-slide relationships,\nasecj , h slide j = PT slide(asecj−1, h slide j−1 , E sec i ), (6)\nwhere hslide0 = h sec i ,E sec i is the section embedding (Eq. 4), and asecj is a binary action variable that tracks the section pointer, i.e, it decides if the model should generate a new slide for the current section Si or proceed to the next section Si+1. We implement PTslide as a GRU and a two-layer MLP with a binary decision head that learns a policy φ to predict asecj = {[NEW SLIDE],[END SEC]},\nasecj = MLP slide φ ([h slide j , ∑ r αslidej,r E sec i,r ]),\nαslidej = softmax(h slide j W (E sec i )\nᵀ). (7)\nHere, αslidej ∈ RN in i +M in is an attention map over Eseci that computes the compatibility between hslidej and E sec i in a bilinear form. Finally, the object PTobj tracks which objects to put on the current slide Oj based on the slide state hslidej ,\naslidek , h obj k = PT obj(aslidek−1 , h obj k−1, E sec i ),\naslidek = MLP obj ψ ([h obj k , ∑ r αobjk,rE sec i,r ]),\nαobjk = softmax(h obj k W (E sec i ) ᵀ),\n(8)\nwhere we set hobj0 = h slide j . Similar to PT slide, aslidek = {[NEW OBJ],[END SLIDE]} is a binary action variable that decides whether to put a new object for the current slide or proceed to the next. We again use a GRU and a two-layer MLP with a policy ψ to implement PTobj , together with an\nattention mechanism that measures the compatibility scores between hobjk andE sec i . Note that each of the three PTs have an independent set of weights to ensure that they model distinctive dynamics in the section-slide-object structure.\nObject Placer (OP) When PTobj takes an action aslidek = [NEW OBJ], the OP selects an object from the current section Si and predicts the location on the current slide Oj in which to place it. For this, we use the attention score αobjk to choose an object (sentence or figure) that has the maximum compatibility score with the current object state hobjk , i.e., argmaxr α obj k . We then employ a two-layer MLP to predict the layout variable for the chosen object,\n{lxk , l y k, l w k , l h k} = MLPlayout([h obj k , ∑ r αobjk,rE sec i,r ]), (9)\nNote that the distinctive style of presentation slides requires special treatment of the objects. If an object is a figure, we take only the image part and resize it to fit the bounding box region while maintaining the original aspect ratio. If an object is a sentence, we first paraphrase it into a concise form and also adjust the font size to fit inside the bounding box region.\nParaphraser (PAR) We paraphrase sentences before placing them on slides. This step is crucial because without it the text would be too verbose for a slide presentation.2 We implement the PAR as an attention-based Seq2Seq [7] with the copy mechanism [28]:\n{w0, ..., wl−1} = PAR(T outj,k , h obj k ), (10)\nwhere T outj,k is a sentence from a document chosen by OP. We condition PAR on the object state hobjk to provide contextual information; we provide the importance of this conditioning in the supplementary material."
    }, {
      "heading" : "3.2. Training",
      "text" : "We design a learning objective that captures both the structural similarity and the content similarity between the ground-truth slides and the generated slides.\nStructural similarity The series of actions asecj and aslidek determines the structure of output slides, i.e., the number of slides per section. To encourage our model to generate slide decks with a similar structure as the groundtruth, we define our structural similarity loss as\nLstructure = ∑\nj CE(asecj ) + ∑ k CE(aslidek ) (11)\nwhere CE is the cross-entropy loss. 2In our dataset, sentences in the documents have an average of 17.3 words, while sentences in slides have 11.6 words; the difference is statistically significant (p = 0.0031).\nContent similarity We formulate our content similarity loss to capture various aspects of slide generation quality, measuring whether the model 1) selected important sentences and figures from the input document, 2) adequately phrased sentences in the presentation style (e.g., shorter sentences), 3) placed sentences and figures to the right locations on a slide, and 4) put sentences and figures on a slide that are relevant to each other.\nWe define our content similarity loss to measure each of the four aspects described above:\nLcontent = ∑\nk CE(αobjk ) + ∑ l CE(wl)+∑\nu,v CE(δ([Etxtu , E fig v ])) + ∑ k MSE(Lk). (12)\nSelection loss. The first term checks whether the model selected the “correct” objects that also appear in the groundtruth slide deck. This term is slide-insensitive, i.e., the correct/incorrect inclusion of an object is not affected by which specific slide it appears in.\nParaphrasing loss. The second term measures the quality of paraphrased sentences by comparing the output sentence and the ground-truth sentence word-by-word.\nText-image matching loss. The third term measures the relevance of text and figures appearing in the same slide. We follow the literature on visual-semantic embedding [26, 38, 37] and learn an additional multimodal projection head δ([Etxtu , E fig v ]) with a sigmoid activation that outputs a scalar variable in [0, 1] indicating the relevance score of text and figure embeddings. We construct training samples with positive and negative pairs. For positive pairs, we sample text-figure pairs from a) the ground-truth slides and b) paragraph-figure pairs where the figure is mentioned in the paragraph. We randomly construct negative pairs.\nLayout loss. The last term measures the quality of slide layout by regressing the predicted bounding box to the ground-truth. While there exist several solutions to bounding box regression [29, 53], we opted for the simple mean squared error (MSE) computed directly over the layout variable Lk = {lxk , l y k, l w k , l h k}.\nThe final loss We define our final learning objective as\nLDOC2PPT = Lstructure + γLcontent (13)\nwhere γ controls the relative importance between structural and content similarity; we set γ = 1 in our experiments.\nTo train our model, which is a sequential prediction task, we follow the standard teacher-forcing approach [61] and provide the ground-truth results for the past prediction steps, e.g., the next actions asecj and a slide k are based on the ground-truth actions ãsecj−1 and ã slide k−1 , the next object α obj k is selected based on the ground-truth object α̃objk−1, etc."
    }, {
      "heading" : "3.3. Inference",
      "text" : "The inference procedures during training and test times largely follow the same process, with one exception: At test time, we utilize the multimodal projection head δ(·) to act as a post-processing tool. That is, once our model generates a slide deck, we remove figures that have relevance scores lower than a threshold θR and add figures with scores higher than a threshold θA. We tune the two hyper-parameters θR and θA via cross-validation (we set θR = 0.8, θA = 0.9)."
    }, {
      "heading" : "4. Dataset",
      "text" : "We collect pairs of documents and the corresponding slide decks from academic proceedings, focusing on three research communities: computer vision (CVPR, ECCV, BMVC), natural language processing (ACL, NAACL, EMNLP), and machine learning (ICML, NeurIPS, ICLR). Table 1 reports the descriptive statistics of our dataset.\nOur dataset contains PDF documents and slides in the JPEG image format. For the training and validation set, we automatically extract text and figures from them and perform matching to create document-to-slide correspondences at various levels. To ensure that our test set is clean and reliable, we use Amazon Mechanical Turk (AMT) and have humans perform image extraction and matching for the entire test set. We provide an overview of our extraction and matching processes below; including details of data collection and automatic extraction/matching processes with reliability analyses in the supplementary material.\nText and Figure Extraction. For each document D, we extract sections S and sentences T in using ScienceParse [4] and figures F in using PDFFigures2.0 [18]. For each slide deck O, we extract sentences T out using Azure OCR [1] and figures Fout using morphological transformation and the border following technique [57, 2].\nSlide Stemming. Many slides are presented with animations, and this makes O contain some successive slides that have similar content minus one element on the preceding slide. For simplicity we consider these near-duplicate slides as redundant and remove them by comparing text and image contents of successive slides: if Oj+1 covers more than 80% of the content of Oj (per text/visual embeddings) we discard it and keepOj+1 as it is deemed more complete.\nSlide-Section Matching. We match slides in a deck to the sections in the corresponding document so that a slide deck is represented as a set of non-overlapping slide groups each with a matching section in the document. To this end, we use RoBERTa [46] to extract embeddings of the text content in each slide and the paragraphs in each section of the document. We assume that a slide deck follows the section order of the corresponding document, and use dynamic programming to find slide-to-section matching based on the cosine similarity between text embeddings.\nSentence Matching. We match sentences from slides to the corresponding document. We again use RoBERTa to extract embeddings of each sentence in slides and documents, and search for the matching sentence based on the cosine similarity. We limit the search space only within the corresponding sections using the slide-section matching result.\nFigure Matching. Lastly, we match figures from slides to those in the corresponding document. We use MobileNet [31] to extract visual embeddings of all Iin and Iout and match them based on the highest cosine similarity. Note that some figures in slides do not appear in the corresponding document (and hence no match). For simplicity, we discard F out if its highest visual embedding similarity is lower than a threshold θI = 0.8."
    }, {
      "heading" : "5. Experiments",
      "text" : "DOC2PPT is a new task with no established evaluation metrics and baselines. To enable large-scale evaluation we propose automatic metrics specifically designed for evaluating slide generation methods. We carefully ablate various components of our approach and evaluate them on our proposed metrics. We also perform human evaluation to assess the generation quality."
    }, {
      "heading" : "5.1. Evaluation Metrics",
      "text" : "Slide-Level ROUGE (ROUGE-SL) To measure the quality of text in the generated slides, we adapt the ROUGE score [41] widely-used in document summarization. Note that ROUGE does not account for the text length in the output, which is problematic for presentation slides (e.g., text in slides are usually shorter).\nIntuitively, the number of slides in a deck is a good proxy for the overall text length. If too short, too much text will be put on the same slide, making it difficult to read; conversely, if a deck has too many slides, each slide can convey\nonly little information while making the whole presentation lengthy. Therefore, we propose the slide-level ROUGE:\nROUGE-SL = ROUGE-L× e |Q−Q̃| Q , (14)\nwhere Q and Q̃ are the number of slides in the generated and the ground-truth slide decks, respectively.\nLongest Common Figure Subsequence (LC-FS) We measure the quality of figures in the output slides by considering both the correctness (whether the figures from the ground-truth deck are included) and the order (whether all the figures are ordered logically – i.e, in a similar manner to the ground-truth deck). To this end, we use the Longest Common Subsequence (LCS) to compare the list of figures in the output {Iout0 , Iout1 , ...} to the ground-truth {Ĩout0 , Ĩout1 , ...} and report precision/recall/F1.\nText-Figure Relevance (TFR) A good slide deck should put text with relevant figures to make the presentation informative and attractive. In addition to considering text and figures independently, we measure the relevance between them. We again adapt ROUGE and modify as\nTFR = 1\nM inF\n∑MinF −1 i=0 ROUGE-L(Pi, P̃i), (15)\nwhere Pi and P̃i are sentences from generated and groundtruth slides that contain Iini , respectively.\nMean Intersection over Union (mIoU) A good design layout makes it easy to consume information presented in slides. To evaluate the layout quality, we adapt the mean intersection over union (mIoU) [24] by incorporating the LCS idea. Given a generated slide deck O and the groundtruth Õ, we compute:\nmIoU(O, Õ) = 1 NoutO ∑NoutO −1 i=0 IoU(Oi, ÕJi) (16)\nwhere IoU(Oi, Õj) computes the IoU between a set of predicted bounding boxes from slide i and a set of ground-truth bounding boxes from slide and Ji. To account for a potential structural mismatch (with missing/extra slides), we find the J = {j0, j1, ..., jNoutO −1} that achieves the maximum mIoU between O and Õ in an increasing order."
    }, {
      "heading" : "5.2. Implementation Detail",
      "text" : "For the DR, we use a Bi-GRU with 1,024 hidden units and set the MLPs to output 1,024-dimensional embeddings. Each layer of the PT is based on a 256-unit GRU. The PAR is designed as an attention-based seq2seq model [7] with 512 hidden units. All the MLPs are two-layer fullyconnected networks. We train our network end-to-end using ADAM [21] with a learning rate of 3e-4."
    }, {
      "heading" : "5.3. Results and Discussions",
      "text" : "Is the hierarchical modeling effective? To answer this question we define a “flattened” version of our Progress Tracker (flat-PT) by replacing the hierarchical RNN with a vanilla RNN that learns a single shared latent space to model the section-slide-object structure. The flat-PT contains a single GRU and a two-layer MLP with a ternary decision head that learns a policy ζ to predict an action at = {[NEW SECTION],[NEW SLIDE],[NEW OBJ]}. For a fair comparison, we increase the number of hidden units in the baseline GRU to 512 (ours is 256) so the model capacities are roughly the same between the two.\nFirst, we compare the structural similarity between the generated and the ground-truth slide decks. For this, we build a list of tokens indicating a section-slide-object structure (e.g., [SEC],[SLIDE],[OBJ], ...,[SLIDE], ...) and compare the lists using the longest common subsequence (LCS). Our hierarchical approach achieves 64.15% vs. the flat approach 51.72%, suggesting that ours was able to learn the structure better than baseline.\nTable 2 (a) and (b) compare the two models on the four metrics introduced in Sec. 5.1. The results show that ours outperforms flat-PT across all metrics. The flat-PT achieves slightly better performance on ROUGE-SL without the slide-length term (w/o SL), which is the same as ROUGE-L. This suggests that ours generates a slide structure more similar to the ground-truth than the flat approach.\nA deeper look into the content similarity loss We ablate different terms in the content similarity loss (Eq. 12) to understand their individual effectiveness; shown in Table 2.\nPAR. The paraphrasing loss improves text quality in slides; see the ROUGE-SL scores of (b) vs. (c), and (d) vs. (e). It also improves the TFR metric because any improvement in text quality will benefit text-image relevance.\nTIM. The text-image matching loss improves the figure quality; see (b) vs. (d) and (c) vs. (e). It particularly improves LC-FS precision with a moderate drop in recall rate, indicating the model added more correct figures. TIM also improves ROUGE-SL because it helps constrain the multimodal embedding space, resulting in better selection of text.\nFigure post-processing At test time, we leverage the multimodal projection head δ(·) as a post-processing module to add missing figures and/or remove unnecessary ones. Table 2 (f) shows this post-processing further improves the two image-related metrics, LC-FS and TFR. For simplicity, we add figures following equally fitting in template-based design instead of using OP to predict its location.\nLayout prediction vs. templates The object placer (OP) predicts the layout to decide where and how to put the extracted objects. We compare this with a template-based approach, which selects the current section title as the slide title and puts sentences and figures in the body line-by-line. For those extracted figures, they will equally fit (with the same width) in the remaining space under the main content.\nThe result shows that the predicted-based layout, which directly learns from the layout loss, can bring out higher mIoU with the groundtruth. But for the template-based design, in the aspect of the visualization, it can make the generated slide deck more consistent.\nTopic-aware evaluation We evaluate performance in a topic-dependent and independent fashion. To do this, we train and test our model on data from each of the three research communities (CV, NLP, and ML). Table 3 shows that models trained and tested within each topic performs the best (not surprisingly), and that models trained on data from all topics achieves the second best performance, showing generalization to different topic areas. Training on NLP data, despite being the smallest among the three, seems to\ngeneralize well to other topics on the text metric, achieving the second best on ROUGE-SL (28.9). Training on CV data provides the second highest performance on the text-figure metric TFR (15.8), and training on ML achieves the highest figure extraction performance (LC-FS F1 of 31.4).\nHuman evaluation We conduct a user study to assess the perceived quality of generates slides. We select 50 documents from the test set and prepare four slide decks per document: the ground-truth deck, and the ones generated by the flat PT (Table 2 (a)), by ours without PAR and TIM (b), and by our final model (f). To make the task easy to complete, we sample 200 sections from 50 documents and create 600 pairs of ground-truth and generate slides.\nWe recruited three AMT Master Workers for each task (HIT). The workers were shown the slides from the groundtruth deck (DECK A) and one of the methods (DECK B). The workers were then asked to answer three questions: Q1. Looking only at the TEXT on the slides, how similar is the content on the slides in DECK A to the content on the slides in DECK B?; Q2. How well do the figure(s)/tables(s) in DECK A match the text or figures/tables in DECK B?; Q3. How well do the figure(s)/table(s) in DECK A match the TEXT in DECK B? The responses were all on a scale of 1 (not similar at all) to 7 (very similar). Fig. 4 shows the average scores for each method. The average rating for our approach was significantly greater for all three questions compared to the other two methods. There was no significant difference between the ratings for the other two methods.\nQualitative Results Fig. 3 illustrates two qualitative examples (top [33] and bottom [11]) of the slide deck gener-\nated by our model. With the post-processing, TIM can add the related figure into the slide and make it more informative. PAR helps create a better presentation by paraphrasing the sentences into bullet point form."
    }, {
      "heading" : "6. Conclusion",
      "text" : "We present a novel task and approach for generating slides from documents. This is a challenging multimodal task that involves understanding and summarizing documents containing text and figures and structuring it into a presentation form. We release a large set of 5,873 paired documents and slide decks, and provide evaluation metrics with our results. We hope our work will help advance the state-of-the-art in vision and language understanding."
    }, {
      "heading" : "A. Details of the Data Processing Steps",
      "text" : "Section 4 in our main paper explains how we construct our DOC2PPT dataset. Here we provide the details of the process and demonstrate the accuracy of the various extraction/matching processes. Fig. 5 illustrates the details of the data processing pipeline that were omitted in the main paper. To evaluate how reliable the various steps in our pipeline are, we manually labeled 100 slide decks (randomly sampled from the validation split) and used them for evaluation.\nText Extraction Fig. 6 shows examples of the extracted slide sentences obtained using Azure OCR [1]. The original slides are shown on left and the extracted text is on the right. Notice that the OCR results are quite reliable as slides contain text in a clear format.\nSlide Stemming Fig. 7 illustrates the slide stemming process. If a slide has a preceding slide with 80% or greater overlap in content, we consider the preceding slide as redundant and remove it. The slides which are opaque (ghosted) are examples of slides that would be removed (they often exist because of animations that sequentially add elements to a slide - e.g., bullet points appearing - thus we just keep the final slide in the sequence to simplify the dataset). Our slide stemming step is 93% accurate based on the human annotated validation set.\nSlide-Section Matching Fig. 8 presents an example of slide-section matching. We adopt RoBERTa [46] to extract embeddings of the text in slides and sections in the document (paper). Specifically, we find slide-to-section matching based on the cosine similarity between text embeddings. Slides are matched with the section with the highest cosine similarity and our slide-section matching has 82% accuracy.\nSentence Matching Table 4 shows examples of matching sentences between the paper and the slide. We again use RoBERTa [46] to search for the matching sentence based on the cosine similarity and build the linking for the extractive summarization.\nFigure Matching Fig. 9 illustrates examples of figures/tables that were matched with a particular slide. We apply morphological transformation [2] and border following [57] to extract possible slide figures. We then match them with figures in the paper using the visual embedding from MobileNet [31]; if the cosine similarity is larger than the threshold θI . Fig. 10 presents the precision, recall, and F1, which are evaluated from human-labeled test set. The x-axis represents different values of threshold θI considered when comparing the cosine similarity of the visual embedding. When θI is lower, more figures from the paper will be included, which increases recall but negatively impacts precision; in contrast, a higher θI results in greater precision but lower recall. Fig. 11 shows examples where the figure matching performs poorly. There are two cases: 1) partial figure matches where a figure has had elements added or removed, and 2) different versions of a figure where the meaning might be similar but the images do not match. These cases make matching difficult, because based on the visual embedding they may not be very similar.\nHuman Labeling To ensure that our test set is clean and reliable, we use Amazon Mechanical Turk (AMT) and have humans perform image extraction and matching for the entire testing set. Fig. 12 shows a screenshot of the MTurk HIT for labeling figure matches within each slide. The slide is shown on the left and figures from the document (paper) were shown on the right. The human annotators can label each figure either as a match (by clicking on the image) or as similar but not an exact match (by ticking the checkbox next to the image). Fig. 13 shows a screenshot of the MTurk HIT for labeling the bounding box around the image on a slide. The candidate figure is shown above and the human annotator is asked to draw a bounding box around the region of the slide where it appeared. We perform figure-slide matching (see above) before bounding box labeling as this produced the best quality annotations (bounding box labeling is not necessary if the image isn’t on the slide at all). For the human-labeled testing set, a slide deck contains on average 2.3 images that are excerpted from the corresponding paper. Please note that since people tend to adopt more new figures or different figures in a slide deck for computer vision (CV) field, the average number of excerpted figure is lower (1.7)."
    }, {
      "heading" : "B. Settings of Approach",
      "text" : "The importance of hobj in Paraphrasing Module Table 5 presents the Rouge-L of paraphrasing module (PAR) with or without using the object state hobj . The results show that the text quality improves in all cases if we apply PAR. Also, using hobj benefits more (w/ 32.27 vs w/o 31.95). This is because hobj provides contextual information, which helps PAR generate a paraphrased sentence more relevant to the content in the document.\nSensitivity of θR and θA in Post-Processing During the post-processing, we remove figures deemed irrelevant by θR and add ones if considered highly relevant based on θA. To achieve the best result, we tune our θR and θA on the 100 labeled validation set. Fig. 14 shows that θR = 0.8 and θA = 0.9 achieves the highest LC-F1."
    }, {
      "heading" : "C. Human Evaluation",
      "text" : "Fig. 15 shows a screenshot of the human rating task for evaluating the quality of the generated slides. The ground-truth slide deck was shown (left) alongside the generated slides (right). The human annotators were asked three questions. 1) How similar the text on slide DECK A was to the text on slide DECK B. 2) How similar the figures on slide DECK A was to the figures on slide DECK B - the could also indicate that no figures were present. 3) How similar the figures in DECK B were to the text in DECK A - again they could indicate that no figures were present if that was the case."
    }, {
      "heading" : "D. Qualitative Examples",
      "text" : "Fig. 17 demonstrates generated slide decks from our approach. We provide more results, including failure cases, on our project webpage.\nApplying PowerPoint Design Ideas As we discussed in the main paper, the output of our method can be used as a draft slide deck for humans to build upon. We provide one such application scenario of our approach. When the slide decks are generated based on a template, the content are all in a fixed size and in the fix position. To make the output more attractive, we can apply off-the-shelf tools such as Microsoft PowerPoint Design Ideas [3] which can automatically produce a layout for the given texts and figures. As shown in Fig. 16, the generated decks are more professional looking."
    } ],
    "references" : [ {
      "title" : "TGIF-QA: Toward Spatio-Temporal Reasoning in Visual Question Answering",
      "author" : [ "Aishwarya Agrawal", "Jiasen Lu", "Stanislaw Antol", "Margaret Mitchell", "C. Lawrence Zitnick", "Dhruv Batra", "Devi Parikh" ],
      "venue" : "In International Conference on Computer Vision (ICCV),",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2015
    }, {
      "title" : "Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering",
      "author" : [ "Peter Anderson", "Xiaodong He", "Chris Buehler", "Damien Teney", "Mark Johnson", "Stephen Gould", "Lei Zhang" ],
      "venue" : "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2018
    }, {
      "title" : "Neural Machine Translation by Jointly Learning to Align and Translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : "In International Conference on Learning Representations (ICLR),",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2015
    }, {
      "title" : "Variations of the Similarity Function of TextRank for Automated Summarization",
      "author" : [ "Federico Barrios", "Federico López", "Luis Argerich", "Rosa Wachenchauzer" ],
      "venue" : "In Argentine Symposium on Artificial Intelligence (ASAI),",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2015
    }, {
      "title" : "Deep Communicating Agents for Abstractive Summarization",
      "author" : [ "Asli Celikyilmaz", "Antoine Bosselut", "Xiaodong He", "Yejin Choi" ],
      "venue" : "In Conference of the North American Chapter of the Association for Computational Linguistics (NAACL),",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2018
    }, {
      "title" : "Explainable and Discourse Topic-aware Neural Language Understanding",
      "author" : [ "Yatin Chaudhary", "Hinrich Schütze", "Pankaj Gupta" ],
      "venue" : "In International Conference of Machine Learning (ICML),",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2020
    }, {
      "title" : "Semi-Supervised Learning in Video Sequences for Urban Scene Segmentation",
      "author" : [ "Liang-Chieh Chen", "Raphael Gontijo Lopes", "Bowen Cheng", "Maxwell D. Collins", "Ekin D. Cubuk", "Barret Zoph", "Hartwig Adam", "Jonathon Shlens" ],
      "venue" : "In European Conference on Computer Vision (ECCV),",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2020
    }, {
      "title" : "Iterative Document Representation Learning Towards Summarization with Polishing",
      "author" : [ "Xiuying Chen", "Shen Gao", "Chongyang Tao", "Yan Song", "Dongyan Zhao", "Rui Yan" ],
      "venue" : "In Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2018
    }, {
      "title" : "Neural Summarization by Extracting Sentences and Words",
      "author" : [ "Jianpeng Cheng", "Mirella Lapata" ],
      "venue" : "In Annual Meeting of the Association for Computational Linguistics (ACL),",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2016
    }, {
      "title" : "Mixture Content Selection for Diverse Sequence Generation",
      "author" : [ "Jaemin Cho", "Minjoon Seo", "Hannaneh Hajishirzi" ],
      "venue" : "In Conference on Empirical Methods in Natural Language Processing and International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2019
    }, {
      "title" : "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
      "author" : [ "Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio" ],
      "venue" : "In Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2014
    }, {
      "title" : "Abstractive Sentence Summarization with Attentive Recurrent Neural Networks",
      "author" : [ "Sumit Chopra", "Michael Auli", "Alexander M Rush" ],
      "venue" : "In Conference of the North American Chapter of the Association for Computational Linguistics (NAACL),",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2016
    }, {
      "title" : "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling",
      "author" : [ "Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio" ],
      "venue" : "In Advances in Neural Information Processing Systems Workshop (NeurIPS WS),",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2014
    }, {
      "title" : "PDFFigures 2.0: Mining Figures from Research Papers",
      "author" : [ "Christopher Clark", "Santosh Divvala" ],
      "venue" : "In IEEE/ACM Joint Conference on Digital Libraries(JCDL),",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2016
    }, {
      "title" : "An Effective Transition-based Model for Discontinuous NER",
      "author" : [ "Xiang Dai", "Sarvnaz Karimi", "Ben Hachey", "Cecile Paris" ],
      "venue" : "In Annual Meeting of the Association for Computational Linguistics (ACL),",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2020
    }, {
      "title" : "Visual Dialog",
      "author" : [ "Abhishek Das", "Satwik Kottur", "Khushi Gupta", "Avi Singh", "Deshraj Yadav", "José M.F. Moura", "Devi Parikh", "Dhruv Batra" ],
      "venue" : "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2017
    }, {
      "title" : "Adam: A Method for Stochastic Optimization",
      "author" : [ "Jimmy Ba Diederik P. Kingma" ],
      "venue" : "In International Conference on Learning Representations (ICLR),",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2014
    }, {
      "title" : "Unified Language Model Pre-training for Natural Language Understanding and Generation",
      "author" : [ "Li Dong", "Nan Yang", "Wenhui Wang", "Furu Wei", "Xiaodong Liu", "Yu Wang", "Jianfeng Gao", "Ming Zhou", "Hsiao-Wuen Hon" ],
      "venue" : "In Advances in Neural Information Processing Systems (NeurIPS),",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2019
    }, {
      "title" : "Blind Men and Elephants: What do Citation Summaries Tell Us about a Research Article",
      "author" : [ "Aaron Elkiss", "Siwei Shen", "Anthony Fader", "Güneş Erkan", "David States", "Dragomir Radkov" ],
      "venue" : "In Journal of the American Society for Information Science and Technology (JASIST),",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2008
    }, {
      "title" : "The PASCAL Visual Object Classes (VOC) Challenge",
      "author" : [ "Mark Everingham", "Luc Van Gool", "Christopher K.I. Williams", "John Winn", "Andrew Zisserman" ],
      "venue" : "In International Journal of Computer Vision (IJCV),",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2010
    }, {
      "title" : "VSE++: Improving Visual-Semantic Embeddings with Hard Negatives",
      "author" : [ "Fartash Faghri", "David J. Fleet", "Jamie Ryan Kiros", "Sanja Fidler" ],
      "venue" : "In British Machine Vision Conference (BMVC),",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2018
    }, {
      "title" : "DeViSE: A Deep Visual-Semantic Embedding Model",
      "author" : [ "Andrea Frome", "Greg S. Corrado", "Jonathon Shlens", "Samy Bengio", "Jeffrey Dean", "Marc’Aurelio Ranzato", "Tomas Mikolov" ],
      "venue" : "In Advances in Neural Information Processing Systems (NeurIPS),",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2013
    }, {
      "title" : "Look, Imagine and Match: Improving Textual-Visual Cross-Modal Retrieval with Generative Models",
      "author" : [ "Jiuxiang Gu", "Jianfei Cai", "Shafiq Joty", "Li Niu", "Gang Wang" ],
      "venue" : "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2018
    }, {
      "title" : "Incorporating Copying Mechanism in Sequence-to-Sequence Learning",
      "author" : [ "Jiatao Gu", "Zhengdong Lu", "Hang Li", "Victor O.K", "Li" ],
      "venue" : "In Annual Meeting of the Association for Computational Linguistics (ACL),",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2016
    }, {
      "title" : "Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "In IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI),",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2015
    }, {
      "title" : "Deep Residual Learning for Image Recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2016
    }, {
      "title" : "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications",
      "author" : [ "Andrew G. Howard", "Menglong Zhu", "Bo Chen", "Dmitry Kalenichenko", "Weijun Wang", "Tobias Weyand", "Marco Andreetto", "Hartwig Adam" ],
      "venue" : "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2017
    }, {
      "title" : "Learning Semantic Concepts and Order for Image and Sentence Matching",
      "author" : [ "Yan Huang", "Qi Wu", "Liang Wang" ],
      "venue" : "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2018
    }, {
      "title" : "Semi-Supervised Learning with Normalizing Flows",
      "author" : [ "Pavel Izmailov", "Polina Kirichenko", "Marc Finzi", "Andrew Gordon Wilson" ],
      "venue" : "In International Conference of Machine Learning (ICML),",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2020
    }, {
      "title" : "Overview of the CL- SciSumm 2016 Shared Task. In Joint Workshop on Bibliometric-enhanced Information Retrieval and NLP for Digital Libraries (BIRNDL), 2016",
      "author" : [ "Kokil Jaidka", "Muthu Kumar", "Chandrase karan", "Sajal Rustagi amd Min-Yen Kan" ],
      "venue" : null,
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2016
    }, {
      "title" : "Learning What and Where to Transfer",
      "author" : [ "Yunhun Jang", "Hankook Lee", "Sung Ju Hwang", "Jinwoo Shin" ],
      "venue" : "In International Conference of Machine Learning (ICML),",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2019
    }, {
      "title" : "VQA: Visual Question Answering",
      "author" : [ "Yunseok Jang", "Yale Song", "Youngjae Yu", "Youngjin Kim", "Gunhee Kim" ],
      "venue" : "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2017
    }, {
      "title" : "Deep Visual-Semantic Alignments for Generating Image Descriptions",
      "author" : [ "Andrej Karpathy", "Li Fei-Fei" ],
      "venue" : "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2014
    }, {
      "title" : "Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models",
      "author" : [ "Ryan Kiros", "Ruslan Salakhutdinov", "Richard S. Zemel" ],
      "venue" : "In Advances in Neural Information Processing Systems Workshop (NeurIPS WS),",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2014
    }, {
      "title" : "TalkSumm: A Dataset and Scalable Annotation Method for Scientific Paper Summarization Based on Conference Talks",
      "author" : [ "Guy Lev", "Michal Shmueli-Scheuer", "Jonathan Herzig", "Achiya Jerbi", "David Konopnicki" ],
      "venue" : "In Annual Meeting of the Association for Computational Linguistics (ACL),",
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2019
    }, {
      "title" : "TGIF: A New Dataset and Benchmark on Animated GIF Description",
      "author" : [ "Yuncheng Li", "Yale Song", "Liangliang Cao", "Joel Tetreault", "Larry Goldberg", "Alejandro Jaimes", "Jiebo Luo" ],
      "venue" : "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 2016
    }, {
      "title" : "ROUGE: A Package for Automatic Evaluation of Summaries",
      "author" : [ "Chin-Yew Lin" ],
      "venue" : "In Annual Meeting of the Association for Computational Linguistics (ACL),",
      "citeRegEx" : "41",
      "shortCiteRegEx" : "41",
      "year" : 2014
    }, {
      "title" : "Boosting Semantic Human Matting with Coarse Annotations",
      "author" : [ "Jinlin Liu", "Yuan Yao", "Wendi Hou", "Miaomiao Cui", "Xuansong Xie", "Changshui Zhang", "Xian sheng Hua" ],
      "venue" : "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "42",
      "shortCiteRegEx" : "42",
      "year" : 2020
    }, {
      "title" : "Generative Adversarial Network for Abstractive Text Summarization",
      "author" : [ "Linqing Liu", "Yao Lu", "Min Yang", "Qiang Qu", "Jia Zhu", "Hongyan Li" ],
      "venue" : "In Association for the Advancement of Artificial Intelligence (AAAI),",
      "citeRegEx" : "43",
      "shortCiteRegEx" : "43",
      "year" : 2018
    }, {
      "title" : "Fine-tune BERT for Extractive Summarization",
      "author" : [ "Yang Liu" ],
      "venue" : "In arXiv:1903.10318,",
      "citeRegEx" : "44",
      "shortCiteRegEx" : "44",
      "year" : 1903
    }, {
      "title" : "Text Summarization with Pretrained Encoders",
      "author" : [ "Yang Liu", "Mirella Lapata" ],
      "venue" : "In Conference on Empirical Methods in Natural Language Processing and International Joint Conference on Natural Language Processing (EMNLP- IJCNLP),",
      "citeRegEx" : "45",
      "shortCiteRegEx" : "45",
      "year" : 2019
    }, {
      "title" : "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov" ],
      "venue" : "In arxiv:1907.11692,",
      "citeRegEx" : "46",
      "shortCiteRegEx" : "46",
      "year" : 2019
    }, {
      "title" : "COMPENDIUM: A Text Summarization System for Generating Abstracts of Research Papers",
      "author" : [ "Elena Lloret", "Marı́a Teresa Romá-Ferri", "Manuel Palomar" ],
      "venue" : "In Data & Knowledge Engineering,",
      "citeRegEx" : "47",
      "shortCiteRegEx" : "47",
      "year" : 2013
    }, {
      "title" : "End-to-End Optimization of Scene Layout",
      "author" : [ "Andrew Luo", "Zhoutong Zhang", "Jiajun Wu", "Joshua B. Tenenbaum" ],
      "venue" : "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "48",
      "shortCiteRegEx" : "48",
      "year" : 2020
    }, {
      "title" : "VMSMO: Learning to Generate Multimodal Summary for Video-based News Articles",
      "author" : [ "Luca Marchesotti", "Florent Perronnin", "Diane Larlus", "Gabriela Csurka" ],
      "venue" : "In International Conference on Computer Vision (ICCV),",
      "citeRegEx" : "49",
      "shortCiteRegEx" : "49",
      "year" : 2011
    }, {
      "title" : "Ranking Sentences for Extractive Summarization with Reinforcement Learning",
      "author" : [ "Shashi Narayan", "Shay B. Cohen", "Mirella Lapata" ],
      "venue" : "In Conference of the North American Chapter of the Association for Computational Linguistics (NAACL),",
      "citeRegEx" : "50",
      "shortCiteRegEx" : "50",
      "year" : 2018
    }, {
      "title" : "Generating Coherent Summaries of Scientific Articles Using Coherence Patterns",
      "author" : [ "Daraksha Parveen", "Mohsen Mesgar", "Michael Strube" ],
      "venue" : "In Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "51",
      "shortCiteRegEx" : "51",
      "year" : 2016
    }, {
      "title" : "A Deep Reinforced Model for Abstractive Summarization",
      "author" : [ "Romain Paulus", "Caiming Xiong", "Richard Socher" ],
      "venue" : "In International Conference on Learning Representations (ICLR),",
      "citeRegEx" : "52",
      "shortCiteRegEx" : "52",
      "year" : 2018
    }, {
      "title" : "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
      "author" : [ "Shaoqing Ren", "Kaiming He", "Ross Girshick", "Jian Sun" ],
      "venue" : "In Advances in Neural Information Processing Systems (NeurIPS),",
      "citeRegEx" : "53",
      "shortCiteRegEx" : "53",
      "year" : 2015
    }, {
      "title" : "A Neural Attention Model for Abstractive Sentence Summarization",
      "author" : [ "Alexander M. Rush", "Sumit Chopra", "Jason Weston" ],
      "venue" : "In Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "54",
      "shortCiteRegEx" : "54",
      "year" : 2015
    }, {
      "title" : "Get To The Point: Summarization with Pointer-Generator Networks",
      "author" : [ "Abigail See", "Peter J. Liu", "Christopher D. Manning" ],
      "venue" : "In Annual Meeting of the Association for Computational Linguistics (ACL),",
      "citeRegEx" : "55",
      "shortCiteRegEx" : "55",
      "year" : 2017
    }, {
      "title" : "Polysemous Visual- Semantic Embedding for Cross-Modal Retrieval",
      "author" : [ "Yale Song", "Mohammad Soleymani" ],
      "venue" : "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "56",
      "shortCiteRegEx" : "56",
      "year" : 2019
    }, {
      "title" : "Topological Structural Analysis of Digitized Images by Border Following",
      "author" : [ "Satoshi Suzuki", "Keiichi Abe" ],
      "venue" : "In Computer Vision, Graphics, and Image Processing (CVGIP),",
      "citeRegEx" : "57",
      "shortCiteRegEx" : "57",
      "year" : 1985
    }, {
      "title" : "Order-Embeddings of Images and Language",
      "author" : [ "Ivan Vendrov", "Ryan Kiros", "Sanja Fidler", "Raquel Urtasun" ],
      "venue" : "In International Conference on Learning Representations (ICLR),",
      "citeRegEx" : "58",
      "shortCiteRegEx" : "58",
      "year" : 2016
    }, {
      "title" : "Show and Tell: Lessons learned from the 2015 MSCOCO Image Captioning Challenge",
      "author" : [ "Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan" ],
      "venue" : "In IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI),",
      "citeRegEx" : "59",
      "shortCiteRegEx" : "59",
      "year" : 2016
    }, {
      "title" : "Reinforced Cross-Modal Matching and Self- Supervised Imitation Learning for Vision-Language Navigation",
      "author" : [ "Xin Wang", "Qiuyuan Huang", "Asli Celikyilmaz", "Jianfeng Gao", "Dinghan Shen", "Yuan-Fang Wang", "William Yang Wang", "Lei Zhang" ],
      "venue" : "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "60",
      "shortCiteRegEx" : "60",
      "year" : 2019
    }, {
      "title" : "A Learning Algorithm for Continually Running Fully Recurrent Neural Networks",
      "author" : [ "Ronald J. Williams", "David Zipser" ],
      "venue" : "In Neural computation,",
      "citeRegEx" : "61",
      "shortCiteRegEx" : "61",
      "year" : 1989
    }, {
      "title" : "MSR-VTT: A Large Video Description Dataset for Bridging Video and Language",
      "author" : [ "Jun Xu", "Tao Mei", "Ting Yao", "Yong Rui" ],
      "venue" : "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "62",
      "shortCiteRegEx" : "62",
      "year" : 2016
    }, {
      "title" : "ScisummNet: A Large Annotated Corpus and Content- Impact Models for Scientific Paper Summarization with Citation Networks",
      "author" : [ "Michihiro Yasunaga", "Jungo Kasai", "Rui Zhang", "Alexander R. Fabbri", "Irene Li", "Dan Friedman", "Dragomir R. Radev" ],
      "venue" : "In Association for the Advancement of Artificial Intelligence (AAAI),",
      "citeRegEx" : "63",
      "shortCiteRegEx" : "63",
      "year" : 2019
    }, {
      "title" : "Graphbased Neural Multi-Document Summarization",
      "author" : [ "Michihiro Yasunaga", "Rui Zhang", "Kshitijh Meelu", "Ayush Pareek", "Krishnan Srinivasan", "Dragomir Radev" ],
      "venue" : "In Conference on Computational Natural Language Learning (CoNLL),",
      "citeRegEx" : "64",
      "shortCiteRegEx" : "64",
      "year" : 2017
    }, {
      "title" : "Optimizing Sentence Modeling and Selection for Document Summarization",
      "author" : [ "Wenpeng Yin", "Yulong Pei" ],
      "venue" : "In International Joint Conference on Artificial Intelligence (IJCAI),",
      "citeRegEx" : "65",
      "shortCiteRegEx" : "65",
      "year" : 2014
    }, {
      "title" : "Image Captioning with Semantic Attention",
      "author" : [ "Quanzeng You", "Hailin Jin", "Zhaowen Wang", "Chen Fang", "Jiebo Luo" ],
      "venue" : "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "66",
      "shortCiteRegEx" : "66",
      "year" : 2016
    }, {
      "title" : "PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization",
      "author" : [ "Jingqing Zhang", "Yao Zhao", "Mohammad Saleh", "Peter J. Liu" ],
      "venue" : "In International Conference of Machine Learning (ICML),",
      "citeRegEx" : "67",
      "shortCiteRegEx" : "67",
      "year" : 2020
    }, {
      "title" : "MSMO: Multimodal Summarization with Multimodal Output",
      "author" : [ "Junnan Zhu", "Haoran Li", "Tianshang Liu", "Yu Zhou", "Jiajun Zhang", "Chengqing Zong" ],
      "venue" : "In Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "68",
      "shortCiteRegEx" : "68",
      "year" : 2019
    }, {
      "title" : "Multimodal Summarization with Guidance of Multimodal Reference",
      "author" : [ "Junnan Zhu", "Yu Zhou", "Jiajun Zhang", "Haoran Li", "Chengqing Zong", "Changliang Li" ],
      "venue" : "In Association for the Advancement of Artificial Intelligence (AAAI),",
      "citeRegEx" : "69",
      "shortCiteRegEx" : "69",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "Document summarization [13, 16] aims to find a concise text summary of the input, but it does not deal with images/figures and lacks multimodal understanding.",
      "startOffset" : 23,
      "endOffset" : 31
    }, {
      "referenceID" : 11,
      "context" : "Document summarization [13, 16] aims to find a concise text summary of the input, but it does not deal with images/figures and lacks multimodal understanding.",
      "startOffset" : 23,
      "endOffset" : 31
    }, {
      "referenceID" : 21,
      "context" : "Cross-modal retrieval [26, 38] focuses on finding a multimodal embedding space but does not produce summarized outputs.",
      "startOffset" : 22,
      "endOffset" : 30
    }, {
      "referenceID" : 33,
      "context" : "Cross-modal retrieval [26, 38] focuses on finding a multimodal embedding space but does not produce summarized outputs.",
      "startOffset" : 22,
      "endOffset" : 30
    }, {
      "referenceID" : 63,
      "context" : "Multimodal summarization [68] deals with both (summarizing documents with text and figures), but it lacks the ability to produce structured output (as in slides).",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 44,
      "context" : "While assessing visual aesthetics have been investigated [49], exiting work focuses on photographic metrics for images that would not translate to slides.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 54,
      "context" : "Image/video captioning [59, 66, 40, 62], visual question answering [36, 5, 6], visually-grounded dialogue generation [20] and visual navigation [60] are all tasks that involve learning relationships between visual imagery and text.",
      "startOffset" : 23,
      "endOffset" : 39
    }, {
      "referenceID" : 61,
      "context" : "Image/video captioning [59, 66, 40, 62], visual question answering [36, 5, 6], visually-grounded dialogue generation [20] and visual navigation [60] are all tasks that involve learning relationships between visual imagery and text.",
      "startOffset" : 23,
      "endOffset" : 39
    }, {
      "referenceID" : 35,
      "context" : "Image/video captioning [59, 66, 40, 62], visual question answering [36, 5, 6], visually-grounded dialogue generation [20] and visual navigation [60] are all tasks that involve learning relationships between visual imagery and text.",
      "startOffset" : 23,
      "endOffset" : 39
    }, {
      "referenceID" : 57,
      "context" : "Image/video captioning [59, 66, 40, 62], visual question answering [36, 5, 6], visually-grounded dialogue generation [20] and visual navigation [60] are all tasks that involve learning relationships between visual imagery and text.",
      "startOffset" : 23,
      "endOffset" : 39
    }, {
      "referenceID" : 31,
      "context" : "Image/video captioning [59, 66, 40, 62], visual question answering [36, 5, 6], visually-grounded dialogue generation [20] and visual navigation [60] are all tasks that involve learning relationships between visual imagery and text.",
      "startOffset" : 67,
      "endOffset" : 77
    }, {
      "referenceID" : 0,
      "context" : "Image/video captioning [59, 66, 40, 62], visual question answering [36, 5, 6], visually-grounded dialogue generation [20] and visual navigation [60] are all tasks that involve learning relationships between visual imagery and text.",
      "startOffset" : 67,
      "endOffset" : 77
    }, {
      "referenceID" : 1,
      "context" : "Image/video captioning [59, 66, 40, 62], visual question answering [36, 5, 6], visually-grounded dialogue generation [20] and visual navigation [60] are all tasks that involve learning relationships between visual imagery and text.",
      "startOffset" : 67,
      "endOffset" : 77
    }, {
      "referenceID" : 15,
      "context" : "Image/video captioning [59, 66, 40, 62], visual question answering [36, 5, 6], visually-grounded dialogue generation [20] and visual navigation [60] are all tasks that involve learning relationships between visual imagery and text.",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 55,
      "context" : "Image/video captioning [59, 66, 40, 62], visual question answering [36, 5, 6], visually-grounded dialogue generation [20] and visual navigation [60] are all tasks that involve learning relationships between visual imagery and text.",
      "startOffset" : 144,
      "endOffset" : 148
    }, {
      "referenceID" : 11,
      "context" : "This task has been tackled from two angles: abstractive [16, 55, 14, 45, 22, 67, 9, 54, 43, 52] and extractive [8, 50, 44, 12, 65, 13, 64].",
      "startOffset" : 56,
      "endOffset" : 95
    }, {
      "referenceID" : 50,
      "context" : "This task has been tackled from two angles: abstractive [16, 55, 14, 45, 22, 67, 9, 54, 43, 52] and extractive [8, 50, 44, 12, 65, 13, 64].",
      "startOffset" : 56,
      "endOffset" : 95
    }, {
      "referenceID" : 9,
      "context" : "This task has been tackled from two angles: abstractive [16, 55, 14, 45, 22, 67, 9, 54, 43, 52] and extractive [8, 50, 44, 12, 65, 13, 64].",
      "startOffset" : 56,
      "endOffset" : 95
    }, {
      "referenceID" : 40,
      "context" : "This task has been tackled from two angles: abstractive [16, 55, 14, 45, 22, 67, 9, 54, 43, 52] and extractive [8, 50, 44, 12, 65, 13, 64].",
      "startOffset" : 56,
      "endOffset" : 95
    }, {
      "referenceID" : 17,
      "context" : "This task has been tackled from two angles: abstractive [16, 55, 14, 45, 22, 67, 9, 54, 43, 52] and extractive [8, 50, 44, 12, 65, 13, 64].",
      "startOffset" : 56,
      "endOffset" : 95
    }, {
      "referenceID" : 62,
      "context" : "This task has been tackled from two angles: abstractive [16, 55, 14, 45, 22, 67, 9, 54, 43, 52] and extractive [8, 50, 44, 12, 65, 13, 64].",
      "startOffset" : 56,
      "endOffset" : 95
    }, {
      "referenceID" : 4,
      "context" : "This task has been tackled from two angles: abstractive [16, 55, 14, 45, 22, 67, 9, 54, 43, 52] and extractive [8, 50, 44, 12, 65, 13, 64].",
      "startOffset" : 56,
      "endOffset" : 95
    }, {
      "referenceID" : 49,
      "context" : "This task has been tackled from two angles: abstractive [16, 55, 14, 45, 22, 67, 9, 54, 43, 52] and extractive [8, 50, 44, 12, 65, 13, 64].",
      "startOffset" : 56,
      "endOffset" : 95
    }, {
      "referenceID" : 38,
      "context" : "This task has been tackled from two angles: abstractive [16, 55, 14, 45, 22, 67, 9, 54, 43, 52] and extractive [8, 50, 44, 12, 65, 13, 64].",
      "startOffset" : 56,
      "endOffset" : 95
    }, {
      "referenceID" : 47,
      "context" : "This task has been tackled from two angles: abstractive [16, 55, 14, 45, 22, 67, 9, 54, 43, 52] and extractive [8, 50, 44, 12, 65, 13, 64].",
      "startOffset" : 56,
      "endOffset" : 95
    }, {
      "referenceID" : 3,
      "context" : "This task has been tackled from two angles: abstractive [16, 55, 14, 45, 22, 67, 9, 54, 43, 52] and extractive [8, 50, 44, 12, 65, 13, 64].",
      "startOffset" : 111,
      "endOffset" : 138
    }, {
      "referenceID" : 45,
      "context" : "This task has been tackled from two angles: abstractive [16, 55, 14, 45, 22, 67, 9, 54, 43, 52] and extractive [8, 50, 44, 12, 65, 13, 64].",
      "startOffset" : 111,
      "endOffset" : 138
    }, {
      "referenceID" : 39,
      "context" : "This task has been tackled from two angles: abstractive [16, 55, 14, 45, 22, 67, 9, 54, 43, 52] and extractive [8, 50, 44, 12, 65, 13, 64].",
      "startOffset" : 111,
      "endOffset" : 138
    }, {
      "referenceID" : 7,
      "context" : "This task has been tackled from two angles: abstractive [16, 55, 14, 45, 22, 67, 9, 54, 43, 52] and extractive [8, 50, 44, 12, 65, 13, 64].",
      "startOffset" : 111,
      "endOffset" : 138
    }, {
      "referenceID" : 60,
      "context" : "This task has been tackled from two angles: abstractive [16, 55, 14, 45, 22, 67, 9, 54, 43, 52] and extractive [8, 50, 44, 12, 65, 13, 64].",
      "startOffset" : 111,
      "endOffset" : 138
    }, {
      "referenceID" : 8,
      "context" : "This task has been tackled from two angles: abstractive [16, 55, 14, 45, 22, 67, 9, 54, 43, 52] and extractive [8, 50, 44, 12, 65, 13, 64].",
      "startOffset" : 111,
      "endOffset" : 138
    }, {
      "referenceID" : 59,
      "context" : "This task has been tackled from two angles: abstractive [16, 55, 14, 45, 22, 67, 9, 54, 43, 52] and extractive [8, 50, 44, 12, 65, 13, 64].",
      "startOffset" : 111,
      "endOffset" : 138
    }, {
      "referenceID" : 18,
      "context" : "A task closely related to ours is scientific document summarization [23, 47, 34, 51], but to date that work has only focused on producing text summaries, while we focus on generating multimedia slides.",
      "startOffset" : 68,
      "endOffset" : 84
    }, {
      "referenceID" : 42,
      "context" : "A task closely related to ours is scientific document summarization [23, 47, 34, 51], but to date that work has only focused on producing text summaries, while we focus on generating multimedia slides.",
      "startOffset" : 68,
      "endOffset" : 84
    }, {
      "referenceID" : 29,
      "context" : "A task closely related to ours is scientific document summarization [23, 47, 34, 51], but to date that work has only focused on producing text summaries, while we focus on generating multimedia slides.",
      "startOffset" : 68,
      "endOffset" : 84
    }, {
      "referenceID" : 46,
      "context" : "A task closely related to ours is scientific document summarization [23, 47, 34, 51], but to date that work has only focused on producing text summaries, while we focus on generating multimedia slides.",
      "startOffset" : 68,
      "endOffset" : 84
    }, {
      "referenceID" : 34,
      "context" : "Furthermore, existing datasets in this domain (such as TalkSumm [39] and ScisummNet [63]) are rather small with only about 1K documents each.",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 58,
      "context" : "Furthermore, existing datasets in this domain (such as TalkSumm [39] and ScisummNet [63]) are rather small with only about 1K documents each.",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 33,
      "context" : "Learning textimage similarity has been studied in the visual-semantic embedding (VSE) literature [38, 37, 58, 25, 32, 27, 56].",
      "startOffset" : 97,
      "endOffset" : 125
    }, {
      "referenceID" : 32,
      "context" : "Learning textimage similarity has been studied in the visual-semantic embedding (VSE) literature [38, 37, 58, 25, 32, 27, 56].",
      "startOffset" : 97,
      "endOffset" : 125
    }, {
      "referenceID" : 53,
      "context" : "Learning textimage similarity has been studied in the visual-semantic embedding (VSE) literature [38, 37, 58, 25, 32, 27, 56].",
      "startOffset" : 97,
      "endOffset" : 125
    }, {
      "referenceID" : 20,
      "context" : "Learning textimage similarity has been studied in the visual-semantic embedding (VSE) literature [38, 37, 58, 25, 32, 27, 56].",
      "startOffset" : 97,
      "endOffset" : 125
    }, {
      "referenceID" : 27,
      "context" : "Learning textimage similarity has been studied in the visual-semantic embedding (VSE) literature [38, 37, 58, 25, 32, 27, 56].",
      "startOffset" : 97,
      "endOffset" : 125
    }, {
      "referenceID" : 22,
      "context" : "Learning textimage similarity has been studied in the visual-semantic embedding (VSE) literature [38, 37, 58, 25, 32, 27, 56].",
      "startOffset" : 97,
      "endOffset" : 125
    }, {
      "referenceID" : 51,
      "context" : "Learning textimage similarity has been studied in the visual-semantic embedding (VSE) literature [38, 37, 58, 25, 32, 27, 56].",
      "startOffset" : 97,
      "endOffset" : 125
    }, {
      "referenceID" : 63,
      "context" : "MultiModal Summarization with Multimodal Output (MSMO) [68, 69] applies an attention mechanism to generate a textual summary with related images for news articles.",
      "startOffset" : 55,
      "endOffset" : 63
    }, {
      "referenceID" : 64,
      "context" : "MultiModal Summarization with Multimodal Output (MSMO) [68, 69] applies an attention mechanism to generate a textual summary with related images for news articles.",
      "startOffset" : 55,
      "endOffset" : 63
    }, {
      "referenceID" : 41,
      "context" : "For each section Si, we use RoBERTa [46] to encode each of the sentences T in i,k, and then use a bidirectional GRU [17] to extract contextualized sentence embeddings X i,k:",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 12,
      "context" : "For each section Si, we use RoBERTa [46] to encode each of the sentences T in i,k, and then use a bidirectional GRU [17] to extract contextualized sentence embeddings X i,k:",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 25,
      "context" : "Similarly, for each figure F in q = {I q , C q }, we apply ResNet-152 [30] to extract the image embedding of I q and RoBERTa for the caption embedding of C q .",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 10,
      "context" : "We use GRU [15] for PT and initialize h 0 to the contextualized sentence embeddings of the first section, i.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 2,
      "context" : "2 We implement the PAR as an attention-based Seq2Seq [7] with the copy mechanism [28]:",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 23,
      "context" : "2 We implement the PAR as an attention-based Seq2Seq [7] with the copy mechanism [28]:",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 21,
      "context" : "We follow the literature on visual-semantic embedding [26, 38, 37] and learn an additional multimodal projection head δ([E u , E fig v ]) with a sigmoid activation that outputs a scalar variable in [0, 1] indicating the relevance score of text and figure embeddings.",
      "startOffset" : 54,
      "endOffset" : 66
    }, {
      "referenceID" : 33,
      "context" : "We follow the literature on visual-semantic embedding [26, 38, 37] and learn an additional multimodal projection head δ([E u , E fig v ]) with a sigmoid activation that outputs a scalar variable in [0, 1] indicating the relevance score of text and figure embeddings.",
      "startOffset" : 54,
      "endOffset" : 66
    }, {
      "referenceID" : 32,
      "context" : "We follow the literature on visual-semantic embedding [26, 38, 37] and learn an additional multimodal projection head δ([E u , E fig v ]) with a sigmoid activation that outputs a scalar variable in [0, 1] indicating the relevance score of text and figure embeddings.",
      "startOffset" : 54,
      "endOffset" : 66
    }, {
      "referenceID" : 24,
      "context" : "While there exist several solutions to bounding box regression [29, 53], we opted for the simple mean squared error (MSE) computed directly over the layout variable Lk = {l k , l y k, l w k , l h k}.",
      "startOffset" : 63,
      "endOffset" : 71
    }, {
      "referenceID" : 48,
      "context" : "While there exist several solutions to bounding box regression [29, 53], we opted for the simple mean squared error (MSE) computed directly over the layout variable Lk = {l k , l y k, l w k , l h k}.",
      "startOffset" : 63,
      "endOffset" : 71
    }, {
      "referenceID" : 56,
      "context" : "To train our model, which is a sequential prediction task, we follow the standard teacher-forcing approach [61] and provide the ground-truth results for the past prediction steps, e.",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 52,
      "context" : "For each slide deck O, we extract sentences T out using Azure OCR [1] and figures F using morphological transformation and the border following technique [57, 2].",
      "startOffset" : 154,
      "endOffset" : 161
    }, {
      "referenceID" : 41,
      "context" : "To this end, we use RoBERTa [46] to extract embeddings of the text content in each slide and the paragraphs in each section of the document.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 26,
      "context" : "We use MobileNet [31] to extract visual embeddings of all I and I and match them based on the highest cosine similarity.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 36,
      "context" : "Slide-Level ROUGE (ROUGE-SL) To measure the quality of text in the generated slides, we adapt the ROUGE score [41] widely-used in document summarization.",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 19,
      "context" : "To evaluate the layout quality, we adapt the mean intersection over union (mIoU) [24] by incorporating the LCS idea.",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 2,
      "context" : "The PAR is designed as an attention-based seq2seq model [7] with 512 hidden units.",
      "startOffset" : 56,
      "endOffset" : 59
    }, {
      "referenceID" : 16,
      "context" : "We train our network end-to-end using ADAM [21] with a learning rate of 3e-4.",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 28,
      "context" : "Qualitative examples of the generated slide deck from our model (Paper source: top [33] and bottom [11]).",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 6,
      "context" : "Qualitative examples of the generated slide deck from our model (Paper source: top [33] and bottom [11]).",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 28,
      "context" : "3 illustrates two qualitative examples (top [33] and bottom [11]) of the slide deck generText Figure Text-Figure 1 2 3 4 5 6 7",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 6,
      "context" : "3 illustrates two qualitative examples (top [33] and bottom [11]) of the slide deck generText Figure Text-Figure 1 2 3 4 5 6 7",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 41,
      "context" : "We adopt RoBERTa [46] to extract embeddings of the text in slides and sections in the document (paper).",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 41,
      "context" : "We again use RoBERTa [46] to search for the matching sentence based on the cosine similarity and build the linking for the extractive summarization.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 52,
      "context" : "We apply morphological transformation [2] and border following [57] to extract possible slide figures.",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 26,
      "context" : "We then match them with figures in the paper using the visual embedding from MobileNet [31]; if the cosine similarity is larger than the threshold θI .",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 26,
      "context" : "The lower figures are those matched from the paper using the cosine similarity and features from MobileNet [31].",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 43,
      "context" : "(from top to bottom: [48], [42], [19], [35], and [10]) Please visit our webpage for more generated slide decks from our approach.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 37,
      "context" : "(from top to bottom: [48], [42], [19], [35], and [10]) Please visit our webpage for more generated slide decks from our approach.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 14,
      "context" : "(from top to bottom: [48], [42], [19], [35], and [10]) Please visit our webpage for more generated slide decks from our approach.",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 30,
      "context" : "(from top to bottom: [48], [42], [19], [35], and [10]) Please visit our webpage for more generated slide decks from our approach.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 5,
      "context" : "(from top to bottom: [48], [42], [19], [35], and [10]) Please visit our webpage for more generated slide decks from our approach.",
      "startOffset" : 49,
      "endOffset" : 53
    } ],
    "year" : 2021,
    "abstractText" : "Creating presentation materials requires complex multimodal reasoning skills to summarize key concepts and arrange them in a logical and visually pleasing manner. Can machines learn to emulate this laborious process? We present a novel task and approach for document-to-slide generation. Solving this involves document summarization, image and text retrieval, slide structure and layout prediction to arrange key elements in a form suitable for presentation. We propose a hierarchical sequence-to-sequence approach to tackle our task in an end-to-end manner. Our approach exploits the inherent structures within documents and slides and incorporates paraphrasing and layout prediction modules to generate slides. To help accelerate research in this domain, we release a dataset about 6K paired documents and slide decks used in our experiments. We show that our approach outperforms strong baselines and produces slides with rich content and aligned imagery.",
    "creator" : "LaTeX with hyperref"
  }
}