0012e0ba095c2d75cdfc79b9c9e2a4e4a6781132	We propose a new variational EM algorithm for fitting factor analysis models with mixed continuous and categorical observations. The algorithm is based on a simple quadratic bound to the log-sum-exp function. In the special case of fully observed binary data, the bound we propose is significantly faster than previous variational methods. We show that EM is significantly more robust in the presence of missing data compared to treating the latent factors as parameters, which is the approach used by exponential family PCA and other related matrix-factorization methods. A further benefit of the variational approach is that it can easily be extended to the case of mixtures of factor analyzers, as we show. We present results on synthetic and real data sets demonstrating several desirable properties of our proposed method.
00273ef0af172a8ae02c59bf49a9b09c22cc68fa	A recent study by two prominent finance researchers, Fama and French, introduces a new framework for studying risk vs. return: the migration of stocks across size-value portfolio space. Given the financial events of 2008, this first attempt to disentangle the relationships between migration behavior and stock returns is especially timely. Their work, however, derives results only for market segments, not individual companies, and only for one-year moves. Thus, we see a new challenge for financial data mining: how to capture and categorize the migration of individual companies, and how such behavior affects their returns. We propose a novel data mining approach to study the multi-year movement of individual companies. Specifically, we address the question: “How does one discover frequent migration patterns in the stock market?” We present a new trajectory mining algorithm to discover migration motifs in financial markets. Novel features of this algorithm are its handling of approximate pattern matching through a graph theoretical method, maximal clique identification, and incorporation of temporal and spatial constraints. We have per- formed a detailed study of the NASDAQ, NYSE, and AMEX stock markets, over a 43-year span. We successfully find migration motifs that confirm existing finance theories and other motifs that may lead to new financial models.
0029ba078e2903d149dd1e7f45c3f71d36ce729b	Over the past few years Content-Centric Networking, a networking architecture in which host-to-content communication protocols are introduced, has been gaining much attention. A central component of such an architecture is a large-scale interconnected caching system. To date, the way these Cache Networks operate and perform is still poorly understood. Following the work of Cruz on queueing networks, in this paper we develop a network calculus for bounding flows in LRU cache networks of arbitrary topology. We analyze the tightness of these bounds as a function of several system parameters. Also, we derive from it several analytical results regarding these systems: the uniformizing impact of LRU on the request stream, and the significance of cache and routing diversity on performance.
004af6a1df10947ffa3eec7ceb0687c6c79db0a3	Today’s complex design processes feature large numbers of varied, interdependent constraints, which often cross interdisciplinary boundaries. Therefore, a computersupported constraint management methodology that automatically detects violations early in the design process, provides useful violation notification to guide redesign efforts, and can be integrated with conventional CAD software can be a great aid to the designer. We presentsuch a methodology and describe its implementation in the Minerva II design process manager, along with an example design session.
0062526e2c87d1fd47388c13050cbc2871f766db	Malware poses a serious threat to Android smartphones. Current security mechanisms offer poor protection and are often too inflexible to quickly mitigate new exploits. In this paper we present FireDroid, a policy-based framework for enforcing security policies by interleaving process system calls. The main advantage of FireDroid is that it is completely transparent to the applications as well as to the Android OS. FireDroid enforces security policies without modifying either the Android OS or its applications. FireDroid is able to perform security checks on third-party and pre-installed applications, as well as malicious native code. We have implemented a novel mechanism that is able to attach, identify, monitor and enforce polices for any process spawned by the Android’s mother process Zygote. We have tested the effectiveness of FireDroid against real malware. Moreover, we show how FireDroid can be used as a swift solution for blocking OS and application vulnerabilities before patches are available. Finally, we provide an experimental evaluation of our approach showing that it has only a limited overhead.  Given these facts, FireDroid represents a practical solution for strengthening security on Android smartphones.
00e17bd820ffdddfeca041f31ae1691d18f6970c	The rapid growth in the development of healthcare information systems has led to an increased interest in utilizing the patient Electronic Health Records (EHR) for assisting disease diagnosis and phenotyping. The patient EHRs are generally longitudinal and naturally represented as medical event sequences, where the events include clinical notes, problems, medications, vital signs, laboratory reports, etc.  The longitudinal and heterogeneous properties make EHR analysis an inherently difficult challenge. To address this challenge, in this paper, we develop a novel representation, namely the temporal graph, for such event sequences. The temporal graph is informative for a variety of challenging analytic tasks, such as predictive modeling, since it can capture temporal relationships of the medical events in each event sequence. By summarizing the longitudinal data, the temporal graphs are also robust and resistant to noisy and irregular observations. Based on the temporal graph representation, we further develop an approach for temporal phenotyping to identify the most significant and interpretable graph basis as phenotypes. This helps us better understand the disease evolving patterns. Moreover, by expressing the temporal graphs with the phenotypes, the expressing coef- ficients can be used for applications such as personalized medicine, disease diagnosis, and patient segmentation. Our temporal phenotyping framework is also flexible to incorporate semi-supervised/supervised information. Finally, we validate our framework on two real-world tasks. One is predicting the onset risk of heart failure. Another is predicting the risk of heart failure related hospitalization for patients with COPD pre-condition. Our results show that the diagnosis performance in both tasks can be improved signifi- cantly by the proposed approaches. Also, we illustrate some interesting phenotypes derived from the data.
0114fb72afbd9cc0bca35940beb21eda596aa5e0	The wide availability and the Single-Instruction Multiple-Thread (SIMT)-style programming model have made graphics processing units (GPUs) a promising choice for high performance computing. However, because of the SIMT style processing, an instruction will be executed in every thread even if the operands are identical for all the threads. To overcome this inefficiency, the AMD’s latest Graphics Core Next (GCN) architecture integrates a scalar unit into a SIMT unit. In GCN, both the SIMT unit and the scalar unit share a single SIMTstyle instruction stream. Depending on its type, an instruction is issued to either a scalar or a SIMT unit. In this paper, we propose to extend the scalar unit so that it can either share the instruction stream with the SIMT unit or execute a separate instruction stream. The program to be executed by the scalar unit is referred to as a scalar program and its purpose is to assist SIMT-unit execution. The scalar programs are either generated from SIMT programs automatically by the compiler or manually developed by expert developers. We make a case for our proposed flexible scalar unit through three collaborative execution paradigms: data prefetching, control divergence elimination, and scalar-workload extraction. Our experimental results show that significant performance gains can be achieved using our proposed approaches compared to the state-of-art SIMT style processing.
01ce0903206717ac40f9a26ce9478bdeff5c1262	In this paper, we propose modulation diversity techniques for Spatial Modulation (SM) system using Complex Interleaved Orthogonal Design (CIOD) meant for two transmit antennas. Specifically, we show that by using the CIOD for two transmit antenna system, the standard SM scheme, where only one transmit antenna is activated in any symbol duration, can achieve a transmit diversity order of two. We show with our simulation results that the proposed schemes offer transmit diversity order of two, and hence, give a better Symbol Error Rate performance than the SM scheme with transmit diversity order of one.
01eb1294d629e348673ad5212fc5eda61dbc8a27	Mathematical morphology is broadly used in image processing, but it is mainly restricted to binary or greyscale images. Extension to color images is not straightforward due to the need of application to an ordered space with an infimum and a supremum. In this paper a new approach for the ordering of the RGB space is presented. The adaptation of a linear growing self-organizing network to the three-dimensional color space allows the definition of an order relationship among colors. This adaptation is measured with the topographic product to guarantee a good topology-preservation of the RGB space. Once an order has been established, several examples of application of mathematical morphology operations to color images are presented.
0202a2cfc2a3e3ae82ea69781be100309f2325be	Spontaneous multi-party interaction – conversation among groups of three or more participants – is part of daily life. While automated modeling of such interactions has received increased attention in ubiquitous computing research, there is little applied research on the organization of this highly dynamic and spontaneous sociable interaction within small groups. We report here on an applied conversation analytic study of small-group sociable talk, emphasizing structural and temporal aspects that can inform computational models. In particular, we examine the mechanics of multiple simultaneous conversational floors – how participants initiate a new floor amidst an on-going floor, and how they subsequently show their affiliation with one floor over another. We also discuss the implications of these findings for the design of “smart” multi-party applications.
024a6b8981750bbce913c8a8d13609be255944fd	A central problem for NLP is grammar induction: the development of unsupervised learning algorithms for syntax. In this paper we present a lattice-theoretic representation for natural language syntax, called Distributional Lattice Grammars. These representations are objective or empiricist, based on a generalisation of distributional learning, and are capable of representing all regular languages, some but not all context-free languages and some noncontext-free languages. We present a simple algorithm for learning these grammars together with a complete self-contained proof of the correctness and efficiency of the algorithm.
027fcf15d4741bde974a127207f28b031536eb1d	We propose two declarative debuggers of missing answers with respect to C- and S-semantics. The debuggers are proved correct for every logic program. Moreover, they are complete and terminating with respect to a large class of programs, namely acceptable logic programs.  The debuggers enhance existing proposals, which suer from a problem due to the implementation of negation as failure. The proposed solution exploits decision procedures for C- and S-semantics introduced in [9].
030cadedef2370bd296af07fc3324c6bb8409ba5	We propose a hull operator, the reflex-free hull, that allows us to define a 3D analogue to bays in polygons. The reflex-free hull allows a rich set of topological types, yet for polyhedral input with n edges, it remains a polyhedral set with O(n) edges. This is in contrast to other possible hull definitions that give non-planar surfaces and higher combinatorial complexity. The reflex-free hull is related to identifying cavities in computer aided design and manufacturing, but we sketch examples to indicate that computing a reflex-free hull will be a challenging problem.
03617650a45a0a29590418d09390a36de3060569	Statistical parsers have become increasingly accurate, to the point where they are useful in many natural language applications. However, estimating parsing accuracy on a wide variety of domains and genres is still a challenge in the absence of gold-standard parse trees.  In this paper, we propose a technique that automatically takes into account certain characteristics of the domains of interest, and accurately predicts parser performance on data from these new domains. As a result, we have a cheap (no annotation involved) and effective recipe for measuring the performance of a statistical parser on any given domain.
03b31204f132962ac46a062fd7effeef8d90db6b	Distributed Video Coding (DVC) is an increasingly popular approach among the researchers in video coding during past few years due to its attractive and promising features. In DVC, the majority of the computational complexity has been shifted from encoder to the decoder in comparison to its conventional counterparts, including MPEG and H.26x enabling a dramatically low cost encoder implementation. Side information generation, carried out at the decoder, is a major function in the DVC coding algorithm and plays a key-role in determining the performance of the codec. In this paper, a novel iterative refinement technique is proposed for the side information generation process.  Simulation results of the proposed technique depict a consistent improvement in performance in comparison to the state-of-the-art in pixel domain DVC.
03f89354a1589885fd600e9dc996bd7c6e1a6515	This paper presents a framework for investigating the relationship between both the auditory and visual modalities in speech. This framework employs intentional agents to analyse multilinear bimodal representations of speech utterances in line with an extended computational phonological model.
042084570ea390686fbe62a00ef59282da67419c	Intelligent user interfaces are characterised by their capability to adapt at run-time and make several communication decisions concerning ‘what’, ‘when’, ‘why’ and ‘how’ to communicate, through a certain adaptation strategy. In this paper, we present a methodological approach to assist this decision making process, which is btyed on a clear separation of the important attributes that characterise the adaptation strategy, namely the adaptation determinants, constituents, goals and rules. Based on this separation, we also present a methodological approach for the formulation of adaptation rules, which utilises techniques from the domain of multiple criteria decision making. It is argued that, following the proposed approach, the adaptation strategy can be easily customised to the requirements of different application domains and user groups, and can be re-used with minor modifications in different applications. As a result, developers of intelligent user interfaces can be significantly assisted, and users can be empowered to exploit the benefits of intelligent interfaces.
05737e80116195486970bad275fe7a958d4a4bde	In designing software systems, security is typically only one design objective among many. It may compete with other objectives such as functionality, usability, and performance. Too often, security mechanisms such as firewalls, access control, or encryption are adopted without explicit recognition of competing design objectives and their origins in stakeholder interests. Recently, there is increasing acknowledgement that security is ultimately about trade-offs. One can only aim for “good enough” security, given the competing demands from many parties. In this paper, we examine how conceptual modeling can provide explicit and systematic support for analyzing security trade-offs. After considering the desirable criteria for conceptual modeling methods, we examine several existing approaches for dealing with security trade-offs. From analyzing the limitations of existing methods, we propose an extension to the i* framework for security trade-off analysis, taking advantage of its multi-agent and goal orientation. The method was applied to several case studies used to exemplify existing approaches.
05d1cee851e2d900ab78b3542237af4db84590fd	A class of queueing networks which consist of single-server fork-join nodes with infinite buffers is examined to derive a representation of the network dynamics in terms of max-plus algebra. For the networks, we present a common dynamic state equation which relates the departure epochs of customers from the network nodes in an explicit vector form determined by a state transition matrix. We show how the matrix may be calculated from the service time of customers in the general case, and give examples of matrices inherent in particular networks.
05f0324f90fe684926fa36a701fbc6b5045af898	Labeled data is not readily available for many natural language domains, and it typically requires expensive human effort with considerable domain knowledge to produce a set of labeled data. In this paper, we propose a simple unsupervised system that helps us create a labeled resource for categorical data (e.g., a document set) using only fifteen minutes of human input.  We utilize the labeled resources to discover important insights about the data.  The entire process is domain independent, and demands no prior annotation samples, or rules specific to an annotation.
06fdfa00ea6a51aafa8fe305d2d69f4b46973892	The ability to determine what day-to-day activity (such as cooking pasta, taking a pill, or watching a video) a person is performing is of interest in many application domains. A system that can do this requires models of the activities of interest, but model construction does not scale well: humans must specify lowlevel details, such as segmentation and feature selection of sensor data, and high-level structure, such as spatio-temporal relations between states of the model, for each and every activity. As a result, previous practical activity recognition systems have been content to model a tiny fraction of the thousands of human activities that are potentially useful to detect. In this paper, we present an approach to sensing and modeling activities that scales to a much larger class of activities than before. We show how a new class of sensors, based on Radio Frequency Identification (RFID) tags, can directly yield semantic terms that describe the state of the physical world. These sensors allow us to formulate activity models by translating labeled activities, such as “cooking pasta”, into probabilistic collections of object terms, such as “pot”. Given this view of activity models as text translations, we show how to mine definitions of activities in an unsupervised manner from the web. We have used our technique to mine definitions for over 20,000 activities. We experimentally validate our approach using data gathered from actual human activity as well as simulated data.
072cae4f4da9d9e810fcb30899a52954c04ccab7	As IP technologies providing both tremendous capacity and the ability to establish dynamic secure associations between endpoints emerge, Virtual Private Networks (VPNs) are going through dramatic growth. The number of endpoints per VPN is growing and the communication pattern between endpoints is becoming increasingly hard to forecast. Consequently, users are demanding dependable, dynamic connectivity between endpoints, with the network expected to accommodate any traffic matrix, as long as the traffic to the endpoints does not overwhelm the rates of the respective ingress and egress links. We propose a new service interface, termed a hose, to provide the appropriate performance abstraction. A hose is characterized by the aggregate traffic to and from one endpoint in the VPN to the set of other endpoints in the VPN, and by an associated performance guarantee.  Hoses provide important advantages to a VPN customer: (i) flexibility to send traffic to a set of endpoints without having to specify the detailed traffic matrix, and (ii) reduction in the size of access links through multiplexing gains obtained from the natural aggregation of the flows between endpoints. As compared with the conventional point to point (or customer-pipe) model for managing &OS, hoses provide reduction in the state information a customer must maintain. On the other hand, hoses would appear to increase the complexity of the already difficult problem of resource management to support &OS. To manage network resources in the face of this increased uncertainty, we consider both conventional statistical multiplexing techniques, and a new resiring technique based on online measurements.  To study these performance issues, we run trace driven simulations, using traffic derived from AT&T’s voice network, and from a large corporate data network. From the customer’s perspective, we fmd that aggregation of traffic at the hose level provides significant multiplexing gains. From the provider’s perspective, we find that the statistical multiplexing and resizing techniques deal effectively with uncertainties about the traffic, providing significant gains over the conventional alternative of a mesh of statically sized customer-pipes between endpoints.
075fe9b777adae7e59e125283ce3960f38e7257c	The semantics of process calculi has traditionally been specified by labelled transition systems (LTS), but with the development of name calculi it turned out that reaction rules (i.e., unlabelled transition rules) are often more natural.  This leads to the question of how behavioural equivalences (bisimilarity, trace equivalence, etc.) defined for LTS can be transferred to unlabelled transition systems.  Recently, in order to answer this question, several proposals have been made with the aim of automatically deriving an LTS from reaction rules in such a way that the resulting equivalences are congruences. Furthermore these equivalences should agree with the intended semantics, whenever one exists.  In this paper we propose saturated semantics, based on a weaker notion of observation and orthogonal to all the previous proposals, and we demonstrate the appropriateness of our semantics by means of two examples: logic programming and a subset of the open π-calculus. Indeed, we prove that our equivalences are congruences and that they coincide with logical equivalence and open bisimilarity respectively, while equivalences studied in previous works are strictly finer.
07c09fd623e4f93752f8809e1ae0b02b9c251ca0	This paper describes a two-part study of animated affective agents that varied by affective state (positive or evasive) and motivational support (present or absent). In the first study, all four conditions significantly improved learning; however, only three conditions significantly improved math self-efficacy, the exception being the animated agent with evasive emotion and no motivational support. To help in interpreting these unexpected results, the second study used a phenomenological approach to gain an understanding of learner perceptions, emotions, interaction patterns, and expectations regarding the roles of agent affective state and motivational support during the learning process. From the qualitative data emerged three overall themes important to learners during the learning process: learner perceptions of the agent, learner perceptions of self, and learner-agent social interaction. This paper describes the results of the phenomenological study and discusses the findings with recommendations for future research.
08245b3e5e1ee8d52d3311e22f8def725f3cd1c6	We introduce the Boom Chameleon, a novel input/output device consisting of a flat-panel display mounted on a tracked mechanical boom. The display acts as a physical window into 3D virtual environments, through which a one-to-one mapping between real and virtual space is preserved. The Boom Chameleon is further augmented with a touch-screen and a microphone/speaker combination. We present a 3D annotation application that exploits this unique configuration in order to simultaneously capture viewpoint, voice and gesture information. Design issues are discussed and results of an informal user study on the device and annotation software are presented. The results show that the Boom Chameleon annotation facilities have the potential to be an effective, easy to learn and operate 3D design review system.
0846a100fcdf2406ec38df867b9ec835e1ebc1e0	While Grammar Inference (GI) has been successfully applied to many diverse domains such as speech recognition and robotics, its application to software engineering has been limited, despite wide use of context-free grammars in software systems. This paper reports current developments and future directions in the applicability of GI to software engineering, where GI is seen to offer innovative solutions to the problems of inference of domain-specific language (DSL) specifications from example DSL programs and recovery of metamodels from instance models.
0876622f6c7a068794d78aabdddf1e173137aa49	There is a large body of work on using noisy communication channels for realizing different cryptographic tasks. In particular, it is known that secure message transmission can be achieved unconditionally using only one-way communication from the sender to the receiver. In contrast, known solutions for more general secure computation tasks inherently require interaction, even when the entire input originates from the sender.  We initiate a general study of cryptographic protocols over noisy channels in a setting where only one party speaks. In this setting, we show that the landscape of what a channel is useful for is much richer. Concretely, we obtain the following results.  • Relationships between channels. The binary erasure channel (BEC) and the binary symmetric channel (BSC), which are known to be securely reducible to each other in the interactive setting, turn out to be qualitatively different in the setting of one-way communication. In particular, a BEC cannot be implemented from a BSC, and while the erasure probability of a BEC can be manipulated in both directions, the crossover probability of a BSC can only be manipulated in one direction.  • Zero-knowledge proofs and secure computation of deterministic functions. One-way communication over BEC or BSC is sufficient for securely realizing any deterministic (possibly reactive) functionality which takes its inputs from a sender and delivers its outputs to a receiver. This provides the first truly non-interactive solutions to the problem of zero-knowledge proofs.  • Secure computation of randomized functions. One-way communication over BEC or BSC cannot be used for realizing general randomized functionalities which take input from a sender and deliver output to a receiver. On the other hand, one-way communication over other natural channels, such as bursty erasure channels, can be used to realize such functionalities. This type of protocols can be used for distributing certified cryptographic keys without revealing the keys to the certification authority.
089ad394f9e0463c2f12e3d3cfbe8485485e6897	We present a cortical surface registration method that simultaneously aligns sulcal landmarks and parameterizes two cortical surfaces. The approach is based on coregistration of cortical surface coordinate systems so that the labeled sulcal features share the same coordinates on both cortical surfaces. We model the cortex as an elastic sheet and solve the associated Cauchy-Navier equilibrium equation subject to sulcal alignment constraints. The elastic energy is computed directly with respect to the intrinsic surface geometry and discretized using a finite element method on triangular tessellations of the two surfaces. In contrast to alternative methods for cortical alignment, the method avoids the need for an intermediate flat space for sulcal landmark matching and provides a fast, accurate and inverse-consistent surface registration and parameterization for inter-subject neuroanatomical studies.
08b8d9c9576a02c6efc5e8bf70b7ad6972973cda	We analyze extensively the temporal properties of the train of spikes emitted by a simple model neuron as a function of the statistics of the synaptic input. In particular we focus on the asynchronous case, in which the synaptic inputs are random and uncorrelated. We show that the NMDA component acts as a non-stationary input that varies on longer time scales than the inter-spike intervals. In the subthreshold regime, this can increase dramatically the coefficient of variability (bringing it beyond one). The analysis provides also simple guidelines for searching parameters that maximize irregularity.
08bc6fbb08945c35ec8e989df0ce05c98b09668b	This paper describes the implementation and performance of M-VIA on the AceNIC Gigabit Ethernet card. The AceNIC adapter has several notable hardware features for high-speed communication, such as jumbo frames and interrupt coalescing. The M-VIA performance characteristics were measured and evaluated based on these hardware features.  Our results show that latency and bandwidth improvement can be obtained when the M-VIA data segmentation size is properly adjusted to utilize the AceNIC’s jumbo frame feature. The M-VIA data segmentation size of 4,096 bytes with MTU size of 4,138 bytes showed the best performance. However, larger MTU sizes did not necessarily result in better performance due to extra segmentation and DMA setup overhead.  In addition, the cost of M-VIA interrupt handling can be reduced with AceNIC’s hardware interrupt coalescing. When the parameters for the hardware interrupt coalescing were properly adjusted, the latency of interrupt handling was reduced by up to 170 µs.
08f624f7ee5c3b05b1b604357fb1532241e208db	We propose an algorithm for semantic segmentation based on 3D point clouds derived from ego-motion. We motivate five simple cues designed to model specific patterns of motion and 3D world structure that vary with object category. We introduce features that project the 3D cues back to the 2D image plane while modeling spatial layout and context. A randomized decision forest combines many such features to achieve a coherent 2D segmentation and recognize the object categories present. Our main contribution is to show how semantic segmentation is possible based solely on motion-derived 3D world structure. Our method works well on sparse, noisy point clouds, and unlike existing approaches, does not need appearance-based descriptors.  Experiments were performed on a challenging new video database containing sequences filmed from a moving car in daylight and at dusk. The results confirm that indeed, accurate segmentation and recognition are possible using only motion and 3D world structure. Further, we show that the motion-derived information complements an existing state-of-the-art appearance-based method, improving both qualitative and quantitative performance.
091621da1f4373b6d6f20a5024b729c43a1ae986	Measuring the similarity of curves is a fundamental problem arising in many application fields. There has been considerable interest in several such measures, both in Euclidean space and in more general setting such as curves on Riemannian surfaces or curves in the plane minus a set of obstacles. However, so far, efficiently computable similarity measures for curves on general surfaces remain elusive. This paper aims at developing a natural curve similarity measure that can be easily extended and computed for curves on general orientable 2-manifolds. Specifically, we measure similarity between homotopic curves based on how hard it is to deform one curve into the other one continuously, and define this “hardness” as the minimum possible surface area swept by a homotopy between the curves. We consider cases where curves are embedded in the plane or on a triangulated orientable surface with genus g, and we present efficient algorithms (which are either quadratic or near linear time, depending on the setting) for both cases.
09437d40dc213ba1a942b233c37d447337f5c056	Simultaneous registration and segmentation (SRS) provides a powerful framework for tracking an object of interest in an image sequence. The state-of-the-art SRSbased tracking methods assume that the illumination is maintained constant across consecutive frames. However, this assumption does not hold in many natural image sequences due to dynamic light source and shadows. We propose a generalized model for SRS-based tracking in this paper to account for non-uniform additive illumination changes. More specifically, we introduce two new terms in the SRS energy functional which address the above mentioned problem. The first term couples the shape-based cue and intensity-based cue to establish a correspondence between them. The second term compensates for the illumination change which is complementary to the first term. We demonstrate that the proposed SRS energy functional yields superior performance over the state-of-the-art SRS-based methods for various indoor and outdoor image sequences.
09e88460a9379a6cd979b3954f8c4d3bbb9de0dc	Abductive inference is an important AI reasoning technique to find explanations of observations, and has recently been applied to scientific discovery.  To find best hypotheses among many logically possible hypotheses, we need to evaluate hypotheses obtained from the process of hypothesis generation.  We propose an abductive inference architecture combined with an EM algorithm working on binary decision diagrams (BDDs). This work opens a way of applying BDDs to compress multiple hypotheses and to select most probable ones from them. An implemented system has been applied to inference of inhibition in metabolic pathways in the domain of systems biology.
0a395aa2965bcfeaa860c967971e5cb6c89bc035	In this paper we present an algorithm to compute set-point (i.e. a position to be reached by the robot) automatically from conic features virtually placed by an operator onto the object. We then use a visual servoing algorithm to guide the gripper to its final position. We have tested this algorithm in trying to open a valve with a 6 degree of freedom robot arm, using only visual information and without any model of the valve.
0a86a047d18cd011dd8df23aad5bb42cf99cc941	Multi-doument discourse analysis has emerged with the potential of improving various information retrieval applications.  Based on the newly proposed Cross-document Structure Theory (CST), this paper describes an empirical study that uses boosting to classify CST relationships between sentence pairs extracted from topically related documents. We show that the binary classifier for determining existence of structural relationships significantly outperforms the baseline.  We also achieve promising results on the multi-class case in which the full taxonomy of relationships are considered.
0ab9dc7290e2ba31028a535d19b66dc02aa492c5	A new face recognition algorithm is presented. It supposes that a video sequence of a person is available both at enrollment and test time. During enrollment, a client Gaussian Mixture Model (GMM) is adapted from a world GMM using eigenface features extracted from each frame of the video. Then, a Support Vector Machine (SVM) is used to find a decision border between the client GMM and pseudoimpostors GMMs. At test time, a GMM is adapted from the test video and a decision is taken using the previously learned client SVM. This algorithm brings a 3.5% Equal Error Rate (EER) improvement over the BioSecure reference system on the Pooled protocol of the BANCA database.
0ad6ed8cc5e5a3d26e047f0c9478fd06f327da3b	The object-oriented modeling has been largely adopted in industry in the last years. Several systems built 4 or 5 years ago may need an adaptive maintenance process in order to better satisfy market and customer needs. In this paper1, a model for effort estimation/prediction of the adaptive maintenance is presented. A selection of metrics for effort estimation has been applied to the general model for evaluating maintenance effort. The metrics presented have been validated against real data. The validation presented has shown that some metrics that can be profitably employed for effort estimation/prediction can be also used with success for the estimation/prediction of the maintenance effort.  Moreover, the results obtained gives some guidelines to maintain under control relevant factors for the adaptive maintenance.
0b225be92ba6aeb9b417eebf63f0e4a33c5cfe27	We present a SAT-based approach to the task and message allocation problem of distributed real-time systems with hierarchical architectures. In contrast to the heuristic approaches usually applied to this problem, our approach is guaranteed to find an optimal allocation for realistic task systems running on complex target architectures.  Our method is based on the transformation of such scheduling problems into nonlinear integer optimization problems.  The core of the numerical optimization procedure we use to discharge those problems is a solver for arbitrary Boolean combinations of integer constraints. Optimal solutions are obtained by imposing a binary search scheme on top of that solver. Experiments show the applicability of our approach to industrial-size task systems, which are mapped to heterogeneous hierarchical hardware architectures.
0b824c4ffe362bf2f8987232c9b5a8eb72f96f3b	In the database community, work on information extraction (IE) has centered on two themes: how to effectively manage IE tasks, and how to manage the uncertainties that arise in the IE process in a scalable manner. Recent work has proposed a probabilistic database (PDB) based declarative IE system that supports a leading statistical IE model, and an associated inference algorithm to answer top-k-style queries over the probabilistic IE outcome. Still, the broader problem of effectively supporting general probabilistic inference inside a PDB-based declarative IE system remains open.  In this paper, we explore the in-database implementations of a wide variety of inference algorithms suited to IE, including two Markov chain Monte Carlo algorithms, the Viterbi and the sum-product algorithms.  We describe the rules for choosing appropriate inference algorithms based on the model, the query and the text, considering the trade-off between accuracy and runtime. Based on these rules, we describe a hybrid approach to optimize the execution of a single probabilistic IE query to employ different inference algorithms appropriate for different records. We show that our techniques can achieve up to 10-fold speedups compared to the non-hybrid solutions proposed in the literature.
0b8ecf769bfefc5daa0ca2cc2c8b097bf34c90a6	AdaBoost.OC has shown to be an effective method in boosting “weak” binary classifiers for multi-class learning. It employs the Error Correcting Output Code (ECOC) method to convert a multi-class learning problem into a set of binary classification problems, and applies the AdaBoost algorithm to solve them efficiently. In this paper, we propose a new boosting algorithm that improves the AdaBoost.OC algorithm in two aspects: 1) It introduces a smoothing mechanism into the boosting algorithm to alleviate the potential overfitting problem with the AdaBoost algorithm, and 2) It introduces a probabilistic coding scheme to generate binary codes for multiple classes such that training errors can be efficiently reduced. Empirical studies with seven UCI datasets have indicated that the proposed boosting algorithm is more robust and effective than the AdaBoost.OC algorithm for multi-class learning.
0c45f23551a88db06af891dcdc633317fa64bc88	A computational model of visual attention using two visual inferences is proposed. The dominant depth and the horizon line position are inferred from low-level visual features. This prior knowledge helps to find salient areas on still color pictures. Regarding the dominant depth, the idea is to favor the lowest spatial frequencies on close-up scenes whereas the highest spatial frequencies are used to predict salient areas on panoramic view. Some studies showed that the horizon line is a natural attractor of our gaze.  Horizon detection is then used to improve the saliency prediction. Results show that the proposed model outperforms existing approaches. However, the dominant depth does not bring any gain in the saliency prediction.
0cb49416099ba101c5fcb3eb6c957d45e106195a	Tools for analysing secure information flow are almost exclusively based on ideas going back to Denning’s work from the 70’s. This approach embodies an imperfect notion of security which turns a blind eye to information flows which are encoded in the termination behaviour of a program. In exchange for this weakness many more programs are deemed ”secure”, using conditions which are easy to check. Previously it was thought that such leaks are limited to at most one bit per run. Recent work by Askarov et al (ESORICS’08) offers some bad news and some good news: the bad news is that for programs which perform output, the amount of information leaked by a Denning style analysis is not bounded; the good news is that if secrets are chosen to be sufficiently large and sufficiently random then they cannot be effectively leaked at all. The problem addressed in this paper is that secrets cannot always be made sufficiently large or sufficiently random. Contrast, for example, an encryption key with an “hasHIV”-field of a patient record. In recognition of this we develop a notion of secret-sensitive noninterference in which “small” secrets are handled more carefully than “big” ones. We illustrate the idea with a type system which combines a liberal Denning-style analysis with a more restrictive system according to the nature of the secrets at hand.
0cc37453fbc8297cc797bba5fb79abb2f0d1eded	A progressive and scalable, region of interest (ROI) image coding scheme based on matching pursuits (MP) is presented. Matching pursuit is a multi-resolutional signal analysis tool and can be employed in order to progressively refine the quality of a set of selected regions of an image up to a specific grade. The computational complexity of this analysis method can be reduced by decreasing the size of MP dictionary.  Thus, the proposed method provides a trade off between complexity, rate, and quality. By the suggested scheme, regions of an image with higher receiver’s priority are refined in an interactive manner. The transmitter sends an initial coarse version of the image. Then, he receiver transmits its preferred ROI parameters. Afterwards, the reconstructed image is refined according to the ROI parameters, in a progressive way.
0d07b9746c3862b6263037ee405e4f7fcd6d51f6	We consider the offline and online versions of a bin packing problem called bin packing with conflicts. Given a set of items V = {1, 2, . . . , n} with sizes s1, s2 . . . , sn ∈ [0, 1] and a conflict graph G = (V, E), the goal is to find a partition of the items into independent sets of G, where the total size of each independent set is at most one, so that the number of independent sets in the partition is minimized. This problem is clearly a generalization of both the classical (one-dimensional) bin packing problem where E = ∅ and of the graph coloring problem where si = 0 for all i = 1, 2, . . . , n. Since coloring problems on general graphs are hard to approximate, following previous work, we study the problem on specific graph classes. For the offline version we design improved approximation algorithms for perfect graphs and other special classes of graphs, these are a 5 2 = 2.5-approximation algorithm for perfect graphs, a 7 3 ≈ 2.33333-approximation for a sub-class of perfect graphs, which contains interval graphs, and a 7 4 = 1.75-approximation for bipartite graphs. For the online problem on interval graphs, we design a 4.7-competitive algorithm and show a lower bound of 155 36 ≈ 4.30556 on the competitive ratio of any algorithm. To derive the last lower bound, we introduce the first lower bound on the asymptotic competitive ratio of any online bin packing algorithm with known optimal value, which is 47 36 ≈ 1.30556.
0dda151030cd64198497ddfefd324fc1b57779d1	Reasoning about actions has been a focus of interest in Al from the beginning and continues to receive attention. Rut the range of situations considered has been rather narrow and falls well short of what is needed for understanding natural language. Language understanding requires sophisticated reasoning about actions and events and the world's languages employ a variety of grammatical and lexical devices to construe, direct attention and focus on, and control inferences about actions and events. We implemented a neurally inspired computational model that is able to reason about, linguistic action and event descriptions, such as those found in news stories. The system uses an active.  event representation that also seems to provide natural and cognitiveIy motivated solutions to classical problems in logical theories of reasoning about actions. For logical approaches to reasoning about actions, we suggest, that looking at story understanding sets up fairly strong desiderata both in terms of the fine-grained event and action distinctions and the kinds of real-time inferences required.
0df395aa3e8c3ab10be1fee425b02782ed575d29	We explore the phenomena of subjective randomness as a case study in understanding how people discover structure embedded in noise. We present a rational account of randomness perception based on the statistical problem of model selection: given a stimulus, inferring whether the process that generated it was random or regular. Inspired by the mathematical definition of randomness given by Kolmogorov complexity, we characterize regularity in terms of a hierarchy of automata that augment a finite controller with different forms of memory. We find that the regularities detected in binary sequences depend upon presentation format, and that the kinds of automata that can identify these regularities are informative about the cognitive processes engaged by different formats.
0e43f60727382032a620d3c08694c0a104005de3	We propose Oblivious Attribute Certificates (OACerts), an attribute certificate scheme in which a certificate holder can select which attributes to use and how to use them. In particular, a user can use attribute values stored in an OACert obliviously, i.e., the user obtains a service if and only if the attribute values satisfy the policy of the service provider, yet the service provider learns nothing about these attribute values. This way, the service provider’s access control policy is enforced in an oblivious fashion. To enable the oblivious access control using OACerts, we propose a new cryptographic primitive called Oblivious Commitment-Based Envelope (OCBE). In an OCBE scheme, Bob has an attribute value committed to Alice and Alice runs a protocol with Bob to send an envelope (encrypted message) to Bob such that: (1) Bob can open the envelope if and only if his committed attribute value satisfies a predicate chosen by Alice, (2) Alice learns nothing about Bob’s attribute value. We develop provably secure and efficient OCBE protocols for the Pedersen commitment scheme and comparison predicates as well as logical combinations of them.
0e51c3ca87748c21a21bd56b49ec9bbbde143d9e	A novel component-based, service-oriented framework for distributed metacomputing is described. Adopting a provider-centric view of resource sharing, this project emphasizes lightweight software infrastructures that maintain minimal state, and interface to current and emerging distributed computing standards. Resource owners host a software backplane onto which owners, clients, or third-party resellers may load components or component-suites that deliver value added services without compromising owner security or control. Standards-based descriptions of services facilitate publication and discovery via established schemes. The architecture of the container framework, design of components, security and access control schemes, and preliminary experiences are described in this paper.
0e59c60eaee54e0db8171b5a2be12833c5cdf2ca	A challenging problem in machine learning, information retrieval and computer vision research is how to recover a low-rank representation of the given data in the presence of outliers and missing entries. The L1-norm low-rank matrix factorization (LRMF) has been a popular approach to solving this problem. However, L1-norm LRMF is difficult to achieve due to its non-convexity and non-smoothness, and existing methods are often inefficient and fail to converge to a desired solution. In this paper we propose a novel cyclic weighted median (CWM) method, which is intrinsically a coordinate decent algorithm, for L1-norm LRMF. The CWM method minimizes the objective by solving a sequence of scalar minimization sub-problems, each of which is convex and can be easily solved by the weighted median filter. The extensive experimental results validate that the CWM method outperforms state-of-the-arts in terms of both accuracy and computational efficiency.
0e74a1690b540a45f864f933d89a1273446710ff	Various relevance feedback algorithms have been proposed in recent years in the area of content-based image retrieval. This paper gives a brief review and analysis on existing techniques—from early heuristic-based feature weighting schemes to recently proposed optimal learning algorithms. In addition, the kernel-based biased discriminant analysis (KBDA) is proposed to fit the unique nature of relevance feedback as a biased classification problem. As a novel variant of traditional discriminant analysis, the proposed algorithm provides a trade-off between discriminant transform and regression. The kernel form is derived to deal with non-linearity in an elegant way. Experimental results indicate that significant improvement in retrieval performance is achieved by the new scheme.
0e797d2c7163a77106bc0f4ab1cea76bb86cfd92	The next generation of computing systems will be embedded, in a virtually unbounded number, and dynamically connected. Although these systems will penetrate every possible domain of our daily life, the expectation is that they will operate outside our normal cognizance, requiring far less attention from the human users than the desktop computers today. The networked embedded computing era will challenge our ways of thinking and computing far more than the PC revolution did in the past. The current software and network architectures and their associated programming models were not designed for these scenarios. Traditional parallel and distributed computing models are based on a distribution of tasks across a stable cluster of similar processing units. In networks of embedded systems however, nodes have properties such as location or functionality that make them only partially substitutable in a specific task. Tasks need to execute on specific nodes to achieve prescribed objectives, necessitating the location of target nodes in a manner that allows partial substitution. We propose a computing model and a system architecture for distributed embedded systems where nodes “cooperate” by providing their computing and communication resources to distributed tasks. The system architecture for cooperative computing is based on Smart Messages (SM), which can be viewed as intelligent carriers of data in a network. Smart Messages are collections of code and mobile data that migrate through the network, a single network hop at a time, executing at each step. Smart Messages are responsible for their own routing. To validate the model we have implemented two previously proposed applications, Directed Diffusion and SPIN, for data collection and data dissemination in sensor networks. We have developed a simulator that executes Cooperative Computing applications and allows us to evaluate the performances by measuring both execution and communication time.
0e9806432739fb799a32acd49853d37c324bac03	We present a method for fully automatic 3D reconstruction from a pair of uncalibrated images in order to deal with the modeling of complex rigid scenes. A 2D triangular mesh model of the scene is calculated using a two-step algorithm mixing sparse matching and dense motion estimation approaches. The 2D mesh is iteratively refined to fit any arbitrary 3D surface. At convergence, each triangular patch corresponds to the pro jection of a 3D plane. The algorithm proposed here relies first on a dense disparity field. The dense field estimation modelized within a robust framework is constrained by the epipolar geometry. The resulting field is then segmented according to homographic models using iterative Delaunay triangulation. In association with a simplified self-calibration algorithm, this 2D planar model is used to obtain a VRML-compatible 3D model of the scene.
0eb7343cdd90265282bd261c0e48cf2fd73ea465	Carbon nanotubes are often seen as the only alternative technology to silicon transistors. While they are the most likely short-term alternative, other longer-term alternatives should be studied as well, even if their properties are less familiar to chip designers. While contemplating biological neurons as an alternative component may seem preposterous at first sight, significant recent progress in CMOS-neuron interface suggests this direction may not be unrealistic; moreover, biological neurons are known to self-assemble into very large networks capable of complex information processing tasks, something that has yet to be achieved with other emerging technologies. The first step to designing computing systems on top of biological neurons is to build an abstract model of selfassembled biological neural networks, much like computer architects manipulate abstract models of transistors and circuits. In this article, we propose a first model of the structure of biological neural networks. We provide empirical evidence that this model matches the biological neural networks found in living organisms, and exhibits the smallworld graph structure properties commonly found in many large and self-organized systems, including biological neural networks. More importantly, we extract the simple local rules and characteristics governing the growth of such networks, enabling the development of potentially large but realistic biological neural networks, as would be needed for complex information processing/computing tasks. Based on this model, future work will be targeted to understanding the evolution and learning properties of such networks, and how they can be used to build computing systems.
0eccc4496649f2c86a6d836c3ced3e36428af5e1	We present a novel method for approximate inference in Bayesian models and regularized risk functionals. It is based on the propagation of mean and variance derived from the Laplace approximation of conditional probabilities in factorizing distributions, much akin to Minka’s Expectation Propagation. In the jointly normal case, it coincides with the latter and belief propagation, whereas in the general case, it provides an optimization strategy containing Support Vector chunking, the Bayes Committee Machine, and Gaussian Process chunking as special cases.
0f8f492fb900141d88e0ddcf239107c6a2a27b42	In the deep submicron era, thermal hot spots and large temperature gradients significantly impact system reliability, performance, cost and leakage power. As the system complexity increases, it is more and more difficult to perform thermal management in a centralized manner because of state explosion and the overhead of monitoring the entire chip. In this paper, we propose a framework for distributed thermal management for many-core systems where balanced thermal profile can be achieved by proactive task migration among neighboring cores. The framework has a low cost agent residing in each core that observes the local workload and temperature and communicates with its nearest neighbor for task migration/exchange. By choosing only those migration requests that will result balanced workload without generating thermal emergency, the proposed framework maintains workload balance across the system and avoids unnecessary migration. Experimental results show that, compared with existing proactive task migration technique, our approach generates less hotspots and smoother thermal gradient with less migration overhead and higher processing throughput.
104f123a189fe58cd4e05335215cbfb073468079	Imitating successful behavior is a natural and frequently applied approach to trust in when facing scenarios for which we have little or no experience upon which we can base our decision. In this paper, we consider such behavior in atomic congestion games. We propose to study concurrent imitation dynamics that emerge when each player samples another player and possibly imitates this agents’ strategy if the anticipated latency gain is sufficiently large. Our main focus is on convergence properties. Using a potential function argument, we show that our dynamics converge in a monotonic fashion to stable states. In such a state none of the players can improve its latency by imitating somebody else. As our main result, we show rapid convergence to approximate equilibria. At an approximate equilibrium only a small fraction of agents sustains a latency significantly above or below average. In particular, imitation dynamics behave like fully polynomial time approximation schemes (FPTAS). Fixing all other parameters, the convergence time depends only in a logarithmic fashion on the number of agents. Since imitation processes are not innovative they cannot discover unused strategies. Furthermore, strategies may become extinct with non-zero probability. For the case of singleton games, we show that the probability of this event occurring is negligible. Additionally, we prove that the social cost of a stable state reached by our dynamics is not much worse than an optimal state in singleton congestion games with linear latency function. Finally, we discuss how the protocol can be extended such that, in the long run, dynamics converge to a Nash equilibrium.
10a1e6233fce78a5c6bd3a40cca3e9298da55abe	Step-wise refinement is a powerful paradigm for developing a complex program from a simple program by adding features incrementally. We present the AHEAD (Algebraic Hierarchical Equations for Application Design) model that shows how step-wise refinement scales to synthesize multiple programs and multiple noncode representations. AHEAD shows that software can have an elegant, hierarchical mathematical structure that is expressible as nested sets of equations. We review a tool set that supports AHEAD. As a demonstration of its viability, we have bootstrapped AHEAD tools from equational specifications, refining Java and nonJava artifacts automatically; a task that was accomplished only by ad hoc means previously.
10a5765bd6efd776ea03084e6597b8de8cbcc6ae	Vdd-programmable FPGAs have been proposed recently to reduce FPGA power, where Vdd levels can be customized for different circuit elements and unused circuit elements can be power-gated. In this paper, we first develop an accurate FPGA power model and then design novel Vddprogrammable interconnect switches with minimum number of configuration SRAM cells. Applying our power model to placed and routed benchmark circuits, we evaluate Vddprogrammable FPGA architecture using the new switches. The best architecture in our study uses Vdd-programmable logic blocks and Vdd-gateable interconnects. Compared to the baseline architecture similar to the leading commercial architecture, the best architecture reduces the minimal energy-delay product by 44.14% with 48% area overhead and 3% SRAM cell increase. Our evaluation results also show that LUT size 4 always gives the lowest energy consumption while LUT size 7 always leads to the highest performance for all evaluated architectures
10ab7743b6aebea3fa6a2b73809a1586b9d956c2	Peer-to-Peer (P2P) streaming applications have lead to the disharmony among the involved parties: Content Service Providers (CSPs), Internet Service Providers (ISPs) and P2P streaming End-Users (EUs). This disharmony is not only a technical problem at the network aspect, but also an economic problem at the business aspect. To handle this tussle, this paper proposes a feasible business model to enable all involved parties to enlarge their benefits with the help of a novel QoS-based architecture integrated with caching techniques. We model the interactions, including competition and innovation, among CSPs, ISPs and EUs as a tripartite game by introducing a pricing scheme, which captures both network and business aspects of the P2P streaming applications. We study the tripartite game in different market scenarios as more and more ISPs and CSPs involve into the market. A three-stage Stackelberg game combining with Cournot game is proposed to study the interdependent, interactive and competitive relationship among CSPs, ISPs and EUs. Moreover, we investigate how the market competition motivates ISPs to upgrade the cache service infrastructure. Our theoretical analysis and empirical study both show that the tripartite game can result in a win-win-win outcome. The market competition plays an important role in curbing the pricing power of CSPs and ISPs, and this effect is more remarkable when the amounts of CSPs and ISPs become infinite. Interestingly, we find that in the tripartite game there exists a longstop at which ISPs may have no incentive to upgrade the cache service infrastructure. However, increasing the market competition level can propel the innovation of ISPs.
10d93f66a7bd213bd91d5ed48ec51c185275e4ad	One method for agents to improve their performance is to form coalitions with other agents. One reason why this might occur is because different agents could have been created by the same owner so an incentive to cooperate naturally exists. Competing agents can also choose to coordinate their actions when there is a mutually beneficial result. The emergence and effects or cooperation depend on the structure of the game being played. In this paper, we study a proportionally fair divisible auction to manage agents bidding for service from network and computational resources. We first show that cooperation is a dominant strategy against any fixed level of competition. We then investigate whether collusion can undermine a non-cooperative equilibrium solution, i.e. allow an agent priced out of the non-cooperative game to enter the game by teaming with other agents. We are able to show that agents not receiving service after a bid equilibrium is reached cannot obtain service by forming coalitions. However, cooperation does allow the possibility that agents with positive allocations can improve their performance. To know whether or not to cooperate with another agent, one must devise a way of assigning a value to every coalition. In classical cooperative game theory, the value of a team is the total utility of the team under the worst case response of all other agents, as a coalition is viewed as a threat by the remaining agents. We show that this analysis is not appropriate in our case. The formation of a coalition under a proportionally fair divisible auction improves the performance of those outside the coalition. This then creates an incentive structure where team play is encouraged.
113cf3a6c3b95943c8667b5fd0a6346f95c5b25c	Distributed applications often use quorums in order to guarantee consistency. With emerging world-wide communication technology, many new applications (e.g., conferencing applications and interactive games) wish to allow users to freely join and leave, without restarting the entire system. The dynamic voting paradigm allows such systems to define quorums adaptively, accounting for the changes in the set of participants. Rrrthermore, dynamic voting was proven to be the most available paradigm for maintaining quorums in unreliable networks. However, the subtleties of implementing dynamic voting were not well understood; in fact, many of the suggested protocols may lead to inconsistencies in case of failure. Other protocols severely limit the availability in case failures occur during the protocol. In this paper we present a robust and efficient dynamic voting protocol for unreliable asynchronous networks. The protocol consistently maintains the primary component in a distributed system. Our protocol allows the system to make progress in cases of repetitive failures in which previously suggested protocols block. The protocol is simple to implement, and its communication requirements are small.
12468e58d3a145449afc973c9f7b521a847c8a76	Assessing rules with interestingness measures is the cornerstone of successful applications of association rule discovery. However, there exists no information-theoretic measure which is adapted to the semantics of association rules. In this article, we present the Directed Information Ratio (DIR), a new rule interestingness measure which is based on information theory. DIR is specially designed for association rules, and in particular it differentiates two opposite rules a → b and a → b. Moreover, to our knowledge, DIR is the only rule interestingness measure which rejects both independence and (what we call) equilibrium, i.e. it discards both the rules whose antecedent and consequent are negatively correlated, and the rules which have more counter-examples than examples. Experimental studies show that DIR is a very filtering measure, which is useful for association rule post-processing.
12aced95ba4026bcd651e844a600150d92e18e58	Modeling of a connectionist rule-based systems (or Neuro-AJ hybrid system) discussed through the paper will be a fruitful step towards the practical modeling of human cognition. This paper investigates a plausible and useful integration method of symbolic Al techniques and connectionist models and proposes a practical implementation, mainly how variables can be included in the structured information provided as facts and rules in the system.
12b1a4fff4f7def7f0e40aa0b0413ccec4e6b453	We study local interchangeability of values in constraint networks based on a new approach where a single value in the domain of a variable can be treated as a combination of “subvalues”. We present an algorithm for breaking up values and combining identical fragments. Experimental results show that the transformed problems take less time to solve for all solutions and yield more compactly-representable, but equivalent, solution sets. We obtain new theoretical results on context dependent interchangeability and full interchangeability, and suggest some other applications.
12d037436e39a3ea8bf46f3ceb7a7078c8e7c73d	This paper proposes a mining-based method to achieve event detection for broadcasting tennis videos. Utilizing visual and aural information, we extract some high-level features to describe video segments. The audiovisual features are further transformed to symbolic streams and an efficient mining technique is applied to derive all frequent patterns that characterize tennis events. After mining, we categorize frequent patterns into several kinds of events and therefore achieve event detection for tennis videos by checking the correspondence between mined patterns and events. The experimental results show that the proposed approach is a promising way to detect events in broadcasting tennis video.
12e1edc7a1775a764ab03aead7522c40b43963f8	Utilizing on-chip caches in embedded multiprocessorsystem-on-a-chip (MPSoC) based systems is critical from both performance and power perspectives. While most of the prior work that targets at optimizing cache behavior are performed at hardware and compilation levels, operating system (OS) can also play major role as it sees the global access pattern information across applications. This paper proposes a cache-conscious OS process scheduling strategy based on data reuse. The proposed scheduler implements two complementary approaches. First, the processes that do not share any data between them are scheduled at different cores if it is possible to do so. Second, the processes that could not be executed at the same time (due to dependences) but share data among each other are mapped to the same processor core so that they share the cache contents. Our experimental results using this new data locality aware OS scheduling strategy are promising, and show significant improvements in task completion times.
12facc1309f486a541d721134b2aaedc00570235	This paper investigates the combination of spatial and probabilistic models for reasoning about pedestrian behaviour in visual surveillance systems. Models are learnt by a multi-step unsupervised method and they are used for trajectory labelling and atypical behaviour detection.
13050adb7aa8aaf2e1c38f2b2c7e3d070358d261	In this paper, we present the first snap-stabilizing message forwarding protocol that uses a number of buffers per node being independent of any global parameter, that is 4 buffers per link. The protocol works on a linear chain of nodes, that is possibly an overlay on a largescale and dynamic system, e.g., Peer-to-Peer systems, Grids. . . Provided that the topology remains a linear chain and that nodes join and leave “neatly”, the protocol tolerates topology changes. We expect that this protocol will be the base to get similar results on more general topologies.
131870be9d65a2c43502870d18a603d4f64139e1	This paper presents an algorithm for detecting multiple salient objects in images. The algorithm extends our previous algorithm which was designed to detect only a single salient object. The new algorithm employs five feature maps (lightness distance, color distance, contrast, sharpness, and edge strength), along with a new image-adaptive technique for estimating the usefulness of each feature map based on a local measure of cluster density. As we will demonstrate, our new version can successfully detect multiple salient objects on images for which the previous version did not succeed. Testing on subsets of images from two databases shows that the proposed algorithm performs well on a variety of images containing multiple salient objects.
133955a22d7531b8658aa5830c02736be4f43b65	Though plenty of research has been conducted to improve Internet P2P streaming quality perceived by endusers, little has been known about the upper bounds of achievable performance with available resources so that different designs could compare against. On the other hand, the current practice has shown increasing demand of server capacities in P2P-assisted streaming systems in order to maintain high-quality streaming to end-users. Both research and practice call for a design that can optimally utilize available peer resources. In the paper, we first present a new design, aiming to reveal the best achievable throughput for heterogeneous P2P streaming systems. We measure the performance gaps between various designs and this optimal resource allocation. Through extensive simulations, we find out that several typical existing designs have not fully exploited the potential of system resources. However, the control overhead prohibits the adoption of this optimal approach. Then, we design a hybrid system in trading off the cost of assignment and utilization of resources. This hybrid approach has a proved theoretical bound on efficiency of utilization. Simulation results show that compared with the optimal resource allocation, our proposed hybrid design can achieve near-optimal (up to 90%) utilization while only use much less (below 4%) control overhead. Our results provide a basis for both server capacity planning in current P2Passisted streaming practice and future protocol designs.
1345349908904a0736a5849db3e4d924371052a7	Local thermal hot-spots in microprocessors lead to worstcase provisioning of global cooling resources, especially in large-scale systems. However, efficiency of cooling solutions degrade non-linearly with supply temperature, resulting in high power consumption and cost in cooling – 50∼100% of IT power. Recent advances in active cooling techniques have shown on-chip thermoelectric coolers (TECs) to be very ef- ficient at selectively eliminating small hot-spots, where applying current to a superlattice film deposited between silicon and the heat spreader results in a Peltier effect that spreads the heat and lowers the temperature of the hot-spot significantly to improve chip reliability. In this paper, we propose that hot-spot mitigation using thermoelectric coolers can be used as a power management mechanism to allow global coolers to be provisioned for a better worst case temperature leading to substantial savings in cooling power. In order to quantify the potential power savings from using TECs in data center servers, we present a detailed power model that integrates on-chip dynamic and leakage power sources, heat diffusion through the entire chip, TEC and global cooler efficiencies, and all their mutual interactions. Our multiscale analysis shows that, for a typical data center, TECs allow global coolers to operate at higher temperatures without degrading chip lifetime, and thus save ∼27% cooling power on average while providing the same processor reliability as a data center running at 288K.
134fc2bb80a4521da5cedfb1fcd5901791c6f011	In this paper, we propose a novel automatic approach for personalized music sports video generation. Two research challenges, semantic sports video content selection and automatic video composition, are addressed. For the first challenge, we propose to use multi-modal (audio, video and text) feature analysis and alignment to detect the semantic of events in sports video. For the second challenge, we propose video-centric and music-centric music video composition schemes to automatically generate personalized music sports video based on user’s preference. The experimental results and user evaluations are promising and show that our system’s generated music sports video is comparable to manually generated ones. The proposed approach greatly facilitates the automatic music sports video generation for both professionals and amateurs.
136744ee008dc534b4048337e76ff182ecdc1214	The problem of pedestrian injury is a significant one throughout the world. In 2001, there were 4724 pedestrian fatalities in Europe and 4882 in the US. Significant advances have been made by automotive safety researchers and vehicle manufacturers to address this issue with respect to the design of vehicles, but the complex nature of pedestrian accident scenarios has resulted in great difficulty when using traditional statistical methods. Specifically, problems have been encountered when attempting to study the effects of individual parameters of vehicle front-end geometry on pedestrian head injury. This paper attempts to demonstrate the feasibility of applying the field of evolutionary computation to the problem of pedestrian safety by using a simple genetic algorithm to optimize the centre-line geometry of a car’s front-end for the reduction of pedestrian head and thoracic injury. The fitness of each design is assessed by creating a multi-body mathematical model of the vehicle front and simulating impacts with models of different sized pedestrians, and ranking according to the combined injury scores.
147856e2a691489587efaa982468e7040c017b1c	During the last years the use of intelligent strategies for tuning Proportional-Integral-Derivative (PID) controllers has been growing. The evolutionary strategies have won an important place thanks to their flexibility. In this paper, the automatic tuning of systems with stable and unstable dynamics, through a genetic approach is presented. The advantages of the proposed approach ere highlighted through the comparison with the Ziegler-Nichols modified closed loop method, and the Visioli genetic approach. The proposed methodology goal is to expand the intelligent tuning application to a wider range of processes (covering systems with oscillatory or unstable modes).
14a3304a40d6ef79e082055e7455d3c3de8b3dcf	Human action analysis has achieved great success especially with the recent development of advanced sensors and algorithms that can effectively track the body joints. Temporal motion of body joints carries crucial information about human actions. However, current dynamic models typically assume stationary local transition and therefore are limited to local dynamics. In contrast, we propose a novel human action recognition algorithm that is able to capture both global and local dynamics of joint trajectories by combining a Gaussian-Binary restricted Boltzmann machine (GB-RBM) with a hidden Markov model (HMM). We present a method to use RBM as a generative model for multi-class classification. Experimental results on benchmark datasets demonstrate the capability of the proposed method in exploiting the dynamic information at different levels.
153ea7038a52eb81b85d4ca66c754bf727ca6406	We propose a novel algorithm that, given a long DNA sequence, finds approximate repeats, allowing general insertions and deletions. The idea is based on the Gibbs sampling approach of Lawrence et al. The novelty of our algorithm stems largely from our handling of gaps in the motif occurrences. Another contribution is the use of relative entropy as a natural scoring function for alignment of the motif occurrences. We describe the results of experiments with this algorithm on the full H. influenzae genome.
158cda9abd6e7a13318151503ee65407d7fd180a	Tandem mass spectrometry has become central in proteomics projects. In particular, it is of prime importance to design sensitive and selective score functions to reliably identify peptides in databases. By using a huge collection of 140 000+ peptide MS/MS spectra, we systematically study the importance of many characteristics of a match (peptide sequence/spectrum) to include in a score function. Besides classical match characteristics, we investigate the value of new characteristics such as amino acid dependence and consecutive fragment matches. We fi- nally select a combination of promising characteristics and show that the corresponding score function achieves very low false positive rates while being very sensitive, thereby enabling highly automated peptide identi- fication in large proteomics projects. We compare our results to widely used protein identification systems and show a significant reduction in false positives.
1597a40368c48f3da5d82ee584e0dec3b618df06	VLSI systems are commonly specified using sequential executable functional specifications, but implemented in a highly concurrent manner. Alhough the methods to transform between the sequential specification and concurrent implementation have been well-studied, there are still substantial difficulties in verifying that the concurrent implementation corresponds to the sequential specification after lowlevel optimization. The majority of methods for doing this verification have focused on strong semantic models for reasoning about systems and their specifications, but these models can add significant unnecessary complexity. In this paper, we explore a weak but effective method for reasoning about implementation relations. We show how a sequential embedding of a concurrent program can be generated, and how that embedding can be used to dramatically reduce the reachable state space of the verification problem while maintaining the semantic model of interest.
15aa277b1054cdcdf7fc018e3a3abe2df7a1691b	Temporal-difference (TD) networks are a class of predictive state representations that use well-established TD methods to learn models of partially observable dynamical systems. Previous research with TD networks has dealt only with dynamical systems with finite sets of observations and actions. We present an algorithm for learning TD network representations of dynamical systems with continuous observations and actions. Our results show that the algorithm is capable of learning accurate and robust models of several noisy continuous dynamical systems. The algorithm presented here is the first fully incremental method for learning a predictive representation of a continuous dynamical system.
15e38e34b3360f59a33a06be63b7f73457915da5	Network management will benefit from automated tools based upon formal methods. Several such tools have been published in the literature. We present a new formal method for a new tool, Atomic Predicates (AP) Verifier, which is much more time and space efficient than existing tools. Given a set of predicates representing packet filters, AP Verifier computes a set of atomic predicates, which is minimum and unique. The use of atomic predicates dramatically speeds up computation of network reachability. We evaluated the performance of AP Verifier using forwarding tables and ACLs from three large real networks. The atomic predicate sets of these networks were computed very quickly and their sizes are surprisingly small. Real networks are subject to dynamic state changes over time as a result of rule insertion and deletion by protocols and operators, failure and recovery of links and boxes, etc. In a software-defined network, the network state can be observed in real time and thus may be controlled in real time. AP Verifier includes algorithms to process such events and check compliance with network policies and properties in real time. We compare time and space costs of AP Verifier with NetPlumber using datasets from the real networks.
1604bdf9a49a98a39218f909ee766a913c7a4721	This report presents a database of about ✟✡✠☛✠ graph invariants for deriving systematically necessary conditions from the graph properties based representation of global constraints. This scheme is based on invariants on the graph characteristics used in the description of a global constraint. A SICStus Prolog implementation based on arithmetic and logical constraints as well as on indexicals is available.
1677d29a108a1c0f27a6a630e74856e7bddcb70d	Sparse representation techniques for robust face recognition have been widely studied in the past several years. Recently face recognition with simultaneous misalignment, occlusion and other variations has achieved interesting results via robust alignment by sparse representation (RASR). In RASR, the best alignment of a testing sample is sought subject by subject in the database. However, such an exhaustive search strategy can make the time complexity of RASR prohibitive in large-scale face databases. In this paper, we propose a novel scheme, namely misalignment robust representation (MRR), by representing the misaligned testing sample in the transformed face space spanned by all subjects. The MRR seeks the best alignment via a two-step optimization with a coarse-to-fine search strategy, which needs only two deformation-recovery operations. Extensive experiments on representative face databases show that MRR has almost the same accuracy as RASR in various face recognition and verification tasks but it runs tens to hundreds of times faster than RASR. The running time of MRR is less than 1 second in the large-scale Multi-PIE face database, demonstrating its great potential for real-time face recognition.
16aeeae61701c74fdb3b560ec1ba7eef086bb4ce	We propose a multimedia content protection framework that is based on biometric data of the users and a layered encryption/decryption scheme. Password-only encryption schemes are vulnerable to illegal key exchange problems. By using biometric data along with hardware identifiers as keys, it is possible to alleviate fraudulent usage of protected content. A combination of symmetric and asymmetric key systems is utilized for this purpose. The computational requirements and applicability of the proposed method are addressed. The results of encryption and decryption experiments related to time measurements are included. Watermarking systems can be used to complement the proposed method to permit novel uses of protected multimedia data.
16c5fb6ff8dd31eac9cb7e4a6881f592f2758b16	The problem of approximating a given probability distribution using a simpler distribution plays an important role in several areas of machine learning, for example variational inference and classification. Within this context, we consider the task of learning a mixture of tree distributions. Although mixtures of trees can be learned by minimizing the KL-divergence using an EM algorithm, its success depends heavily on the initialization. We propose an efficient strategy for obtaining a good initial set of trees that attempts to cover the entire observed distribution by minimizing the α-divergence with α = ∞. We formulate the problem using the fractional covering framework and present a convergent sequential algorithm that only relies on solving a convex program at each iteration. Compared to previous methods, our approach results in a significantly smaller mixture of trees that provides similar or better accuracies. We demonstrate the usefulness of our approach by learning pictorial structures for face recognition.
16c8bc97a022637ba53bab8be1b815a02e26b561	In the weighted paging problem there is a weight (cost) for fetching each page into the cache. We design a randomized O(log k)-competitive online algorithm for the weighted paging problem, where k is the cache size. This is the first randomized o(k)-competitive algorithm and its competitiveness matches the known lower bound on the problem. More generally, we design an O(log(k/(k − h + 1)))-competitive online algorithm for the version of the problem where the online algorithm has cache size k and the offline algorithm has cache size h ≤ k. Weighted paging is a special case (weighted star metric) of the well known k-server problem for which it is a major open question whether randomization can be useful in obtaining sublinear competitive algorithms. Therefore, abstracting and extending the insights from paging is a key step in the resolution of the k-server problem. Our solution for the weighted paging problem is based on a two-step approach. In the first step we obtain an O(log k)-competitive fractional algorithm which is based on a novel online primal-dual approach. In the second step we obtain a randomized algorithm by rounding online the fractional solution to an actual distribution on integral cache solutions. We conclude with a randomized O(log N)- competitive algorithm for the well studied Metrical Task System problem (MTS) on a metric defined by a weighted star on N leaves, improving upon a previous O(log2 N)- competitive algorithm of Blum et al. [9].
16f93683743c13c8ce3e6c9b0475f23832bb0754	The creation and maintenance of online production communities depend on the complex ecology created by the interaction of social roles, and these roles are essential for the governance of the community. This study investigates the organizational structure of one of the most notable peerproduction projects: Wikipedia. While online communities have often been depicted as ‘flat’ and egalitarian, recent studies of Wikipedia suggest that it has developed a cumbersome beaurocratic structure that includes a hierarchy of organizational role. The objective of this study is, thus, to empirically study the organization of roles in Wikipedia and the hierarchy formed through their power relationships. Our research method employs Wikipedia’s formal set of access privileges as indicators of roles, and analyses all 4,902,643 Wikipedia members (of which 10,496 hold special access privileges). Applying statistical techniques traditionally employed to validate the psychometric properties of scales, we find that Wikipedia has an intricate ecology of roles. Our analysis of power relationships within these twelve roles reveals Wikipedia’s organizational hierarchy. Implications for theory and practice are discussed.
17283370adbedf8bca327ce4e43ccaf9182f3c0d	We consider a model of a 3D image obtained by discretizing it into a multiresolution tetrahedral mesh known as a hierarchy of diamonds. This model enables us to extract crack-free approximations of the 3D image at any uniform or variable resolution, thus reducing the size of the data set without reducing the accuracy. A 3D intensity image is a scalar field (the intensity field) defined at the vertices of a 3D regular grid and thus the graph of the image is a hypersurface in R4. We measure the discrete distortion, a generalization of the notion of curvature, of the transformation which maps the tetrahedralized 3D grid onto its graph in R4. We evaluate the use of a hierarchy of diamonds to analyze properties of a 3D image, such as its discrete distortion, directly on lower resolution approximations. Our results indicate that distortion-guided extractions focus the resolution of approximated images on the salient features of the intensity image.
176600b839d1f198831b1d43938f483784abb420	CPM or cost per thousand impressions is the prevalent metric used for selling online display ads. In previous work, we have shown that the exposure duration of an ad has strong effects on the likelihood of an ad being remembered [Goldstein et al., 2011], with the first seconds of exposure having the greatest impact on memory. Because an ad pricing metric that is based on both time and impressions should be more exact than one based on impressions alone, the industry has good reasons to move towards time-based advertising. We address the following unanswered question: how should time-based ads be scheduled? We test and present one schedule that leads to greater total recollection, which advertisers want, and increased revenue, which publishers want. First, we find that presenting two short, successive ads results in more total recollection than presenting one longer ad of twice the duration. Second, we show that this effect disappears as the duration of these ads increases. Together, these findings suggest a form of timebased ad pricing that should appeal to advertisers and publishers alike.
17672157ae97ca104c956ad1a6312b71bd8ad038	We present a new approach for computing generalized Voronoi diagrams in two and three dimensions using interpolation-based polygon rasterization hardware. The input primitives may be points, lines, polygons, curves, or surfaces. The algorithm computes a discrete Voronoi diagram by rendering a three dimensional distance mesh corresponding to each primitive. The polygonal mesh is a bounded-error approximation of a nonlinear distance function. The algorithm divides the space into regular cells. For each cell it computes the closest primitive and the distance to that primitive using polygon scan-conversion and Z-buffer depth comparison. We present efficient techniques to detect Voronoi boundaries and compute Voronoi neighbors. The algorithm has been implemented on SGI workstations and PCs using OpenGL and applied to complex 2D and 3D datasets. We also demonstrate the applications of our algorithm to fast motion planning in static and dynamic environments, and improving the performance of continuous Voronoi diagram computation.
178eceeae8aed820b789d7b5e4e2ddd1cfa7c557	Finding binary sequences with low autocorrelation is a very hard problem with many practical applications. In this paper we analyze several metaheuristic approaches to tackle the construction of this kind of sequences. We focus on two different local search strategies, steepest descent local search (SDLS) and tabu search (TS), and their use both as stand-alone techniques and embedded within a memetic algorithm (MA). Plain evolutionary algorithms are shown to perform worse than stand-alone local search strategies. However, a MA endowed with TS turns out to be a stateof-the-art algorithm: it consistently finds optimal sequences in considerably less time than previous approaches reported in the literature.
17b6ef93649bfe4c273b531028ee4550fbc5ac0a	Sparse coding has shown its power as an effective data representation method. However, up to now, all the sparse coding approaches are limited within the single domain learning problem. In this paper, we extend the sparse coding to cross domain learning problem, which tries to learn from a source domain to a target domain with significant different distribution. We impose the Maximum Mean Discrepancy (MMD) criterion to reduce the cross-domain distribution difference of sparse codes, and also regularize the sparse codes by the class labels of the samples from both domains to increase the discriminative ability. The encouraging experiment results of the proposed cross-domain sparse coding algorithm on two challenging tasks — image classification of photograph and oil painting domains, and multiple user spam detection — show the advantage of the proposed method over other cross-domain data representation methods.
180b6f72a143c218f84d675fe42dc0d2b3e0152e	Statistical database security is an important problem that has major implications for privacy and protection in the computerised, online world. In this paper we prove that the bounds on the number of answerable range queries in a 1-dimensional statistical database of size n for n odd, or n even and greater than 52, derived in [4] in fact hold for all n except for n = 12, and are achievable using two threads.
1820869030fca660212fb7a6449b6ad1aa99d9db	Multithreaded programs are notoriously prone to race conditions, a problem exacerbated by the widespread adoption of multi-core processors with complex memory models and cache coherence protocols. Much prior work has focused on static and dynamic analyses for race detection, but these algorithms typically are unable to distinguish destructive races that cause erroneous behavior from benign races that do not. Performing this classification manually is difficult, time consuming, and error prone. This paper presents a new dynamic analysis technique that uses adversarial memory to classify race conditions as destructive or benign on systems with relaxed memory models. Unlike a typical language implementation, which may only infrequently exhibit non-sequentially consistent behavior, our adversarial memory implementation exploits the full freedom of the memory model to return older, unexpected, or stale values for memory reads whenever possible, in an attempt to crash the target program (that is, to force the program to behave erroneously). A crashing execution provides concrete evidence of a destructive bug, and this bug can be strongly correlated with a specific race condition in the target program. Experimental results with our JUMBLE prototype for Java demonstrate that adversarial memory is highly effective at identifying destructive race conditions, and in distinguishing them from race conditions that are real but benign. Adversarial memory can also reveal destructive races that would not be detected by traditional testing (even after thousands of runs) or by model checkers that assume sequential consistency.
18879a57e4f01b6f7f642a99c72d82907e78fe94	As observed in several recent publications, improved retrieval performance is achieved when pairwise similarities between the query and the database objects are replaced with more global affinities that also consider the relation among the database objects. This is commonly achieved by propagating the similarity information in a weighted graph representing the database and query objects. Instead of propagating the similarity information on the original graph, we propose to utilize the tensor product graph (TPG) obtained by the tensor product of the original graph with itself. By virtue of this construction, not only local but also long range similarities among graph nodes are explicitly represented as higher order relations, making it possible to better reveal the intrinsic structure of the data manifold. In addition, we improve the local neighborhood structure of the original graph in a preprocessing stage. We illustrate the benefits of the proposed approach on shape and image ranking and retrieval tasks. We are able to achieve the bull’s eye retrieval score of 99.99% on MPEG-7 shape dataset, which is much higher than the state-of-the-art algorithms.
197e2e4d2992484435b5323649668b10a9cce0a2	With the increasing impact and popularity of Web service technologies in today’s World Wide Web, composition of Web services has received much interest to support enterprise-to-enterprise application integrations. As for service providers and their partners, the Quality of service (QoS) offered by a composite Web service is important. The QoS guarantee for composite services has been investigated in a number of works. However, those works consider only an individual composition or take the viewpoint of a single provider. In this paper, we focus on the problem of QoS guarantees for multiple inter-related compositions and consider the global viewpoints of all providers engaged in the compositions. The contributions of this paper are two folds. We first formalize the problem of QoS guarantees for multi-compositions and show that it can be modelled as a Distributed Constraint Satisfaction Problem (DisCSP). We also take into account the dynamic nature of the Web service environment of which compositions may be formed or dissolved any time. Secondly, we present a dynamic DisCSP algorithm to solve the problem and discuss our initial experiment to show the feasibility of our approach for multiple Web service compositions with QoS guarantees.
198dcf518298e0afe39d005da5320cfe840480c1	In this paper the relation between nonanticipative rate distortion function (RDF) and Bayesian filtering theory is further investigated on general Polish spaces. The relation is established via an optimization on the space of conditional distributions of the so-called directed information subject to fidelity constraints. Existence of the optimal reproduction distribution of the nonanticipative RDF is shown using the topology of weak convergence of probability measures. Subsequently, we use the solution of the nonanticipative RDF to present the realization of a multidimensional partially observable source over a scalar Gaussian channel. We show that linear encoders are optimal, establishing joint source-channel coding in real-time.
1990afd6d3c11cc6b7b9e272b3fe43dc8aed3138	The execution of a complex task in any environment requires planning. Planning is the process of constructing an activity graph given by the current state of the system, a goal state, and a set of activities. If we wish to execute a complex computing task in a heterogeneous computing environment with autonomous resource providers, we should be able to adapt to changes in the environment. A possible solution is to construct a family of activity graphs beforehand and investigate the means of switching from one member of the family to another when the execution of one activity graph fails. In this paper, we study the conditions when plan switching is feasible. Then we introduce an approach for plan switching and report the simulation results of this approach.
19c94455ca5015a8a25e89ede1b6418645b57912	In this paper we review the notion of direct and indirect causal effect as introduced by Pearl (2001). We show how it can be formulated without counterfactuals, using regime indicators instead. This allows to consider the natural (in)direct effect as a special case of sequential treatments discussed by Dawid & Didelez (2005) which immediately yields conditions for identifiability as well as a graphical way of checking identifiability.
19e851f2c61ffece4e5fada6deac7fecddd8792f	We present a technique for compression of shortest paths routing tables for wireless ad hoc networks. The main characteristic of such networks is that geographic location of nodes determines network topology. As opposed to encoding individual node locations, at each node our approach groups the remaining nodes in the network into regions. All shortest paths to nodes in a specific region are routed via the same neighboring node. In this paper, we propose an algorithm for dividing a network field into distinct regions to minimize routing table size while guaranteeing shortest path routes. We show that this problem is NP-hard, propose a heuristic to find efficient solutions, and empirically demonstrate the resulting system performance from the perspective of compression ratio and scalability. In our experiments, routing tables compressed using this technique, require 88.9% to 97.9% less storage than uncompressed tables. In order to achieve energy efficient routing, we propose an augmentation to the original routing mechanism that enables load balancing flexibility along with guaranteed shortest path routing at the expense of larger routing tables. Preliminary experiments estimate 10% lifetime extension of network nodes with a tradeoff of an increase in the size of routing tables. Finally, we propose a compression technique that aims at representing trajectories in a sensing network in a compact manner. This approach relies on trajectory prediction using three weighted Markov models, a local, regional and global one, all of them with context-length equal to one. Finally, we discuss a range of possible applications that rely on the developed prediction and routing models.
1a73a8e958cde8e96be43f94bca50b196b07d595	Wireless network emulator testbeds have become increasingly important for realistic, at-scale experimental evaluation of new network architectures and protocols. Typically, wireless network performance measurements are made at multiple layers of the wireless protocol stack, i.e. link layer, MAC layer and network layer. This study highlights the impact of layer 2 frame aggregation that is enabled by default in the software drivers for commodity wireless 802.11 devices while it is still not a part of the core 802.11 standard. Using experimental measurements, it is shown that this feature has an impact across a diverse set of wireless experiments and should be considered while comparing results. Measurements on the ORBIT testbed show that throughput measurements can vary up to a startling 25% for certain packet sizes and the variance in receiver side interframe delays can almost double if MAC aggregation and preset transmission opportunities are not taken into consideration. Further results for VoIP traffic show a deterioration in jitter of up to 8 times when coupled with MAC layer aggregation in 802.11.
1a9eb04b9b07d4a58aa78eb9f68a77ade0199fab	Human action in video sequences can be seen as silhouettes of a moving torso and protruding limbs undergoing articulated motion. We regard human actions as threedimensional shapes induced by the silhouettes in the spacetime volume. We adopt a recent approach [9] for analyzing 2D shapes and generalize it to deal with volumetric spacetime action shapes. Our method utilizes properties of the solution to the Poisson equation to extract space-time features such as local space-time saliency, action dynamics, shape structure and orientation. We show that these features are useful for action recognition, detection and clustering. The method is fast, does not require video alignment and is applicable in (but not limited to) many scenarios where the background is known. Moreover, we demonstrate the robustness of our method to partial occlusions, non-rigid deformations, significant changes in scale and viewpoint, high irregularities in the performance of an action and low quality video.
1aadad3872f7fe564a98a3d9205ba3c329594075	Distributed message relaying is an important function of a peer-topeer system to discover service providers. Existing search protocols in unstructured peer-to-peer systems either create huge burden on communications or cause long response time. Moreover, these systems are also vulnerable to the free riding problem. In this paper we present an incentive mechanism that not only mitigates the free riding problem, but also achieves good system efficiency in message relaying for peer discovery. In this mechanism promised rewards are passed along the message propagation process. A peer is rewarded if a service provider is found via a relaying path that includes this peer. We provide some analytic insights to the symmetric Nash equilibrium strategies of this game, and an approximate approach to calculate this equilibrium. Experiments show that this incentive mechanism brings a system utility generally higher than breadth-first search and random walks, based on both the estimated utility from our approximate equilibrium and the utility generated from learning in the incentive mechanism.
1ab75f37d1ee196f1c1087fc95af79216ba826fe	The capacity or Vapnik-Chervonenkis dimension of a feedforward neural architecture is the maximum number of input patterns that can be mapped correctly to fixed arbitrary outputs. So far it is known that the upper bound for the capacity of two-layer feedforward architectures with independent weights depends on the number of connections in the neural architecture [1]. In this paper we focus on the capacity of multilayer feedforward networks structured by shared weights. We show that these structured architectures can be transformed into equivalent conventional multilayer feedforward architectures. Known estimations for the capacity are extended to achieve upper bounds for the capacity of these general multi-layer feedforward architectures. As a result an upper bound for the capacity of structured architectures is derived that increases with the number of independent network parameters. This means that weight sharing in a fixed neural architecture leads to a significant reduction of the upper bound of the capacity.
1ad4974c4d79b00c890bd2dd1562600bd9c7e2bd	In a significant minority of cases, certain pronouns, especially the pronoun it, can be used without referring to any specific entity. This phenomenon of pleonastic pronoun usage poses serious problems for systems aiming at even a shallow understanding of natural language texts. In this paper, a novel approach is proposed to identify such uses of it: the extrapositional cases are identified using a series of queries against the web, and the cleft cases are identified using a simple set of syntactic rules. The system is evaluated with four sets of news articles containing 679 extrapositional cases as well as 78 cleft constructs. The identification results are comparable to those obtained by human efforts.
1aede27149f28f7590ca1b8ee8f926e7477cd5d4	The significance of regular path queries (RPQs) on graphlike data structures has grown steadily over the past decade. RPQs are, often in restricted forms, part of graph-oriented query languages such as XQuery/XPath and SPARQL, and have applications in areas such as semantic, social, and biomedical networks. However, existing systems for evaluating RPQs are restricted either in the type of the graph (e.g., only trees), the type of regular expressions (e.g., only single steps), and/or the size of the graphs they can handle. No method has yet been developed that would be capable of efficiently evaluating general RPQs on large graphs, i.e., with millions of nodes/edges. We present a novel approach for answering RPQs on large graphs. Our method exploits the fact that not all labels in a graph are equally frequent. We devise an algorithm which decomposes an RPQ into a series of smaller RPQs using rare labels, i.e., elements of the query with few matches, as way-points. A search thereby is decomposed into a set of smaller search problems which are tackled in a bi-directional fashion, supported by a set of graph indexes. Comparison of our algorithm with two approaches following the traditional methods for tackling such problems, i.e., the usage of automata, reveals that (a) the automata-based methods are not able to handle large graphs due to the amount of memory they require, and that (b) our algorithm outperforms the automatabased approach, often by orders of magnitude. Another advantage of our algorithm is that it can be parallelized easily.
1b802ca0a6dd56e8ebec34d16511cd83f8ef917d	In this paper we review several knowledge base update operations that have recently been proposed in order to handle disjunctive updates and updates under integrity constraints. Updating with disjunctive information is problematic for minimal change semantics, in particular for Winslett's Possible Models Approach (PMA). Zhang and Foo have recently defined the MCD semantics to overcome this drawback. In this paper we show that the MCD is still problematic, and propose a correction. Then we address a second important problem plaguing the PMA and its variations, viz. the handling of integrity constraints. We investigate a dependence-based semantics, which Liberatore has shown to be coNP-complete. We give a simple automated deduction method, and show that integrity constraints as well as disjunctive updates are treated correctly in that semantics, and discuss the Katsuno-Mendelzon postulates.
1bca65a562c74e7b8012f8501b6ba0bff15e869a	Pervasive services may have to rely on multimodal classification to implement situation-recognition. However, the effectiveness of current multimodal classifiers is often not satisfactory. In this paper, we describe a novel approach to multimodal classification based on integrating a vision sensor with a commonsense knowledge base. Specifically, our approach is based on extracting the individual objects perceived by a camera and classifying them individually with nonparametric algorithms; then, using a commonsense knowledge base, classifying the overall scene with high effectiveness. Such classification results can then be fused together with other sensors, again on a commonsense basis, for both improving classification accuracy and dealing with missing labels. Experimental results are presented to assess, under different configurations, the effectiveness of our vision sensor and its integration with other kinds of sensors, proving that the approach is effective and able to correctly recognize a number of situations in open-ended environments.
1c14996f9cbb1ea09f5dae69215a5ffe21a21ad9	Business performance is affected by the quality management practices of the organization. Holistic programs, including the national quality award programs, six-sigma initiatives, and enterprise resource planning (ERP) applications may characterize the highest level of quality initiatives. Arguably, the most popular quality initiative is the ISO 9000 program. The effect of different levels of quality initiatives on business performance was studied in a stratified sample of 280 firms in the Canadian provinces of Ontario and Quebec. The effect of using these quality initiatives in combination with ISO 9000 was also studied. Finally, the relationship between a firm’s use of enterprise resource applications and performance excellence was examined. The study revealed that, regarding quality initiatives, ISO 9000 certification was deemed positively beneficial only when combined with quality initiatives of the highest level. Further, there is a moderately significant relationship between ERP applications and performance excellence.
27028fac2782734b4d1627c535ea8c59351ac749	Shopbots and Internet sites that help users locate the best price for a product are changing the way people shop by providing valuable information on goods and services. This paper presents a first attempt to measure the value of one piece of information: the price charged for goods and services. We first establish a theoretical limit to the value of price information for the first seller in a market that decides to sell price information to a shopbot and quantify the revenues that the seller can expect to receive. We then proceed to discuss whether and how much of this theoretical value can actually be realized in equilibrium settings.
2774393ecb042926ba7fa6957841853ffff0396d	Recent work on loglinear models in probabilistic constraint logic programming is applied to firstorder probabilistic reasoning. Probabilities are defined directly on the proofs of atomic formulae, and by marginalisation on the atomic formulae themselves. We use Stochastic Logic Programs (SLPs) composed of labelled and unlabelled definite clauses to define the proof probabilities. We have a conservative extension of first-order reasoning, so that, for example, there is a one-one mapping between logical and random variables. We show how, in this framework, Inductive Logic Programming (ILP) can be used to induce the features of a loglinear model from data. We also compare the presented framework with other approaches to first-order probabilistic reasoning.
2e939ed3bb378ea966bf9f710fc1138f4e16ef38	Conditional Value at Risk (CVaR) is a prominent risk measure that is being used extensively in various domains. We develop a new formula for the gradient of the CVaR in the form of a conditional expectation. Based on this formula, we propose a novel sampling-based estimator for the gradient of the CVaR, in the spirit of the likelihood-ratio method. We analyze the bias of the estimator, and prove the convergence of a corresponding stochastic gradient descent algorithm to a local CVaR optimum. Our method allows to consider CVaR optimization in new domains. As an example, we consider a reinforcement learning application, and learn a risksensitive controller for the game of Tetris.
31368c6398a34b489f78708039177d858b171d13	Leading agent-based trust models address two important needs. First, they show how an agent may estimate the trustworthiness of another agent based on prior interactions. Second, they show how agents may share their knowledge in order to cooperatively assess the trustworthiness of others. However, in real-life settings, information relevant to trust is usually obtained piecemeal, not all at once. Unfortunately, the problem of maintaining trust has drawn little attention. Existing approaches handle trust updates in a heuristic, not a principled, manner. This paper builds on a formal model that considers probability and certainty as two dimensions of trust. It proposes a mechanism using which an agent can update the amount of trust it places in other agents on an ongoing basis. This paper shows via simulation that the proposed approach (a) provides accurate estimates of the trustworthiness of agents that change behavior frequently; and (b) captures the dynamic behavior of the agents. This paper includes an evaluation based on a real dataset drawn from Amazon Marketplace, a leading e-commerce site.
3521f22e34fef8a53d55df180a76df5a7a4e7f87	The rise of the social media sites, such as blogs, wikis, Digg and Flickr among others, underscores the transformation of the Web to a participatory medium in which users are collaboratively creating, evaluating and distributing information. The innovations introduced by social media has lead to a new paradigm for interacting with information, what we call ’social information processing’. In this paper, we study how social news aggregator Digg exploits social information processing to solve the problems of document recommendation and rating. First, we show, by tracking stories over time, that social networks play an important role in document recommendation. The second contribution of this paper consists of two mathematical models. The first model describes how collaborative rating and promotion of stories emerges from the independent decisions made by many users. The second model describes how a user’s influence, the number of promoted stories and the user’s social network, changes in time. We find qualitative agreement between predictions of the model and user data gathered from Digg.
3717dc0dba9fdb13d1459ed4edf7955dce2e06b3	A theoretic framework for multimedia information retrieval is introduced which guarantees optimal retrieval effectiveness. In particular, a Ranking Principle for Distributed Multimedia-Documents (RPDM) is described together with an algorithm that satisfies this principle. Finally, the RPDM is shown to be a generalization of the Probability Ranking principle (PRP) which guarantees optimal retrieval effectiveness in the case of text document retrieval. The PRP justifies theoretically the relevance ranking adopted by modern search engines. In contrast to the classical PRP, the new RPDM takes into account transmission and inspection time, and most importantly, aspectual recall rather than simple recall.
3b66e64f48434d1a15cdc6ee383e925299bf56c8	The paper presents and evaluates the power of a new scheme that generates search heuristics mechanically for problems expressed using a set of functions or relations over a nite set of variables. The heuristics are extracted from a parameterized approximation scheme called Mini-Bucket elimination that allows controlled trade-oo between computation and accuracy. The heuristics are used to guide Branch-and-Bound and Best-First search. Their performance is compared on two optimization tasks: the Max-CSP task deened on deterministic databases and the Most Probable Explanation task deened on probabilistic databases. Benchmarks were random data sets as well as applications to coding and medical diagnosis problems. Our results demonstrate that the heuristics generated are eeective for both search schemes, permitting controlled trade-oo between preprocessing (for heuristic generation) and search.
40d627ad2931613b3d95d90ac29834deb7cd4b83	An algorithm for the automatic reconstruction of triangular mesh surface model from range images is presented. The optimal piecewise linear surface approximation problem is defined as: Given a set S of points uniformly sampled from a bivariate function ƒ(x,y) on a rectangular grid of dimension W×H, find a minimum triangular mesh approximating the surface with vertices anchored at a subset S’ of S, such that the deviation (the error between the approximated value and f(x, y)) at any sample point is within a given bound of ε > 0. The algorithm deploys a multi-agent resource planning approach to achieve adaptive, accurate and concise piecewise linear (triangular) approximation using the L-∞ norm. The resulting manifold triangular mesh can be directly used as 3D rendering model for visualization with controllable and guaranteed quality (by ε). Due to this dual optimality, the algorithm achieves both storage efficiency and visual quality. The error control scheme further facilitates the construction of models in multiple levels of details, which is desirable in animation and virtual reality moving scenes. Experiments with various benchmark range images from smooth functional surfaces to satellite terrain images yield succinct, accurate and visually pleasant triangular meshes. Furthermore, the independence and multiplicity of agents suggests a natural parallelism for triangulation computation, which provides a promising solution for the real-time exploration of large data sets.
41b3a5272d0c8f98b73e7275481cd917802ad8b7	In this paper, we strengthen the competitive analysis results obtained for a fundamental online streaming problem, the Frequent Items Problem. Additionally, we contribute with a more detailed analysis of this problem, using alternative performance measures, supplementing the insight gained from competitive analysis. The results also contribute to the general study of performance measures for online algorithms. It has long been known that competitive analysis suffers from drawbacks in certain situations, and many alternative measures have been proposed. However, more systematic comparative studies of performance measures have been initiated recently, and we continue this work, using competitive analysis, relative interval analysis, and relative worst order analysis on the Frequent Items Problem.
41c86d417baa3c67b6a027fb8bd3784ea31a3b8e	This article describes an algorithm for reducing the intermediate alphabets in cascades of finite-state transducers (FSTs). Although the method modifies the component FSTs, there is no change in the overall relation described by the whole cascade. No additional information or special algorithm, that could decelerate the processing of input, is required at runtime. Two examples from Natural Language Processing are used to illustrate the effect of the algorithm on the sizes of the FSTs and their alphabets. With some FSTs the number of arcs and symbols shrank considerably.
45099df43a2692bb4eea8ec12b331bb827403d57	Simulating distributed database systems is inherently difficult, as there are many factors that may influence the results. This includes architectural options as well as workload and data distribution. In this paper we present the DBsim simulator and some simulation results. The DBsim simulator architecture is extendible, and it is easy to change parameters and configuration. The simulation results in this paper is a comparison of performance and responsetimes for two concurrency control algorithms, timestamp ordering and two-phase locking. The simulations have been run with different number of nodes, network types, data declustering and workloads. The results show that for a mix of small and long transactions, the throughput is significantly higher for a system with a timestamp ordering scheduler than for a system with a two-phase locking scheduler. With only short transactions, the performance of the two schedulers are almost identical. Long transactions are treated more fair by a two-phase locking scheduler, because a timestamp ordering scheduler has a very high abort rate for long transactions.
4699e2c09244f3496b1c202925618ccf732a617d	Bidimensionality theory appears to be a powerful framework for the development of metaalgorithmic techniques. It was introduced by Demaine et al. [J. ACM 2005 ] as a tool to obtain sub-exponential time parameterized algorithms for problems on H-minor free graphs. Demaine and Hajiaghayi [SODA 2005 ] extended the theory to obtain polynomial time approximation schemes (PTASs) for bidimensional problems, and subsequently improved these results to EPTASs. Fomin et. al [SODA 2010 ] established a third meta-algorithmic direction for bidimensionality theory by relating it to the existence of linear kernels for parameterized problems. In this paper we revisit bidimensionality theory from the perspective of approximation algorithms and redesign the framework for obtaining EPTASs to be more powerful, easier to apply and easier to understand. Two of the most widely used approaches to obtain PTASs on planar graphs are the LiptonTarjan separator based approach [SICOMP 1980 ], and Baker’s approach [J.ACM 1994 ]. Demaine and Hajiaghayi [SODA 2005 ] strengthened both approaches using bidimensionality and obtained EPTASs for several problems, including CONNECTED DOMINATING SET and FEEDBACK VERTEX SET. We unify the two strenghtened approaches to combine the best of both worlds. At the heart of our framework is a decomposition lemma which states that for “most” bidimensional problems, there is a polynomial time algorithm which given an H-minor-free graph G as input and an  > 0 outputs a vertex set X of size  · OP T such that the treewidth of G \ X is f(). Here, OP T is the objective function value of the problem in question and f is a function depending only on . This allows us to obtain EPTASs on (apex)-minor-free graphs for all problems covered by the previous framework, as well as for a wide range of packing problems, partial covering problems and problems that are neither closed under taking minors, nor contractions. To the best of our knowledge for many of these problems including CYCLE PACKING, VERTEX-H-PACKING, MAXIMUM LEAF SPANNING TREE, and PARTIAL r-DOMINATING SET no EPTASs on planar graphs were previously known.
4b34c1f7d288eaa824e588aabddc83afc200bf62	Functional dependencies (FDs) and inclusion dependencies (INDs) are the most fundamental integrity constraints that arise in practice in relational databases. A given set of FDs does not interact with a given set of INDs if logical implication of any FD can be determined solely by the given set of FDs, and logical implication of any IND can be determined solely by the given set of INDs. We exhibit a necessary condition and two novel sucient conditions for a set of FDs and a set of proper circular INDs not to interact; these two sucient conditions are orthogonal to known results in the database literature. We also discuss the diculty in obtaining a syntactic necessary and sucient condition for no interaction between FDs and INDs.
4dae3150920e09732e0fe23134c5707bab7feb6a	We investigate using the Mondriaan matrix partitioner for unweighted graph partitioning in the communication volume and edge-cut metrics. By converting the unweighted graphs to appropriate matrices, we measure Mondriaan’s performance as a graph partitioner for the 10th DIMACS challenge on graph partitioning and clustering. We find that Mondriaan can effectively be used as a graph partitioner: w.r.t. the edge-cut metric, Mondriaan’s best results are on average within 13% of the best known results as listed in Chris Walshaw’s partitioning archive, but it is an order of magnitude slower than dedicated graph partitioners.
520633c68777988873f5aa011df45a5289c04217	We describe a pure divide-and-conquer parallel algorithm for computing 3D convex hulls. We implement that algorithm on GPU hardware, and find a significant speedup over comparable CPU implementations.
546f8d1fbae5e8a7a20195efacea24bd3183b708	This paper studies the interaction between knowledge, time and coordination in systems in which timing information is available. Necessary conditions are given for the causal structure in coordination problems consisting of orchestrating a set of actions in a manner that satisfies a variety of temporal ordering assumptions. Results are obtained in two main steps: A specification of coordination is shown to require epistemic properties, and the causal structure required to obtain these properties is characterised via “knowledge gain” theorems. A new causal structure called a centibroom structure is presented, generalising previous causal structures for this model. It is shown to capture coordination tasks in which a sequence of clusters of events is performed in linear order, while within each cluster all actions must take place simultaneously. This form of coordination is shown to require the agents to gain a nested common knowledge of particular facts, which in turn requires a centibroom. Altogether, the results presented provide a broad view of the causal shape underlying partially ordered coordinated actions. This, in turn, provides insight into and can enable the design of efficient solutions to the coordination tasks in question.
557e25cedbf8f9d55ae1a8919eb109dd96dedaa5	A common approach to distributed control design is to impose sparsity constraints on the controller structure. Such constraints, however, may greatly complicate the control design procedure. This paper puts forward an alternative structure, which is not sparse yet might nevertheless be well suited for distributed control purposes. The structure appears as the optimal solution to a class of coordination problems arising in multi-agent applications. The controller comprises a diagonal (decentralized) part, complemented by a rank-one coordination term. Although this term relies on information about all subsystems, its implementation only requires a simple averaging operation.
588bef69f7e78fea84fc0ad7bb9e85f13dd61217	In the first of this pair of papers, it was proven that there cannot be a physical computer to which one can properly pose any and all computational tasks concerning the physical universe. It was then further proven that no physical computer C can correctly carry out all computational tasks that can be posed to C. As a particular example, this result means that no physical computer that can, for any physical system external to that computer, take the specification of that external system’s state as input and then correctly predict its future state before that future state actually occurs; one cannot build a physical computer that can be assured of correctly “processing information faster than the universe does”. These results do not rely on systems that are infinite, and/or non-classical, and/or obey chaotic dynamics. They also hold even if one uses an infinitely fast, infinitely dense computer, with computational powers greater than that of a Turing Machine. This generality is a direct consequence of the fact that a novel definition of computation — “physical computation” — is needed to address the issues considered in these papers, which concern real physical computers. While this novel definition does not fit into the traditional Chomsky hierarchy, the mathematical structure and impossibility results associated with it have parallels in the mathematics of the Chomsky hierarchy. This second paper of the pair presents a preliminary exploration of some of this mathematical structure. Analogues of Chomskian results concerning universal Turing Machines and the Halting theorem are derived, as are results concerning the (im)possibility of certain kinds of error-correcting codes. In addition, an analogue of algorithmic information complexity, “prediction complexity”, is elaborated. A task-independent bound is derived on how much the prediction complexity of a computational task can differ for two different reference universal physical computers used to solve that task, a bound similar to the “encoding” bound governing how much the algorithm information complexity of a Turing machine calculation can differ for two reference universal Turing machines. Finally, it is proven that either the Hamiltonian of our universe proscribes a certain type of computation, or prediction complexity is unique (unlike algorithmic information complexity), in that there is one and only version of it that can be applicable throughout our universe.
5c55cc6b449bcc6b8a6cf189452135796e7a0346	In this paper, we are interested in the number of fixed points of functions f : An → An over a finite alphabet A defined on a given signed digraph D. We first use techniques from network coding to derive some lower bounds on the number of fixed points that only depends on D. We then discover relationships between the number of fixed points of f and problems in coding theory, especially the design of codes for the asymmetric channel. Using these relationships, we derive upper and lower bounds on the number of fixed points, which significantly improve those given in the literature. We also unveil some interesting behaviour of the number of fixed points of functions with a given signed digraph when the alphabet varies. We finally prove that signed digraphs with more (disjoint) positive cycles actually do not necessarily have functions with more fixed points.
5c7eebc8ba8fe8df00f54496ab743ede61314419	The Bayesian framework is a well-studied and successful framework for inductive reasoning, which includes hypothesis testing and confirmation, parameter estimation, sequence prediction, classification, and regression. But standard statistical guidelines for choosing the model class and prior are not always available or can fail, in particular in complex situations. Solomonoff completed the Bayesian framework by providing a rigorous, unique, formal, and universal choice for the model class and the prior. I discuss in breadth how and in which sense universal (non-i.i.d.) sequence prediction solves various (philosophical) problems of traditional Bayesian sequence prediction. I show that Solomonoff’s model possesses many desirable properties: Strong total and future bounds, and weak instantaneous bounds, and in contrast to most classical continuous prior densities has no zero p(oste)rior problem, i.e. can confirm universal hypotheses, is reparametrization and regrouping invariant, and avoids the old-evidence and updating problem. It even performs well (actually better) in non-computable environments.
5f17cac51538fc860379e2a4887586757be6182e	In contrast to the classical cyclic prefix (CP)-OFDM, the time domain synchronous (TDS)-OFDM employs a known pseudo noise (PN) sequence as guard interval (GI). Conventional channel estimation methods for TDS-OFDM are based on the exploitation of the PN sequence and consequently suffer from intersymbol interference (ISI). This paper proposes a novel dataaided channel estimation method which combines the channel estimates obtained from the PN sequence and, most importantly, additional channel estimates extracted from OFDM data symbols. Data-aided channel estimation is carried out using the rebuilt OFDM data symbols as virtual training sequences. In contrast to the classical turbo channel estimation, interleaving and decoding functions are not included in the feedback loop when rebuilding OFDM data symbols thereby reducing the complexity. Several improved techniques are proposed to refine the data-aided channel estimates, namely one-dimensional (1- D)/two-dimensional (2-D) moving average and Wiener filtering. Finally, the MMSE criteria is used to obtain the best combination results and an iterative process is proposed to progressively refine the estimation. Both MSE and BER simulations using specifications of the DTMB system are carried out to prove the effectiveness of the proposed algorithm even in very harsh channel conditions such as in the single frequency network (SFN) case.
6cf3ea2c96208d8360fa8e4187b0ddf06cffba70	Planetary exploration systems, operating under severe environmental and operating conditions, have thus far successfully employed carefully calibrated stereo cameras and manipulators to achieve desired precision in instrument placement activities. However, the environmental and functional restrictions imposed by the remote operation of semi-autonomous robots in a harsh planetary atmosphere for long periods of time limit the ultimate operational accuracy of this approach. This paper builds on an algorithm, referred to as Hybrid Image-Plane/Stereo (HIPS), developed to optimize the positioning accuracy of a manipulator. The HIPS method generates camera models through direct visual sensing of the manipulator end-effector. It estimates and subsequently uses these models to position the manipulator at a target location specified in the image-planes of a stereo camera pair using stereo correlation and triangulation. While positioning control of manipulation systems is important, orientation control of these systems is also crucial. Many planetary manipulation tasks being considered for the Mars Science Laboratory rover, due to launch in 2009, and subsequent missions, will require precise orientation control of manipulators. This paper studies the effect of using HIPS models to control the position and orientation of manipulator end-effector. As seen in previous position control experiments, the static version of the HIPS technique reduces position error by almost an order of magnitude. This paper additionally shows that orientation error is reduced by almost a factor of two.
6e5228a2c4c5d2bd25e92b2f42269c9b3e40bbdf	We present a new system, called Cirrin, for pen input of ASCII characters using word-level unistrokes. Our system addresses the tradeoff between speed and accuracy of penbased text entry by substituting precision on the part of the user for ease of recognitionon the part of the computer. Cirrin supports ease of recognition by the computer combined with natural, script-like input. This paper discusses the design space of word-level, unistroke input, focusing on the choices made in the circular model of Cirrin that is currently in daily use by the first author.
6ee7d70f2dbfc0d45fbf20485f82a9ed7e175725	Convolutional neural nets (convnets) trained from massive labeled datasets [1] have substantially improved the state-of-the-art in image classification [2] and object detection [3]. However, visual understanding requires establishing correspondence on a finer level than object category. Given their large pooling regions and training from whole-image labels, it is not clear that convnets derive their success from an accurate correspondence model which could be used for precise localization. In this paper, we study the effectiveness of convnet activation features for tasks requiring correspondence. We present evidence that convnet features localize at a much finer scale than their receptive field sizes, that they can be used to perform intraclass alignment as well as conventional hand-engineered features, and that they outperform conventional features in keypoint prediction on objects from PASCAL VOC 2011 [4].
6f9167ddb392a43a7e36a2df1feefa184d82763e	Recently an increasing amount of research is devoted to the question of how the most influential nodes (seeds) can be found effectively in a complex network. There are a number of measures proposed for this purpose, for instance, high-degree centrality measure reflects the importance of the network topology and has a reasonable runtime performance to find a set of nodes with highest degree, but they do not have a satisfactory dissemination potentiality in the network due to having many common neighbors (CN(1)) and common neighbors of neighbors (CN(2)). This flaw holds in other measures as well. In this paper, we compare high-degree centrality measure with other well-known measures using ten datasets in order to find a proportion for the common seeds in the seed sets obtained by them. We, thereof, propose an improved high-degree centrality measure (named DegreeDistance) and improve it to enhance accuracy in two phases, FIDD and SIDD, by putting a threshold on the number of common neighbors of already-selected seed nodes and a non-seed node which is under investigation to be selected as a seed as well as considering the influence score of seed nodes directly or through their common neighbors over the non-seed node. To evaluate the accuracy and runtime performance of DegreeDistance, FIDD, and SIDD, they are applied to eight large-scale networks and it finally turns out that SIDD dramatically outperforms other well-known measures and evinces comparatively more accurate performance in identifying the most influential nodes.
765f6ca92d5c228847c2ceb37b756ecf980c95a4	At the birth of participatory design, there was a strong political consciousness surrounding the design of new technology, the design process in particular, establishing a rich set of methods and tools for user-centered design. Today, the term design has extended its scope of concern beyond the process of design and into how users interact with the designed product on a day-to-day basis. This paper is an attempt to call to attention the need for a new set of methods, attitudes and approaches, along with the existing, to discuss, analyze and reflect upon the politics at the interface. By presenting a critical analysis of two design cases, we elicit the importance of such an agenda and the implications for design in doing so. We use the Foucauldian notion of power to analyze the power relationships in these two cases and to articulate the politics at the interface. We conclude by emphasizing the need for furthering this agenda and outlining future work.
80fecc3c40948d36b3de4dfffa28b62f8da52edd	This paper presents a new interaction technique for browsing large visual information bases in a collaborative environment. The ATELIER project deals with learning environments for architecture and interaction design students. Since students’ attitude is to collect large amounts of data, pictures and videos in particular, an important issue is how to keep information organized according to intuitive criteria has been seriously taken into account. This paper shows how we managed to align different approaches for browsing large image sets and describes the interfaces used to seamlessly switch from one view to another. We also implemented search facilities based both on color layout and on keywords taken from an ontology. Integration was achieved by means of physical handles, i.e., barcodes.
8ab8bb8aa41151b85ab367f1152ff17cc17b87d1	A permutation graph is an intersection graph of segments lying between two parallel lines. A Seidel complementation of a finite graph at a vertex v consists in complementing the edges between the neighborhood and the non-neighborhood of v. Two graphs are Seidel complement equivalent if one can be obtained from the other by a sequence of Seidel complementations. In this paper we introduce the new concept of Seidel complementation and Seidel minor. We show that this operation preserves cographs and the structure of modular decomposition. The main contribution of this paper is to provide a new and succinct characterization of permutation graphs namely, a graph is a permutation graph if and only if it does not contain any of the following graphs: C5, C7, XF2 6 , XF2n+3 5 , C2n, n > 6 and their complements as a Seidel minor. This characterization is in a sense similar to Kuratowski’s characterization [15] of planar graphs by forbidden topological minors.
8c1c76248e128c7e5789c7f0a9a89fbba17e4c11	I describe a simple modification which can be applied to any citation count-based index (e.g. Hirsch’s h-index) quantifying a researcher’s publication output. The key idea behind the proposed approach is that the merit for the citations of a paper should be distributed amongst its authors according to their relative contributions. In addition to producing inherently fairer metrics I show that the proposed modification has the potential to partially normalize for the unfair effects of honorary authorship and thus discourage this practice.
8e0ee021514e2029430159fed8a3340bc2327b37	In this paper, we present two near-optimal methods to determine the real-time collision-free path for a mobile vehicle moving in a dynamically changing environment. The proposed designs are based on the polynomial parameterization of feasible trajectories by explicitly taking into account boundary conditions, kinematic constraints, and collision-avoidance criteria. The problems of finding optimal solutions to the parameterized feasible trajectories are then formulated with respect to a near-minimal control-energy performance index and a near-shortest distance performance index, respectively. The obtained optimal solutions are analytical and suitable for practical applications which may require real-time trajectory planning and replanning. Computer simulations are provided to validate the effectiveness of the proposed near-optimal trajectory-planning methods.
8e924eec08583abd0fdabb45a56346ab4f467aa1	The cut-elimination method CERES (for first- and higherorder classical logic) is based on the notion of a characteristic clause set, which is extracted from an LK-proof and is always unsatisfiable. A resolution refutation of this clause set can be used as a skeleton for a proof with atomic cuts only (atomic cut normal form). This is achieved by replacing clauses from the resolution refutation by the corresponding projections of the original proof. We present a generalization of CERES (called CERESs) to first-order proof schemata and define a schematic version of the sequent calculus called LKSE , and a notion of proof schema based on primitive recursive definitions. A method is developed to extract schematic characteristic clause sets and schematic projections from these proof schemata. We also define a schematic resolution calculus for refutation of schemata of clause sets, which can be applied to refute the schematic characteristic clause sets. Finally the projection schemata and resolution schemata are plugged together and a schematic representation of the atomic cut normal forms is obtained. A major benefit of CERESs is the extension of cut-elimination to inductively defined proofs: we compare CERESs with standard calculi using induction rules and demonstrate that CERESs is capable of performing cut-elimination where traditional methods fail. The algorithmic handling of CERESs is supported by a recent extension of the CERES system.
95e5fe77778b2fdb3918af94d27e1bb461f66515	A key idea in coding for the broadcast channel (BC) is binning, in which the transmitter encode information by selecting a codeword from an appropriate bin (the messages are thus the bin indexes). This selection is normally done by solving an appropriate (possibly difficult) combinatorial problem. Recently it has been shown that binning for the Blackwell channel –a particular BC– can be done by iterative schemes based on Survey Propagation (SP). This method uses decimation for SP and suffers a complexity of ✂✁☎✄✝✆✟✞ . In this paper we propose a new variation of the Belief Propagation (BP) algorithm, named Reinforced BP algorithm, that turns BP into a solver. Our simulations show that this new algorithm has complexity ✠✁☎✄☛✡✌☞✎✍✏✄✝✞ . Using this new algorithm together with a non-linear coding scheme, we can efficiently achieve rates close to the border of the capacity region of the Blackwell channel.
97a6613da7eeb0ac4f70ae2e3b793364c794e6dc	Medical image registration is a difficult problem. Not only a registration algorithm needs to capture both large and small scale image deformations, it also has to deal with global and local image intensity variations. In this paper we describe a new multiresolution elastic image registration method that challenges these difficulties in image registration. To capture large and small scale image deformations, we use both global and local affine transformation algorithms. To address global and local image intensity variations, we apply an image intensity standardization algorithm to correct image intensity variations. This transforms image intensities into a standard intensity scale, which allows highly accurate registration of medical images.
97c4cc36e62d0ca4f5eddde679c0edec6fcb8c51	The recent emergence of Cloud Computing has drastically altered everyone’s perception of infrastructure architectures, software delivery and development models. Projecting as an evolutionary step, following the transition from mainframe computers to client/server deployment models, cloud computing encompasses elements from grid computing, utility computing and autonomic computing, into an innovative deployment architecture. This rapid transition towards the clouds, has fuelled concerns on a critical issue for the success of information systems, communication and information security. From a security perspective, a number of unchartered risks and challenges have been introduced from this relocation to the clouds, deteriorating much of the effectiveness of traditional protection mechanisms. As a result the aim of this paper is twofold; firstly to evaluate cloud security by identifying unique security requirements and secondly to attempt to present a viable solution that eliminates these potential threats. This paper proposes introducing a Trusted Third Party, tasked with assuring specific security characteristics within a cloud environment. The proposed solution calls upon cryptography, specifically Public Key Infrastructure operating in concert with SSO and LDAP, to ensure the authentication, integrity and confidentiality of involved data and communications. The solution, presents a horizontal level of service, available to all implicated entities, that realizes a security mesh, within which essential trust is maintained.
9aeb7f55d4050111022c91d86cbb436273fa79d3	Let G be an embedded planar graph whose edges may be curves. The detour between two points, p and q (on edges or vertices) of G, is the ratio between the shortest path in G between p and q and their Euclidean distance. The supremum over all pairs of points of all these ratios is called the geometric dilation of G. Our research is motivated by the problem of designing graphs of low dilation. We provide a characterization of closed curves of constant halving distance (i.e., curves for which all chords dividing the curve length in half are of constant length) which are useful in this context. We then relate the halving distance of curves to other geometric quantities such as area and width. Among others, this enables us to derive a new upper bound on the geometric dilation of closed curves, as a function of D/w, where D and w are the diameter and width, respectively. We further give lower bounds on the geometric dilation of polygons with n sides as a function of n. Our bounds are tight for centrally symmetric convex polygons.
9c039c648b93202f920537fb88480779ee0a37e2	Online community administrators are attempting to encourage their users to contribute knowledge and resources in order to provide value to members and ensure sustainability. A large number of online communities fail mainly due to the reluctance of users to share knowledge in them. Many studies on this topic have highlighted the importance of reciprocity for knowledge contribution. However, it is unclear how reciprocity is developed and what influences its development. Motivated by this concern, this study focuses on investigating the antecedents of knowledge receivers’ reciprocity in online communities. It formulates and tests a theoretical model to explain reciprocity behaviour of knowledge receivers based on equity theory and Social Identity explanation of Deindividuation Effects (SIDE) model. Our proposed model is validated through a large-scale survey in an online forum for English language learning. The results reveal that indebtedness and community norm not only are key antecedents of intention to reciprocate but are also positively related to each other. The perceived anonymity of the online community not only has a positive effect on indebtedness and intention to reciprocate, but also has an interactive effect with community norm on intention to reciprocate. Theoretical and practical implications of this study are discussed.
9e59d03fbb534832ced523250ee429f41893ab39	Commutative encryption is a useful but rather strict notion in cryptography. In this paper, we define a loose variation of commutative encryption-commutative-like encryption and give an example: the generalization of ElGamal scheme. The application of the new variation is also discussed.
a98488969aed4d6add1115ce18c19c89b4826a92	The synchrosqueezing method aims at decomposing 1D functions as superpositions of a small number of “Intrinsic Modes”, supposed to be well separated both in time and frequency. Based on the unidimensional wavelet transform and its reconstruction properties, the synchrosqueezing transform provides a powerful representation of multicomponent signals in the time-frequency plane, together with a reconstruction of each mode. In this paper, a bidimensional version of the synchrosqueezing transform is defined, by considering a well–adapted extension of the concept of analytic signal to images: the monogenic signal. The natural bidimensional counterpart of the notion of Intrinsic Mode is then the concept of “Intrinsic Monogenic Mode” that we define. Thereafter, we investigate the properties of its associated Monogenic Wavelet Decomposition. This leads to a natural bivariate extension of the Synchrosqueezed Wavelet Transform, for decomposing and processing multicomponent images. Numerical tests validate the effectiveness of the method for different examples.
aeb717fbb9aac3501236bce498cbf8b98f5d8926	For storing a word or the whole text segment, we need a huge storage space. Typically a character requires 1 Byte for storing it in memory. Compression of the memory is very important for data management. In case of memory requirement compression for text data, loseless memory compression is needed. We are suggesting a lossless memory requirement compression method for text data compression. The proposed compression method will compress the text segment or the text file based on two level approaches firstly reduction and secondly compression. Reduction will be done using a word lookup table not using traditional indexing system, then compression will be done using currently available compression methods. The word lookup table will be a part of the operating system and the reduction will be done by the operating system. According to this method each word will be replaced by an address value. This method can quite effectively reduce the size of persistent memory required for text data. At the end of the first level compression with the use of word lookup table, a binary file containing the addresses will be generated. Since the proposed method does not use any compression algorithm in the first level so this file can be compressed using the popular compression algorithms and finally will provide a great deal of data compression on purely English text data.
b58e020f6c9c834ba83f07322adc75f3756e087f	In the past few years, Reddit – a community-driven platform for submitting, commenting and rating links and text posts – has grown exponentially, from a small community of users into one of the largest online communities on the Web. To the best of our knowledge, this work represents the most comprehensive longitudinal study of Reddit’s evolution to date, studying both (i) how user submissions have evolved over time and (ii) how the community’s allocation of attention and its perception of submissions have changed over 5 years based on an analysis of almost 60 million submissions. Our work reveals an ever-increasing diversification of topics accompanied by a simultaneous concentration towards a few selected domains both in terms of posted submissions as well as perception and attention. By and large, our investigations suggest that Reddit has transformed itself from a dedicated gateway to the Web to an increasingly self-referential community that focuses on and reinforces its own user-generated image- and textual content over external sources.
b5a189b46ae26c36de0fd78050f2490a786057bc	Semi-supervised support vector machines (S3VMs) are a kind of popular approaches which try to improve learning performance by exploiting unlabeled data. Though S3VMs have been found helpful in many situations, they may degenerate performance and the resultant generalization ability may be even worse than using the labeled data only. In this paper, we try to reduce the chance of performance degeneration of S3VMs. Our basic idea is that, rather than exploiting all unlabeled data, the unlabeled instances should be selected such that only the ones which are very likely to be helpful are exploited, while some highly risky unlabeled instances are avoided. We propose the S3VM-us method by using hierarchical clustering to select the unlabeled instances. Experiments on a broad range of data sets over eighty-eight different settings show that the chance of performance degeneration of S3VM-us is much smaller than that of existing S3VMs.
b5e6da04c35a586609a46bbbd7b1ad031a658b08	This work presents a distributed method for control centers to monitor the operating condition of a power network, i.e., to estimate the network state, and to ultimately determine the occurrence of threatening situations. State estimation has been recognized to be a fundamental task for network control centers to ensure correct and safe functionalities of power grids. We consider (static) state estimation problems, in which the state vector consists of the voltage magnitude and angle at all network buses. We consider the state to be linearly related to network measurements, which include power flows, current injections, and voltages phasors at some buses. We admit the presence of several cooperating control centers, and we design two distributed methods for them to compute the minimum variance estimate of the state given the network measurements. The two distributed methods rely on different modes of cooperation among control centers: in the first method an incremental mode of cooperation is used, whereas, in the second method, a diffusive interaction is implemented. Our procedures, which require each control center to know only the measurements and structure of a subpart of the whole network, are computationally efficient and scalable with respect to the network dimension, provided that the number of control centers also increases with the network cardinality. Additionally, a finite-memory approximation of our diffusive algorithm is proposed, and its accuracy is characterized. Finally, our estimation methods are exploited to develop a distributed algorithm to detect corrupted data among the network measurements.
b8f9f7dd41837eb34793a6c5fd7703792f792288	Recently, Fan et al. proposed an anonymous multireceiver encryption scheme (FHH) , and they declared that their scheme achieves confidentiality and anonymity. In this letter, We point out that the FHH scheme does not hold the defined security properties. In particular, we state that the FHH scheme does not hold the anonymity but achieves the weaker confidentiality of IND-CPA.
bcae858633935c727739a73447b50b40b7c52794	The issue of single range based observability analysis and observer design for the kinematics model of a 3D vehicle eventually subject to a constant unknown drift velocity is addressed. The proposed method departs from alternative solutions to the problem and leads to the definition of a linear time invariant state equation with a linear time varying output that can be used to globally solve the original nonlinear state estimation with a standard Kalman filter. Simple necessary and sufficient observability conditions are derived. Numerical simulation examples are described to illustrate the performance of the method.
bdb1b4128730838eb2fed83829f46a9077eca9f7	Transportation processes, which play a prominent role in the life and social sciences, are typically described by discrete models on lattices. For studying their dynamics a continuous formulation of the problem via partial differential equations (PDE) is employed. In this paper we propose a symbolic computation approach to derive mean-field PDEs from a latticebased model. We start with the microscopic equations, which state the probability to find a particle at a given lattice site. Then the PDEs are formally derived by Taylor expansions of the probability densities and by passing to an appropriate limit as the time steps and the distances between lattice sites tend to zero. We present an implementation in a computer algebra system that performs this transition for a general class of models. In order to rewrite the mean-field PDEs in a conservative formulation, we adapt and implement symbolic integration methods that can handle unspecified functions in several variables. To illustrate our approach, we consider an application in crowd motion analysis where the dynamics of bidirectional flows are studied. However, the presented approach can be applied to various transportation processes of multiple species with variable size in any dimension, for example, to confirm several proposed mean-field models for cell motility.
cc0b1d8e8108e7f37702ea6f649b1d15e4382a95	The ALIZ-E project’s goal is to design a robot companion able to maintain affective interactions with young users over a period of time. One of these interactions consists in teaching a dance to hospitalized children according to their capabilities. We propose a methodology for adapting both, the movements used in the dance based on the user’s cognitive and physical capabilities through a set of metrics, and the robot’s interaction based on the user’s personality traits.
d3ba6b48f62e2fe1802efb46c3799362572eeb1d	By characterizing the worst case profile, which maximizes the content of a buffer fed with leaky bucket regulated flows in packet telecommunication networks, we derive a tight upper bound in the many-sources regime for the tail distribution of the workload generated by these flows in a FIFO queue with constant service rate. Furthermore, we compare this workload distribution with an M/G/1 queue and get insights on the better-than-Poisson property of regulated flows. We conclude that the superposition of independent regulated flows generates an asymptotically smaller workload than a marked Poisson process whose service times and intensity depend on the parameters of regulated sources.
d7e464a1e466fb04e72a961c24d10ecf65c20890	A number of representation schemes have been presented for use within Learning Classifier Systems, ranging from binary encodings to neural networks. This paper presents results from an investigation into using discrete and fuzzy dynamical system representations within the XCSF Learning Classifier System. In particular, asynchronous Random Boolean Networks are used to represent the traditional condition-action production system rules in the discrete case and asynchronous Fuzzy Logic Networks in the continuous-valued case. It is shown possible to use self-adaptive, open-ended evolution to design an ensemble of such dynamical systems within XCSF to solve a number of well-known test problems.
d9f69149832fa4ceb0ab3b0285863ae2c8977d2e	Possibilistic answer set programming (PASP) extends answer set programming (ASP) by attaching to each rule a degree of certainty. While such an extension is important from an application point of view, existing seman- tics are not well-motivated, and do not al- ways yield intuitive results. To develop a more suitable semantics, we first introduce a characterization of answer sets of classi- cal ASP programs in terms of possibilistic logic where an ASP program specifies a set of constraints on possibility distributions. This characterization is then naturally generalized to define answer sets of PASP programs. We furthermore provide a syntactic counterpart, leading to a possibilistic generalization of the well-known Gelfond-Lifschitz reduct, and we show how our framework can readily be im- plemented using standard ASP solvers.
ddf1ea128fbc14a203c3b3d44d0135bb4dc33ffe	Since its introduction by Gauss, Matrix Algebra has facilitated understanding of scientific problems, hiding distracting details and finding more elegant and efficient ways of computational solving. Todays largest problems, which often originate from multidimensional data, might profit from even higher levels of abstraction. We developed a framework for solving tensor structured problems with tensor algebra that unifies concepts from tensor analysis, multilinear algebra and multidimensional signal processing. In contrast to the conventional matrix approach, it allows the formulation of multidimensional problems, in a multidimensional way, preserving structure and data coherence; and the implementation of automated optimizations of solving algorithms, based on the commutativity of all tensor operations. Its ability to handle large scientific tasks is showcased by a real-world, 4D medical imaging problem, with more than 30 million unknown parameters solved on a current, inexpensive hardware. This significantly surpassed the best published matrix-based approach.
e88ac0622f65a56cecc29c22a5c59eb1bdd07457	One of the key problems in model-based reinforcement learning is balancing exploration and exploitation. Another is learning and acting in large relational domains, in which there is a varying number of objects and relations between them. We provide a solution to exploring large relational Markov decision processes by developing relational extensions of the concepts of the Explicit Explore or Exploit (E 3 ) algorithm. A key insight is that the inherent generalization of learnt knowledge in the relational representation has profound implications also on the exploration strategy: what in a propositional setting would be considered a novel situation and worth exploration may in the relational setting be an instance of a well-known context in which exploitation is promising. Our experimental evaluation shows the effectiveness and benefit of relational exploration over several propositional benchmark approaches on noisy 3D simulated robot manipulation problems.
