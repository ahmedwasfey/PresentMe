{
  "aaai14_7": {
    "figures": [
      {
        "caption": "Figure 2. Simultaneous relationships",
        "name": "2",
        "region_bb": [
          65.52,
          421.2,
          275.76,
          520.5600000000001
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          92.16,
          524.88,
          254.16,
          534.24
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1. An example of a Raven\u2019s problem",
        "name": "1",
        "region_bb": [
          393.12,
          316.8,
          480.24,
          443.52000000000004
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          347.76,
          458.64000000000004,
          538.5600000000001,
          468.0
        ],
        "page": 1,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 2. The Effect of Confidence on Score and Ambiguity",
        "name": "2",
        "region_bb": [
          117.36,
          543.6,
          493.92,
          675.36
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          226.8,
          685.44,
          419.04,
          692.64
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Figure 3. Mutual Fractal Representations",
        "name": "3",
        "region_bb": [
          316.8,
          297.36,
          550.8000000000001,
          467.28000000000003
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          346.32,
          480.96000000000004,
          529.9200000000001,
          490.32
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Yellow indicates ambiguous results, red indicates that the result is unambiguous",
        "name": "1",
        "region_bb": [
          48.96,
          252.0,
          290.16,
          629.28
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          55.440000000000005,
          649.44,
          291.6,
          680.4
        ],
        "page": 4,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7
    ]
  },
  "nips12_6": {
    "figures": [
      {
        "caption": "Figure 2: Difficult and easy stimuli.",
        "name": "2",
        "region_bb": [
          373.68,
          131.76,
          501.84000000000003,
          205.92000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          384.48,
          211.68,
          495.36,
          219.6
        ],
        "page": 3,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 5: Train and test polynomial stimuli used in experiments 2 through 6.",
        "name": "5",
        "region_bb": [
          108.72,
          652.32,
          503.28000000000003,
          708.48
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          167.04,
          712.08,
          444.24,
          720.72
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 8: Results of the optimization task 2. a and b) distance of human and random clicks from locations of max Q (i.e., GP mean and max GP std). c) actual function and GP mean values at human and random clicks. d) normalized GP standard deviation at human vs. random clicks. Errors bars show s.e.m over test trials.",
        "name": "8",
        "region_bb": [
          108.72,
          72.0,
          503.28000000000003,
          200.16
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          107.28,
          206.64000000000001,
          504.0,
          234.0
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 9: Exploration vs. exploitation balance in Optimization 3 task. clicks, implying that humans chose infor- mative clicks regarding optimization (figure inset). Humans converge to the maximum location",
        "name": "9",
        "region_bb": [
          275.04,
          83.52,
          508.32,
          222.48000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          292.32,
          226.08,
          497.52000000000004,
          234.72
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 4: Results of our second-level analysis. The lower the distance and the higher the agreement, the better (red arrows). Boxes represent median (red line) and 25 th, 75 th percentiles. Panel (e) shows average regret of algorithms and humans (f \u2217 is normalized to f max for each trial separately).",
        "name": "4",
        "region_bb": [
          108.0,
          80.64,
          503.28000000000003,
          152.64000000000001
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          156.96,
          504.0,
          183.6
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: Left: a sample search trial. The unknown function (blue curve) Sinc, etc. The goal was to cover many was only displayed at the end of training trials. During search for the function\u2019s maximum, a red dot at (x, f (x)) was drawn for each x selected by participants. cases and to investigate the generaliza- Right: A sample function and the pdf of human clicks. tion power of algorithms and humans. To generate a polynomial stimulus of degree m (m > 2), we randomly generated m + 1 pairs",
        "name": "1",
        "region_bb": [
          277.2,
          227.52,
          504.0,
          310.32
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          277.92,
          313.2,
          503.28000000000003,
          345.6
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 1: Baseline algorithms. We set maxItr",
        "name": "1",
        "region_bb": [
          337.68,
          269.28000000000003,
          503.28000000000003,
          416.88
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          336.24,
          249.12,
          503.28000000000003,
          267.12
        ],
        "page": 3,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Figure 7: a) Mean distance of human clicks from models. Errors bars show standard error of the mean (s.e.m) over test trials. Inset shows the standard deviation of humans and the GP model at x = 0. b) mean GP std at human vs. random clicks in active interpolation. c and d correspond to a and b, for the extrapolation task.",
        "name": "7",
        "region_bb": [
          108.72,
          560.16,
          503.28000000000003,
          685.44
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          691.9200000000001,
          503.28000000000003,
          720.72
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 6: Illustration of experiments. In extrapolation, polynomials (degrees 1, 2 & 3) fail to explain our data.",
        "name": "6",
        "region_bb": [
          109.44,
          72.0,
          503.28000000000003,
          130.32
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.72,
          134.64000000000001,
          502.56,
          143.28
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3: Human vs. algorithm 1D search accuracy.",
        "name": "3",
        "region_bb": [
          303.84000000000003,
          318.24,
          502.56,
          506.88
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          313.2,
          510.48,
          501.12,
          519.12
        ],
        "page": 4,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "icml14_1": {
    "figures": [
      {
        "caption": "Table 2: Tab. (a) presents summary statistics for the forum data. Tab. (b) presents results on the forum data for author-based and discussion topic-based clustering. Note that small VI is desirable. For author-based clustering, one pass over the data was sufficient for online algorithms. For topic-based clustering, we report results with one pass as well as five passes (last column) during training for online algorithms (note that one pass vs five passes distinction only holds for online clustering algorithms; for batch techniques we make ten passes.) All the results are scaled by 100. In all cases, L 3 M (tuned \u03b3) is statistically significantly better than all other approaches.",
        "name": "2",
        "region_bb": [
          84.24000000000001,
          315.36,
          522.0,
          414.72
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          55.440000000000005,
          428.40000000000003,
          540.72,
          484.56
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 1: Performance on ACE 2004 and OntoNotes-5.0. Corr-Clustering is proposed by Finley & Joachims (2005); Spanning Forest is the latent spanning forest-based approach by Yu & Joachims (2009); Sum-Link is an online clustering technique by Haider et al. (2007); Bin-Left-Link uses a Best-Left-Link inference and the training strategy by Bengtson & Roth (2008). Our proposed approach is L 3 M\u2014 L 3 M with tuned \u03b3 is when we tune the value of \u03b3 using a development set; L 3 M (\u03b3 = 0) is with \u03b3 fixed to 0. Corr-Clustering and Spanning Forest are batch clustering techniques. Sum-Link, Bin-Left-Link, L 3 M (tuned \u03b3), and L 3 M (\u03b3 = 0) are online clustering techniques. \u201c(1 pass)\u201d means when trained with just one pass over the data.",
        "name": "1",
        "region_bb": [
          89.28,
          66.96000000000001,
          507.6,
          221.76000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.72,
          233.28,
          540.72,
          301.68
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "icml13_10": {
    "figures": [
      {
        "caption": "Figure 2. Reduction from 3-dimensional matching.",
        "name": "2",
        "region_bb": [
          350.64,
          66.96000000000001,
          494.64000000000004,
          114.48
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          322.56,
          126.0,
          525.6,
          134.64000000000001
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 5. Relative error of learned parameters versus run- ning time for different EM algorithms (MAP and MCEM: 100 EM iterations, SAEM: 600 EM iterations, M = 1000).",
        "name": "5",
        "region_bb": [
          321.12,
          69.84,
          528.48,
          236.16
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          308.16,
          253.44,
          540.72,
          282.96000000000004
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 1. Comparison of Gibbs vs. MAP: seconds to achieve the same relative error compared with reference solution.",
        "name": "1",
        "region_bb": [
          55.440000000000005,
          112.32000000000001,
          293.76,
          167.04
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          56.160000000000004,
          76.32000000000001,
          288.72,
          95.04
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Figure 4. Relative difference from reference solution versus time (log scale) for MAP and Gibbs (M = 1000, L = 49). Gibbs and MAP: average over 10 trials; 95% confidence intervals are negligible.",
        "name": "4",
        "region_bb": [
          98.64,
          186.48000000000002,
          239.04000000000002,
          303.12
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          56.160000000000004,
          318.24,
          288.72,
          360.0
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1. CGM example: (a) Individuals are explicitly modeled. (b) After marginalization, the hidden variables",
        "name": "1",
        "region_bb": [
          84.96000000000001,
          66.96000000000001,
          260.64,
          178.56
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          56.160000000000004,
          198.0,
          288.72,
          226.08
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3. The effect of population size M on accuracy and running time of approximate MAP (L = 4, T = 6). Left:",
        "name": "3",
        "region_bb": [
          337.68,
          72.0,
          510.48,
          172.8
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          308.16,
          187.20000000000002,
          540.0,
          217.44
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "nips12_3": {
    "figures": [
      {
        "caption": "Figure 2: Real Network Experiments. Training and heldout variational lower bound (equivalent to perplex- ity) convergence plots for all experiments in Table 3. Each plot shows both lower bounds over 10 data passes (i.e. 100 iterations with 10% minibatches). In all cases, we observe convergence between 2-5 data passes, and the shape of the heldout curve closely mirrors the training curve (i.e. no overfitting).",
        "name": "2",
        "region_bb": [
          125.28,
          205.20000000000002,
          484.56,
          351.36
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          350.64,
          503.28000000000003,
          393.84000000000003
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: Synthetic Experiments. Left/Center: Latent space recovery accuracy (measured using Normalized Mutual Information) and runtime per data pass for our method and baselines. With the MMTM/PTM Gibbs and MMSB Variational algorithms, the larger networks did not complete within 12 hours. The runtime plots for MMSB easy and Power-Law easy experiments are very similar to the hard experiments, so we omit them. Right: Convergence of our stochastic variational algorithm (with 10% minibatches) versus a batch variational version of our algorithm. On N = 1, 000 networks, our minibatch algorithm converges within 1-2 data passes.",
        "name": "1",
        "region_bb": [
          125.28,
          76.32000000000001,
          484.56,
          241.92000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          246.24,
          503.28000000000003,
          304.56
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 3: Real Network Experiments. All networks were taken from the Stanford Large Network Dataset Collection; directed networks were converted to undirected networks via symmetrization. Some networks were run with more than one choice of settings. Runtime is the time taken for 10 data passes (which was more than sufficient for convergence on all networks, see Figure 2).",
        "name": "3",
        "region_bb": [
          110.88000000000001,
          76.32000000000001,
          498.96000000000004,
          160.56
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          159.12,
          503.28000000000003,
          200.16
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 2: Link Prediction Experiments, measured using AUC. Our method performs similarly to MMSB Variational on synthetic data. MMSB performs better on smaller, non-social networks, while we perform better on larger, social networks (or MMSB fails to complete due to lack of scalability). Roget, Odlis and Yeast networks are from Pajek datasets (http://vlado.fmf.uni-lj.si/pub/networks/data/); the rest are from Stanford Large Network Dataset Collection (http://snap.stanford.edu/data/). (even after many iterations and trials), without reaching a good solution 4 . We believe our method",
        "name": "2",
        "region_bb": [
          111.60000000000001,
          306.72,
          498.96000000000004,
          367.92
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          365.04,
          503.28000000000003,
          420.48
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 1: Equivalence classes and conditional probabilities of E ijk given s i,jk , s j,ik , s k,ij (see text for details).",
        "name": "1",
        "region_bb": [
          136.8,
          87.84,
          474.48,
          175.68
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          182.88,
          502.56,
          192.24
        ],
        "page": 3,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "icml14_4": {
    "figures": [],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "icml11_6": {
    "figures": [
      {
        "caption": "Table 1. Generalization errors (in %) on the handwritten character data set for four different classifiers (Bayes clas- sifier, logistic regressor, SVM, and LMNN + k-NN) on three different object embeddings (log-likelihoods of class- specific models, Fisher representations, and FKL represen- tations), using various numbers of hidden states. The table reports the generalization error over 10 folds. Best per- formance for each number of hidden states is typeset in boldface.",
        "name": "1",
        "region_bb": [
          328.32,
          175.68,
          520.5600000000001,
          310.32
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          306.72,
          69.84,
          540.72,
          164.16
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 4. Generalization errors (in %) on the mutagenicity data set for four different classifiers on three different em- beddings. The table reports the generalization error on a fixed 90% training/ 10% test division.",
        "name": "4",
        "region_bb": [
          74.88000000000001,
          123.12,
          270.0,
          257.76
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          55.440000000000005,
          69.84,
          288.72,
          110.88000000000001
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 2. Generalization errors (in %) on the Arabic speech data set for four different classifiers on three different em- beddings. The table reports the generalization error on the fixed training/test division proposed by Hammami & Bedda (2010).",
        "name": "2",
        "region_bb": [
          76.32000000000001,
          134.64000000000001,
          268.56,
          269.28000000000003
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          55.440000000000005,
          69.84,
          288.72,
          122.4
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 3. Generalization errors (in %) on the Cohn-Kanade data set for four different classifiers on three different em- beddings. The table reports the mean generalization error",
        "name": "3",
        "region_bb": [
          326.88,
          120.96000000000001,
          522.0,
          255.60000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          307.44,
          69.84,
          540.72,
          108.72
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8
    ]
  },
  "aaai12_10": {
    "figures": [
      {
        "caption": "Figure 2: Comparison of all bagging predictors against each other with the Nemenyi test, where the x-axis indicates the average rank of the bagging predictors, the y-axis indicates the ascending order of the average rank of CG performance, and the horizontal bars indicate the CD.",
        "name": "2",
        "region_bb": [
          100.08,
          54.0,
          244.8,
          132.48000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.0,
          138.96,
          292.32,
          183.6
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: Designed framework",
        "name": "1",
        "region_bb": [
          366.48,
          216.72,
          509.04,
          309.6
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          382.32,
          318.24,
          495.36,
          326.88
        ],
        "page": 1,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3: Comparison of ROC curve and G \u2212 mean among three selected bagging predictors on diabetes data-set. Figure 3 presents graphical comparisons of G-mean and",
        "name": "3",
        "region_bb": [
          366.48,
          54.0,
          511.20000000000005,
          108.72
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          319.68,
          120.96000000000001,
          557.28,
          138.96
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2
    ]
  },
  "icml14_10": {
    "figures": [
      {
        "caption": "Figure 2. Logistic Regression: Risk in predictive mean.",
        "name": "2",
        "region_bb": [
          61.2,
          67.68,
          284.40000000000003,
          187.92000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          73.44,
          203.76000000000002,
          270.72,
          211.68
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 5. Pitfalls of using uncorrected SGLD",
        "name": "5",
        "region_bb": [
          313.2,
          366.48,
          544.32,
          574.5600000000001
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          343.44,
          590.4,
          505.44,
          599.0400000000001
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 4. RJMCMC: Risk in predictive mean",
        "name": "4",
        "region_bb": [
          87.84,
          74.88000000000001,
          252.72,
          208.08
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          91.44,
          223.20000000000002,
          252.72,
          231.12
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1. Error E estimated using simulation (blue cross with 1 error bar) and dynamic programming (red line). An upper bound (black dashed line) is also shown.",
        "name": "1",
        "region_bb": [
          334.8,
          66.96000000000001,
          516.96,
          174.24
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          307.44,
          191.52,
          540.72,
          221.76000000000002
        ],
        "page": 4,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 6. Test average error in P a and data usage E u [\u00af \u21e1 ] for the ICA experiment using average design over both m and \u270f ( ), with fixed m = 600 (4), and worst-case design (\u21e4).",
        "name": "6",
        "region_bb": [
          95.04,
          334.08,
          250.56,
          593.28
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          55.440000000000005,
          609.12,
          288.72,
          640.08
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3. ICA: Risk in mean of Amari distance",
        "name": "3",
        "region_bb": [
          324.72,
          75.60000000000001,
          516.24,
          219.6
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          339.84000000000003,
          236.16,
          508.32,
          244.8
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "icml13_4": {
    "figures": [
      {
        "caption": "Figure 2. MAE on 100k: user-based RBM (U-RBM), item-based RBM (I-RBM), neighborhood-based boosted with data from I-RBM (I-RBM+INB) and hybrid RBM model (UI-RBM), F U = 40, F I = 40 (right side only). On the left, the x-axis is hidden nodes, for 200 epochs. On the right, the x-axis is epochs, for the optimal hidden size from the left.",
        "name": "2",
        "region_bb": [
          55.440000000000005,
          70.56,
          527.04,
          194.4
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.72,
          203.76000000000002,
          540.72,
          234.0
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 2. Comparison (on 1M) of the prediction quality of various CF models and our RBM-based models (in bold). The values for (Truyen et al., 2009) are approximate and derived from the graphs in their work.",
        "name": "2",
        "region_bb": [
          307.44,
          134.64000000000001,
          556.5600000000001,
          278.64
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          307.44,
          77.04,
          541.44,
          118.08
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 1. Comparison (on 100k) of the prediction quality of various CF models and our RBM-based models (in bold).",
        "name": "1",
        "region_bb": [
          56.160000000000004,
          441.36,
          288.72,
          586.08
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          55.440000000000005,
          415.44,
          289.44,
          434.88
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Figure 4. MAE on 1M for two hybrid RBM models: with a real-valued visible layer, F U = 70, F I = 70 and with a multinomial visible layer, F U = 50, F I = 50.",
        "name": "4",
        "region_bb": [
          57.6,
          154.8,
          283.68,
          276.48
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          55.440000000000005,
          291.6,
          288.72,
          322.56
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1. The user-based RBM model (left) vs. our user- item-based RBM (right). Each visible unit represents a numerical user rating. In the left model it is connected to one hidden layer, while in the right it is linked to two inde- pendent hidden layers: one modeling correlations between items and another one modeling correlations between users.",
        "name": "1",
        "region_bb": [
          55.440000000000005,
          57.6,
          288.0,
          144.0
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          55.440000000000005,
          161.28,
          288.72,
          224.64000000000001
        ],
        "page": 3,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3. MAE on 1M: a user-based RBM with a real- valued visible layer (Real U-RBM) vs. a user-based RBM with multinomial visible layer (Multinomial U-RBM).",
        "name": "3",
        "region_bb": [
          308.88,
          447.84000000000003,
          538.5600000000001,
          570.24
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          308.16,
          584.64,
          540.72,
          614.88
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "icml10_9": {
    "figures": [
      {
        "caption": "Figure 2. Performance versus the number of selected features. The performance measures the average error on 50 splits of Madelon (left), Arcene (center) and Colon (right). D-FUSE and C-FUSE are only visible at the beginning of the curve as they select from 5 to 15 features.",
        "name": "2",
        "region_bb": [
          61.92,
          70.56,
          538.5600000000001,
          190.08
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.72,
          205.20000000000002,
          540.72,
          236.16
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1. One iteration of FUSE (UCT approach for FS).",
        "name": "1",
        "region_bb": [
          71.28,
          239.76000000000002,
          147.6,
          374.40000000000003
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.72,
          392.40000000000003,
          163.44,
          424.8
        ],
        "page": 3,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3. Average test error (a) and size of D-FUSE and C-FUSE feature subset (b) on Madelon vs. number N of iterations in logscale. D-FUSE R and C-FUSE R use the top 20 features after the RAVE score built from N iterations; RAND R use the top-20 features after the average score out of N uniformly selected 20-feature subsets.",
        "name": "3",
        "region_bb": [
          98.64,
          321.12,
          241.92000000000002,
          564.48
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.72,
          585.36,
          290.16,
          649.44
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 1. Datasets characteristics.",
        "name": "1",
        "region_bb": [
          60.480000000000004,
          94.32000000000001,
          280.8,
          136.8
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          105.12,
          76.32000000000001,
          238.32000000000002,
          82.8
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8
    ]
  },
  "aaai10_5": {
    "figures": [
      {
        "caption": "Figure 2: Classification results on a segment of the data from user 6 using a classifier trained on user 4. The top coloured bar above the data shows the labels assigned by a human labeller and the bot- tom coloured bar shows labels assigned by the classifier. The black line shows the magnitude of the accelerometer data and the red line is the gradient of the barometric pressure.",
        "name": "2",
        "region_bb": [
          66.96000000000001,
          54.0,
          277.92,
          192.24
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.0,
          194.4,
          292.32,
          252.72
        ],
        "page": 4,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: Example of time-delay embedding for accelerometer Time-Delay Embeddings data from biking activity. Left: raw accelerometer data. Right: time-delay embedding with m = 6, = 11, p = 3. The coloured points in the left diagram are mapped to the same coloured points in the embedding.",
        "name": "1",
        "region_bb": [
          320.40000000000003,
          53.28,
          550.8000000000001,
          148.32
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          318.96000000000004,
          152.64000000000001,
          557.28,
          201.6
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 4: Scores for the gait recognition task. Rows represent test sets and columns represent models. The shading represents the difference between the score and the maximum score for the row in terms of empirical standard deviations. White represents no difference (i.e., the maximum score for the row), and darker shading represents larger differences.",
        "name": "4",
        "region_bb": [
          360.0,
          207.36,
          516.24,
          329.76
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          318.96000000000004,
          333.36,
          557.28,
          391.68
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3: An example segment of the accelerometer data.",
        "name": "3",
        "region_bb": [
          329.76,
          54.72,
          545.04,
          122.4
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          334.08,
          123.84,
          542.16,
          132.48000000000002
        ],
        "page": 4,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 1: Results for activity recognition task. Each row corresponds to the results of training on the participant given in the first column, and the values indicate the classification accuracy on the data set for the user specified by the column header. The last column gives the results on the entire data set. The first row corresponds to using only the raw accelerometer and barometric pressure gradient data, without using time-delay embedding.",
        "name": "1",
        "region_bb": [
          109.44,
          54.0,
          500.40000000000003,
          157.68
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.0,
          160.56,
          557.28,
          198.72
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6
    ]
  },
  "icml13_2": {
    "figures": [
      {
        "caption": "Figure 2. Comparing Cycle Solvers. Note that time axis is in log scale. AOBB is AND/OR graph branch-and- bound (Marinescu & Dechter, 2006). Naive-MinSum is the naive way of passing clique tree messages. Fast-MinSum is algorithm 2 in (Felzenszwalb & McAuley, 2011). Loopy-BP is loopy min-sum message passing. Sometimes it takes extremely long to converge, especially for short cycles. We always cut it off at 500 seconds. Branch-and-Bound is the method of (Sun et al., 2012).",
        "name": "2",
        "region_bb": [
          109.44,
          67.68,
          470.16,
          202.32
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          55.440000000000005,
          215.28,
          541.44,
          267.84000000000003
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 5. In the Synthetic Grid experiment, applying \u2126+ has a great impact on the cycle solver running time when the cycle potentials are iteratively updated in dual decomposition. See text for more explanation.",
        "name": "5",
        "region_bb": [
          104.4,
          248.4,
          230.4,
          331.92
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          55.440000000000005,
          343.44,
          288.72,
          383.76
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 3. Average time (seconds) for solving one cycle subproblem in dual decomposition. Loopy-BP is cut off at 200 seconds if not converging.",
        "name": "3",
        "region_bb": [
          81.36,
          213.84,
          511.92,
          276.48
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          55.440000000000005,
          195.12,
          540.72,
          215.28
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 2. Summary of notations in Section 3",
        "name": "2",
        "region_bb": [
          313.2,
          85.68,
          543.6,
          199.44
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          336.24,
          76.32000000000001,
          512.64,
          84.24000000000001
        ],
        "page": 4,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 1. Summary of notations in Section 2",
        "name": "1",
        "region_bb": [
          313.92,
          183.6,
          530.64,
          282.96000000000004
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          336.24,
          175.68,
          512.64,
          182.88
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Figure 4. Effect of reparametrization. See text for explanation",
        "name": "4",
        "region_bb": [
          85.68,
          72.72,
          243.36,
          202.32
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          55.440000000000005,
          213.12,
          288.72,
          231.84
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1. Messages passed in our fast cycle solver. Wide red arrows: messages in the clique tree (triangulated cycle). Narrow blue arrows: messages on cycle edges.",
        "name": "1",
        "region_bb": [
          56.88,
          72.0,
          284.40000000000003,
          137.52
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.72,
          161.28,
          288.72,
          191.52
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3. Primal-dual objectives in four MRF energy minimization tasks. See text for explanation.",
        "name": "3",
        "region_bb": [
          64.8,
          73.44,
          523.44,
          152.64000000000001
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          98.64,
          164.88,
          496.8,
          173.52
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "aaai14_2": {
    "figures": [
      {
        "caption": "Table 2: Results about transformations (legend in Table 1).",
        "name": "2",
        "region_bb": [
          328.32,
          79.92,
          549.36,
          248.4
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          321.12,
          64.08,
          554.4,
          72.72
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 1: p Results about basic queries, optimization, and - cutting; means \u201csatisfies\u201d, \u2022 means \u201cdoes not satisfy\u201d, and means \u201cdoes not satisfy unless P = NP\u201d. Results for additive valued constraint satisfaction problems (VCSP + ) are given here as a baseline.",
        "name": "1",
        "region_bb": [
          58.32,
          123.12,
          288.0,
          339.84000000000003
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.0,
          64.08,
          292.32,
          116.64
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7
    ]
  },
  "icml11_4": {
    "figures": [
      {
        "caption": "Figure 2. Example of alignment matrix for prefix order to postfix order transduction. The example is (\u00d71 + 3 \u00d7 22, 1322 \u00d7 +\u00d7). On the left an alignment matrix, and on the right an alignment diagram showing the crossing dependencies.",
        "name": "2",
        "region_bb": [
          71.28,
          275.04,
          272.16,
          381.6
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          55.440000000000005,
          400.32,
          288.72,
          452.16
        ],
        "page": 3,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1. Derivation (+12, 12+). P S S 2/2 1/1 trees for O S +/+ 2/2 input output pair",
        "name": "1",
        "region_bb": [
          80.64,
          66.96000000000001,
          264.24,
          208.8
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          56.160000000000004,
          231.84,
          289.44,
          251.28
        ],
        "page": 3,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8
    ]
  },
  "nips09_3": {
    "figures": [
      {
        "caption": "Figure 2: Left: Graphical model representing our latent topic model of patch appearance and quan- tization into a potentially overlapping set of visual words. See text for details. Right: Local factors learned by latent topic model for example training data.",
        "name": "2",
        "region_bb": [
          115.2,
          81.36,
          493.92,
          210.96
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          224.64000000000001,
          503.28000000000003,
          254.88
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 5: Performance evaluation of detector based on transparent visual words w.r.t. baseline. See",
        "name": "5",
        "region_bb": [
          210.24,
          82.8,
          404.64,
          250.56
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.72,
          266.40000000000003,
          503.28000000000003,
          283.68
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 4: Example images from our training set of transparent objects in front of varying back- ground.",
        "name": "4",
        "region_bb": [
          125.28,
          81.36,
          486.72,
          262.08
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          275.76,
          503.28000000000003,
          295.2
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: Left: Images of a transparent object in different environments. A point on the object is highlighted in each image, and the local orientation edge energy map is shown. While the back- ground dominates the local patch, there is a latent structure that is discriminative of the object. Right: Our model finds local transparent structure by applying a latent factor model (e.g., LDA) before a quantization step. In contrast to previous approaches that applied such models to a quantized vi- sual word model, we apply them directly to the SIFT representation, and then quantize the resulting model into descriptors according to the learned topic distribution.",
        "name": "1",
        "region_bb": [
          123.84,
          82.08,
          488.88,
          249.84
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          260.64,
          504.0,
          334.8
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 6: Example of transparent object detection with transparent local features.",
        "name": "6",
        "region_bb": [
          141.12,
          82.08,
          470.88,
          444.96000000000004
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          144.72,
          459.36,
          466.56,
          468.0
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3: Detected quantized transparent local features (transparent visual words) on an example image. Each image shows the detected locations for the transparent visual word corresponding to the latent topics depicted on the left.",
        "name": "3",
        "region_bb": [
          113.76,
          276.48,
          501.84000000000003,
          343.44
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          107.28,
          365.04,
          503.28000000000003,
          395.28000000000003
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "aaai13_10": {
    "figures": [
      {
        "caption": "Figure 2: System Architecture.",
        "name": "2",
        "region_bb": [
          327.6,
          59.04,
          553.6800000000001,
          179.28
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          377.28000000000003,
          195.84,
          499.68,
          205.20000000000002
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 4: Framing results. Implied LMs are facet/target pairs from a single CM while foils are taken from different CMs.",
        "name": "4",
        "region_bb": [
          328.32,
          197.28,
          549.36,
          252.72
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          318.96000000000004,
          169.20000000000002,
          557.28,
          190.08
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 2: Top conceptual metaphors generated by the system for governance-related targets.",
        "name": "2",
        "region_bb": [
          94.32000000000001,
          602.64,
          252.0,
          702.0
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.0,
          575.28,
          291.6,
          595.44
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 1: Linguistic metaphor detection results for RCV1",
        "name": "1",
        "region_bb": [
          62.64,
          79.92,
          281.52,
          136.08
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          60.480000000000004,
          63.36,
          283.68,
          72.72
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Figure 1: Three levels of metaphor processing.",
        "name": "1",
        "region_bb": [
          342.0,
          267.84000000000003,
          537.12,
          352.8
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          345.6,
          370.8,
          530.64,
          380.16
        ],
        "page": 1,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3: A schematic partial view of the nominal analogy \u201cgovernment \u223c door.\u201d Facets in the middle are associated with the target noun on the left metaphorically, and with the source noun on the right literally.",
        "name": "3",
        "region_bb": [
          328.32,
          57.6,
          550.08,
          207.36
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          318.96000000000004,
          223.20000000000002,
          558.0,
          264.96000000000004
        ],
        "page": 3,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 3: Conceptual metaphor validation results. See text for explanation.",
        "name": "3",
        "region_bb": [
          318.96000000000004,
          90.72,
          567.36,
          146.88
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          318.96000000000004,
          63.36,
          558.0,
          83.52
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7
    ]
  },
  "nips09_8": {
    "figures": [
      {
        "caption": "Figure 2: Accuracy (left) and running time (right) as a function of maximum order for the handwrit- ing recognition data set.",
        "name": "2",
        "region_bb": [
          115.92,
          231.12,
          491.76,
          364.32
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          374.40000000000003,
          503.28000000000003,
          393.84000000000003
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: Accuracy as a function of maximum order on the synthetic data set.",
        "name": "1",
        "region_bb": [
          115.92,
          87.12,
          491.76,
          214.56
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          151.92000000000002,
          223.92000000000002,
          458.64000000000004,
          232.56
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3: Named entity recognition results.",
        "name": "3",
        "region_bb": [
          314.64,
          112.32000000000001,
          493.92,
          240.48000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          321.12,
          251.28,
          491.76,
          259.92
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "aaai13_6": {
    "figures": [
      {
        "caption": "Figure 2: The emotional component",
        "name": "2",
        "region_bb": [
          324.72,
          53.28,
          552.24,
          111.60000000000001
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          367.2,
          131.04,
          510.48,
          139.68
        ],
        "page": 4,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: The normative component",
        "name": "1",
        "region_bb": [
          324.72,
          54.0,
          552.24,
          185.04000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          367.2,
          198.72,
          510.48,
          207.36
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 2: The four versions of the scenario generated by vary- ing the salience of the norm and the group of the smoker",
        "name": "2",
        "region_bb": [
          97.92,
          175.68,
          248.4,
          221.76000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.0,
          231.84,
          291.6,
          252.0
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Figure 3: Two of the four versions created. In the top is version B (friend smokes and the norm has low salience) and in the bottom is version C (stranger smokes and the norm has high salience)",
        "name": "3",
        "region_bb": [
          339.12,
          53.28,
          538.5600000000001,
          258.48
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          318.96000000000004,
          272.16,
          558.0,
          313.92
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 1: How the appraisal variables determine the type of the emotion. Praiseworthiness determines if the emotion is positive or negative, while the cognitive unit strength de- termines if the emotion is directed to self (if higher than a threshold \u03b1) or directed to others (if lower than \u03b1)",
        "name": "1",
        "region_bb": [
          79.92,
          54.0,
          267.12,
          100.08
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.0,
          110.88000000000001,
          292.32,
          162.72
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7
    ]
  },
  "nips09_10": {
    "figures": [
      {
        "caption": "Figure 2: Resulting motion segmentations produced by the various techniques on the Hopkins 155 dataset. 2(a)-2(d): two cranes, 2(e)-2(h): three cars, and 2(i)-2(l): cars2 07. In two cranes (first row), EM produces more segmentation errors at the left crane. In three cars (second row), the max-min method gives the least segmentation error (at the front side of the middle car) and EM produces more segmentation errors at the front side of the left car. The contrast of EM and convex methods is apparent for cars2 07 (third row): the convex methods segment correctly the static grass field object, while EM makes mistakes. Further, the min-min method can almost perfectly segment the car in the middle of the scene from the static tree background.",
        "name": "2",
        "region_bb": [
          117.36,
          376.56,
          497.52000000000004,
          622.08
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          107.28,
          638.64,
          504.0,
          723.6
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: Recovered membership on synthetic data with EM and convex relaxation. 30 data points are generated according to y i = (\u03c0 i \u2297 x i )w +   i , with x i \u2208 R,   i \u223c N (0, 1) and w \u2208 U (0, 1).",
        "name": "1",
        "region_bb": [
          156.24,
          241.92000000000002,
          457.92,
          322.56
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.72,
          339.12,
          503.28000000000003,
          358.56
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 1: Error rate on several datasets from the Hopkins 155",
        "name": "1",
        "region_bb": [
          151.20000000000002,
          147.6,
          460.08000000000004,
          214.56
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          185.04000000000002,
          134.64000000000001,
          425.52000000000004,
          143.28
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "aaai13_7": {
    "figures": [
      {
        "caption": "Figure 1: Comparison of the performance of four prediction models with the Nemenyi test, where the x-axis indicates the average rank of F value and G mean , respectively, the y-axis indicates the ranking order of the four prediction models, and the vertical bars indicate the \u201cCritical Difference\u201d.",
        "name": "1",
        "region_bb": [
          319.68,
          59.04,
          559.44,
          115.92
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          318.96000000000004,
          121.68,
          558.0,
          159.12
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 1: Comparison of the performance of four prediction models",
        "name": "1",
        "region_bb": [
          59.760000000000005,
          415.44,
          287.28000000000003,
          466.56
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.0,
          395.28000000000003,
          291.6,
          414.0
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2
    ]
  },
  "nips12_10": {
    "figures": [
      {
        "caption": "Figure 2: Palimpsest learning. a. The cascade model. Colored circles are latent states (V ) that belong to two different synaptic weights (W ), arrows are state transitions (blue: depression, red: potentiation) b. Different variants of mapping pre- and post-synaptic activations to depression (D) and potentiation (P): R1\u2013postsynaptically gated, R2\u2013presynaptically gated, R3\u2013XOR rule. c. Cor- relation structure induced by these learning rules. c. Retrieval performance for each rule.",
        "name": "2",
        "region_bb": [
          110.16000000000001,
          82.08,
          462.24,
          190.8
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          204.48000000000002,
          504.0,
          257.04
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: Memory recall as inference and additive learning rules. a. Top: Synaptic weights, W, arise by storing the target pattern x together with T \u22121 other patterns, {x (t) } t=1...T\u22121 . During \u02dc , is a noisy version of the target pattern. The task of recall is to infer x given W recall, the cue, x and x \u02dc (by marginalising out {x (t) }). Bottom: The activity of neuron i across the stored patterns is a source of shared variability between synapses connecting it to neurons j and k. b-c. Covariance rule: patterns of synaptic correlations and recall performance for retrieval dynamics ignoring or considering synaptic correlations; T = 5. d-e. Same for the simple Hebbian learning rule. The control is an optimal decoder that ignores W.",
        "name": "1",
        "region_bb": [
          110.16000000000001,
          83.52,
          489.6,
          217.44
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          232.56,
          504.0,
          320.40000000000003
        ],
        "page": 3,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3: Implications for neural dynamics. a. R1: parameters for I i rec ; linear modulation by network activity, n b . b. R2: nonlinear modulation of pairwise term by network activity (cf. middle panel in a); other parameters have P linear dependences on n b . c. R1: Total current as P function of number of coactivated inputs, j W ij x j ; lines: different levels of neural excitability j W ij , line widths scale with frequency of occurrence in a sample run. d. Same for R2. e. Nonlinear integration in dendrites, reproduced from [11], cf. curves in c.",
        "name": "3",
        "region_bb": [
          110.88000000000001,
          82.8,
          478.08000000000004,
          253.44
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          107.28,
          268.56,
          504.0,
          334.08
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 1: Results summary: circuit adaptations against correlations for different learning rules.",
        "name": "1",
        "region_bb": [
          108.0,
          82.8,
          512.64,
          151.92000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          118.8,
          169.20000000000002,
          491.76,
          177.84
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "aaai11_4": {
    "figures": [
      {
        "caption": "Figure 2: Comparing the Amount of Acquired Feedback",
        "name": "2",
        "region_bb": [
          344.88,
          248.4,
          529.9200000000001,
          383.76
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          318.96000000000004,
          396.72,
          558.0,
          414.72
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: Comparing Different Fusion Schemes of Item and Attribute Queries",
        "name": "1",
        "region_bb": [
          340.56,
          57.6,
          529.9200000000001,
          198.72
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          318.96000000000004,
          211.68,
          558.0,
          231.12
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3: Comparing ADCF using Different Query Strate- gies with PAL",
        "name": "3",
        "region_bb": [
          78.48,
          168.48000000000002,
          267.84000000000003,
          309.6
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.0,
          323.28000000000003,
          291.6,
          343.44
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6
    ]
  },
  "nips10_4": {
    "figures": [
      {
        "caption": "Figure 2: This is a plot of the final test-errors of standard implementations of M1, MH and pared to C4.5. connect4 forest letter pendigits poker satimage New method after 500 rounds of boosting. We next investigated how each algorithm performs with less powerful weak-classifiers, namely, decision trees whose size has been sharply limited to various",
        "name": "2",
        "region_bb": [
          346.32,
          483.12,
          498.96000000000004,
          584.64
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          345.6,
          594.0,
          503.28000000000003,
          622.08
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: Figure 1(a) plots the final test-errors of M1(black, dashed), MH(blue, dotted) and New method(red, solid) against the maximum tree-sizes allowed as weak classifiers. Figure 1(b) plots how fast the test-errors of these algorithms drop with rounds, when the maximum tree-size allowed is 5.",
        "name": "1",
        "region_bb": [
          110.88000000000001,
          102.96000000000001,
          496.08000000000004,
          270.72
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          282.96000000000004,
          504.0,
          311.04
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "aaai14_9": {
    "figures": [
      {
        "caption": "Table 5: Binary classification results on human annotated datasets for models trained on heuristically labeled data. In brackets we show accuracy on the agreed and adjudicated subsets of the test set respectively.",
        "name": "5",
        "region_bb": [
          56.160000000000004,
          72.0,
          289.44,
          261.36
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.0,
          271.44,
          292.32,
          313.2
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 2: Inter-annotator agreement on manual annotations. Percent agreement is computed on the binary annotation, correlation is computed on the real-value degree of informa- tiveness of the article. All correlations are highly significant, with p < 0.001.",
        "name": "2",
        "region_bb": [
          73.44,
          54.0,
          272.88,
          110.16000000000001
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.0,
          120.96000000000001,
          291.6,
          173.52
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 1: Binary classification accuracy of 10-fold cross val- idation on the automatically labeled set for different classes of features: MRC Dataset (s 1 ), MRC Concreteness (s 2 ), LIWC (s 3 ), General Inquirer (s 4 ), Mutual Information (s 5 ), d-sequence (s 6 ), Productions (s 7 ), PDTB Discourse (s 8 ), Entity Grid (s 9 ), and all features combined (f). Domain- specific models are trained and tested only on data from the same domain, the general model uses all domains combined.",
        "name": "1",
        "region_bb": [
          325.44,
          54.0,
          552.24,
          167.76000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          318.96000000000004,
          177.84,
          557.28,
          263.52
        ],
        "page": 4,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Figure 1: Predication precision based on probability ranking",
        "name": "1",
        "region_bb": [
          75.60000000000001,
          464.40000000000003,
          267.84000000000003,
          599.0400000000001
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.72,
          446.40000000000003,
          292.32,
          455.04
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 6: Correlation between predicted probabilities and hu- man annotated scores. All correlations are highly significant with p < 0.001.",
        "name": "6",
        "region_bb": [
          106.56,
          326.16,
          240.48000000000002,
          383.04
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.0,
          393.12,
          292.32,
          424.08000000000004
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 3: Percentage/number of information-dense articles in",
        "name": "3",
        "region_bb": [
          79.92,
          491.04,
          267.12,
          547.2
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.0,
          558.0,
          291.6,
          575.28
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7
    ]
  },
  "nips13_4": {
    "figures": [
      {
        "caption": "Figure 2: Root Mean Squared Error (RMSE) of tensor trace norm and the proposed regularizer for ILEA dataset (Left) and Ocean video (Right).",
        "name": "2",
        "region_bb": [
          129.6,
          88.56,
          469.44,
          203.76000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          107.28,
          217.44,
          504.0,
          238.32000000000002
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: Synthetic dataset: (Left) Root Mean Squared Error (RMSE) of tensor trace norm and the proposed regularizer. (Right) Running time execution for different sizes of the tensor.",
        "name": "1",
        "region_bb": [
          127.44,
          88.56,
          472.32,
          209.52
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          107.28,
          223.20000000000002,
          503.28000000000003,
          244.08
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "icml10_4": {
    "figures": [
      {
        "caption": "Figure 2. An example of VaR at level \u03bb=5%, the return follows a",
        "name": "2",
        "region_bb": [
          60.480000000000004,
          68.4,
          288.0,
          190.8
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          55.440000000000005,
          205.92000000000002,
          288.72,
          223.20000000000002
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 5. MAE of VaR deviation on the S&P 500 index from 1960\u223c2005 for Garch(N) (circle), Garch(T) (diamond), SV(N) (x), SV(T) (plus), DPoT(N) (square) and DPoT(T) (triangle) model. The x axis is the VaR level in percentage.",
        "name": "5",
        "region_bb": [
          56.160000000000004,
          503.28000000000003,
          289.44,
          643.6800000000001
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          55.440000000000005,
          658.8000000000001,
          288.72,
          699.84
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 8. Deviation plots on the entire dataset of 10 stocks for ICA-Garch(N) (triangle), PCA-Garch(N) (pentagram), ICA- Garch(T) (diamond) and PCA-Garch(T) (hexagram), MSV(N) (x), MSV(T) (plus), DPoT(N) (square), DPoT(T) (circle) model. F (\u03bb) is the proportion of cdf s below \u03bb in all the 10 stocks. The closer to the horizontal line, the better prediction a model makes.",
        "name": "8",
        "region_bb": [
          308.88,
          275.04,
          541.44,
          447.12
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          306.72,
          462.96000000000004,
          540.72,
          526.32
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 1. Mean ranks of models over 10 stocks and their final ranking (in parentheses) across models at each VaR level according to MAE",
        "name": "1",
        "region_bb": [
          66.96000000000001,
          100.08,
          529.9200000000001,
          249.12
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          55.440000000000005,
          76.32000000000001,
          540.72,
          93.60000000000001
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Figure 4. Estimated \u03b1 for S&P 500 index from 1960\u223c2005. The time axis corresponds to the end of each sliding window.",
        "name": "4",
        "region_bb": [
          64.08,
          290.16,
          290.16,
          465.84000000000003
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          55.440000000000005,
          482.40000000000003,
          288.72,
          501.84000000000003
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1. Causal Bayes net structure for the dynamical PoT model",
        "name": "1",
        "region_bb": [
          309.6,
          254.88,
          539.28,
          416.16
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          308.16,
          430.56,
          540.0,
          439.2
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 7. Deviation plots on the S&P 500 index from 1960\u223c2005 for Garch(N) (circle), Garch(T) (diamond), SV(N) (x), SV(T) (plus), DPoT(N) (square) and DPoT(T) (triangle) model. The x axis is the VaR level in percentage and the y axis is the deviation of the failure rate from the given level in percentage. The closer to the horizontal line, the better prediction a model makes.",
        "name": "7",
        "region_bb": [
          56.88,
          275.04,
          289.44,
          447.12
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.72,
          463.68,
          288.72,
          527.04
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 6. RMSE of VaR deviation on the S&P 500 index from 1960\u223c2005 for Garch(N) (circle), Garch(T) (diamond), SV(N) (x), SV(T) (plus), DPoT(N) (square) and DPoT(T) (triangle) model. The x axis is the VaR level in percentage.",
        "name": "6",
        "region_bb": [
          308.16,
          67.68,
          541.44,
          207.36
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          307.44,
          222.48000000000002,
          540.72,
          263.52
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3. Prices and Percentage log-returns of S&P 500 index",
        "name": "3",
        "region_bb": [
          63.36,
          85.68,
          280.8,
          252.72
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.72,
          272.16,
          288.0,
          290.88
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8
    ]
  },
  "icml11_3": {
    "figures": [
      {
        "caption": "Figure 2. Results for a 20 \u00d7 20 Potts model 2 ) parsing al- 8-valued nodes and coupling gorithm, factor \u03c1 yielding = 10. an We O(n plot the gorithm. We evaluate these methods dual objectives, and the value of the true dual optimum. on the Prague Dependency Treebank us- For DD-Acc,   = 1.0; for DD-ADMM, \u03b7 = 0.5, \u03c4 = 1.0. ing online large-margin learning",
        "name": "2",
        "region_bb": [
          62.64,
          75.60000000000001,
          261.36,
          182.16
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          55.440000000000005,
          198.0,
          288.72,
          239.76000000000002
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1. Results for 30 \u00d7 30 random Ising models with several edge couplings. We plot the dual objectives and the best primal feasible solution at each iteration. For DD-subgradient, we set \u03b7 t = \u03b7 0 /t, with \u03b7 0 yielding the maximum dual improvement in 10 iterations, with halving steps (those iterations are not plotted). For DD-Acc we plot the most favorable   \u2208 {0.1, 1, 10, 100}. For DD-ADMM, we set \u03b7 = 5.0 and \u03c4 = 1.0. All decompositions are edge-based.",
        "name": "1",
        "region_bb": [
          97.92,
          66.96000000000001,
          498.96000000000004,
          201.6
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          55.440000000000005,
          216.0,
          540.72,
          257.04
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 4. Dependency parsing with 2nd-order models. For DD-subgradient, we consider both tree-based and flow- based factor graphs, and \u03b7 0 as in Fig. 1. DD-ADMM (\u03b7 = 0.05, \u03c4 = 1.5), MPLP, and Star-MSD ran only on the flow-based factor graph (see footnote 2). DD-Acc is not shown due to numerical problems when computing some low-temperature marginals. Top: synthetic 10-word sen- tences; we randomly generated (unary) arc log-potentials from N(0, 1) and (pairwise) grandparent and sibling log- potentials from N(0, 0.1). Bottom: \u00a723 of the Penn Tree- bank; the plot shows relative errors per iteration w.r.t. the dual optimum, averaged over the 2,399 test sentences.",
        "name": "4",
        "region_bb": [
          318.24,
          67.68,
          523.44,
          262.08
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          306.72,
          278.64,
          540.72,
          407.52000000000004
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "instead of no crossing edges, as illustrated in Figure 2. In lan- before, since they can reasons handle for multi-valued variables; Figure Top: with a more dependency parse tree, more informative phrase in structures is 3. guages flexible word order than where English, each arc for DD-ADMM we binarized the graph lexicalized as described that they are more efficient to learn and parse (h, while such as German, Dutch and a Czech, non-projective m) links a head word h to modifier word m. Middle: Sect. 4.3. Fig. 2 shows the best dual solutions obtained still encoding much of the predicate-argument tree-based infor- dependencies are more frequent. Rich factor graph corresponding to inflection a second-order at each iteration for the mation three needed algorithms. We observe in applications. systems reduce model reliance with on word order and to express dependency parsing sibling grandparent that MPLP decreases the objective very rapidly in the beginning and then slows down. DD-Acc manages to converge faster, but it is relatively slow to take off. DD-ADMM has the best features of both methods. features, including a tree hard constraint factor. Bottom: the flow-based factor graph is an alternative representation for the same model, in which extra flow and path variables are added, and the tree factor is replaced by smaller xor, or and or-out. See Martins et al. (2010) for details.",
        "name": "3",
        "region_bb": [
          335.52000000000004,
          83.52,
          513.36,
          380.16
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          306.72,
          390.96000000000004,
          540.72,
          487.44
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8
    ]
  },
  "icml13_5": {
    "figures": [
      {
        "caption": "Figure 2. Comparison of kernel learning methods in terms of test error (left) and training time (right).",
        "name": "2",
        "region_bb": [
          143.28,
          230.4,
          460.08000000000004,
          379.44
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          95.76,
          389.52000000000004,
          499.68,
          398.16
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1. Convergence comparison of our method and other algorithms.",
        "name": "1",
        "region_bb": [
          67.68,
          69.84,
          529.2,
          198.0
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          156.24,
          206.64000000000001,
          439.2,
          215.28
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "aaai13_8": {
    "figures": [],
    "pages_annotated": [
      1,
      2
    ]
  },
  "icml12_5": {
    "figures": [
      {
        "caption": "Figure 2. Graphical models: (a) LDA without the plate notation for words (b) Variable-gram Topic model.",
        "name": "2",
        "region_bb": [
          55.440000000000005,
          66.96000000000001,
          289.44,
          167.76000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          55.440000000000005,
          180.0,
          288.72,
          199.44
        ],
        "page": 3,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 5. (4, 1) mismatch kernels computed between 10 test sequences (rows 1-10) and 10 model samples (rows 11-20). (a) Uniform(26) (b) Dirichlet-VMM.De (c) Topic Bigram.50 (d) Topic Var-gram.De.50 (e) Topic Var-gram.De.50-Given",
        "name": "5",
        "region_bb": [
          56.88,
          68.4,
          288.0,
          260.64
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.72,
          276.48,
          289.44,
          329.04
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 1. Average next-step prediction log-likelihood of the",
        "name": "1",
        "region_bb": [
          307.44,
          105.12,
          542.16,
          292.32
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          307.44,
          76.32000000000001,
          540.72,
          93.60000000000001
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Figure 4. Estimated squared MMD between test sequences and model samples using the (4, 1) mismatch kernel. Colour describes the temporal structure in a model. Groups de- scribe models with different numbers of topics. EmpMarg: the empirical marginal distribution of the training data. Train: the train sequences. (*): The models are sampled conditional on the topic allocations (see text for details).",
        "name": "4",
        "region_bb": [
          309.6,
          70.56,
          537.84,
          269.28000000000003
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          306.72,
          279.36,
          542.16,
          354.24
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1. An example Dirichlet-VMM tree for a binary se- quence. Contexts 01 and 11 are only observed once and thus are not included in the tree. Note that for readability, contexts in this figure are denoted in chronological order.",
        "name": "1",
        "region_bb": [
          308.16,
          57.6,
          541.44,
          156.96
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          307.44,
          170.64000000000001,
          542.16,
          211.68
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 7. Conditional distributions over notes for 2 topics that are mostly assigned in pieces from the key of D. Each distribution is conditioned on a different context. Model:",
        "name": "7",
        "region_bb": [
          57.6,
          70.56,
          286.56,
          242.64000000000001
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.72,
          261.36,
          289.44,
          303.12
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 6. Scatter plot of the number of times each latent topic is allocated in pieces from the key of G (x-axis) and in pieces from the key of D (y-axis), after Gibbs sampling in the training data has converged.",
        "name": "6",
        "region_bb": [
          310.32,
          69.84,
          539.28,
          242.64000000000001
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          307.44,
          257.76,
          540.72,
          299.52000000000004
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3. Data Representation: Time is discretized in eighth note intervals. At each time-step, pitch is represented by a 1-of-26 dimensional vector. Red arrows: a G4 quarter note (lasts for two eighths) is represented by G4 followed by \u2018continuation\u2019. Blue Arrow: a G4 eighth note is represented by a single G4. Notes outside the C4\u2013B5 interval are",
        "name": "3",
        "region_bb": [
          64.8,
          66.96000000000001,
          281.52,
          221.76000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.72,
          236.16,
          289.44,
          308.16
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8
    ]
  },
  "aaai14_3": {
    "figures": [
      {
        "caption": "Figure 2: The employed four data sets.",
        "name": "2",
        "region_bb": [
          59.760000000000005,
          431.28000000000003,
          285.12,
          535.6800000000001
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          96.48,
          555.12,
          249.12,
          563.76
        ],
        "page": 4,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 5: Manifold clustering behaviour of various methods.",
        "name": "5",
        "region_bb": [
          56.160000000000004,
          297.36,
          290.16,
          393.84000000000003
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.72,
          414.0,
          290.88,
          422.64000000000004
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 8: The embeddings of first two clusters of Ex- tendedYaleB, which are given by using the proposed LNP scheme, and the corresponding dev at the right bottom.",
        "name": "8",
        "region_bb": [
          321.84000000000003,
          578.16,
          554.4,
          655.9200000000001
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          319.68,
          674.64,
          557.28,
          705.6
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 4: Intrinsic manifold structure preservation behaviour of various manifold learning methods.",
        "name": "4",
        "region_bb": [
          55.440000000000005,
          54.0,
          290.88,
          235.44
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.0,
          255.60000000000002,
          292.32,
          275.04
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: (a): Data manifold and S a {A 3 } + ; (b): The affine representation of a 3 learnt by LLE; (c): The corresponding 1-d embedding of A, denoted by Y = {y 1 , y 2 , y 3 , y 4 }.",
        "name": "1",
        "region_bb": [
          322.56,
          54.0,
          555.84,
          123.12
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          318.96000000000004,
          143.28,
          558.0,
          174.96
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 7: Embeddings of COIL-20. For each method, the left is the obtained embeddings while the right is the corre- sponding devs.",
        "name": "7",
        "region_bb": [
          331.2,
          295.2,
          546.48,
          487.44
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          319.68,
          507.6,
          557.28,
          537.84
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 6: The execution time of various methods.",
        "name": "6",
        "region_bb": [
          321.12,
          53.28,
          555.84,
          236.88
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          340.56,
          256.32,
          535.6800000000001,
          264.96000000000004
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3: Embeddings of trefoil. For each method, the left is the obtained embeddings while the right is the correspond- ing devs.",
        "name": "3",
        "region_bb": [
          323.28000000000003,
          54.0,
          554.4,
          257.76
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          318.96000000000004,
          278.64,
          557.28,
          309.6
        ],
        "page": 4,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7
    ]
  },
  "icml14_5": {
    "figures": [
      {
        "caption": "Figure 11. RMSE for transfer learning on the real grape data",
        "name": "11",
        "region_bb": [
          63.36,
          220.32000000000002,
          265.68,
          370.08000000000004
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          56.160000000000004,
          389.52000000000004,
          272.16,
          398.16
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 7. Absolute Error for Active Surveying on Synthetic",
        "name": "7",
        "region_bb": [
          73.44,
          270.72,
          266.40000000000003,
          414.72
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          55.440000000000005,
          434.16,
          288.72,
          451.44
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 4. MSE for transfer learning on synthetic dataset 2",
        "name": "4",
        "region_bb": [
          61.92,
          388.08000000000004,
          269.28000000000003,
          541.44
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          69.12,
          561.6,
          275.04,
          570.24
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 9. Absolute Error for Active Surveying on Synthetic",
        "name": "9",
        "region_bb": [
          325.44,
          76.32000000000001,
          518.4,
          220.32000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          307.44,
          239.76000000000002,
          540.72,
          257.04
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1. Toy example showing the transfer/active learning prob-",
        "name": "1",
        "region_bb": [
          303.84000000000003,
          227.52,
          539.28,
          292.32
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          308.16,
          308.16,
          540.72,
          325.44
        ],
        "page": 3,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 13. Absolute error for Active Surveying on the real data",
        "name": "13",
        "region_bb": [
          310.32,
          274.32,
          522.0,
          429.84000000000003
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          308.16,
          444.96000000000004,
          531.36,
          453.6
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 6. MSE for Active Learning on Synthetic Dataset 1",
        "name": "6",
        "region_bb": [
          68.4,
          76.32000000000001,
          266.40000000000003,
          220.32000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          67.68,
          239.76000000000002,
          275.04,
          248.4
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3. MSE for transfer learning on synthetic dataset 1",
        "name": "3",
        "region_bb": [
          65.52,
          171.36,
          268.56,
          327.6
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          69.12,
          347.76,
          274.32,
          356.40000000000003
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 10. A part of one image from each grape dataset",
        "name": "10",
        "region_bb": [
          92.88000000000001,
          66.96000000000001,
          252.0,
          185.04000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          73.44,
          198.72,
          271.44,
          207.36
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 2. Illustration of two synthetic datasets",
        "name": "2",
        "region_bb": [
          324.72,
          268.56,
          538.5600000000001,
          351.36
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          342.0,
          367.92,
          506.16,
          376.56
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 5. The comparison of different covariance matrices. Red stars show the data from the source domain, and blue circles show the data from the target domain. The black bars show the error bar/uncertainty (diagonal elements of the covariance matrix) on the prediction of unlabeled test points. The two labeled test points are shown in filled blue circles (x 1 = \u22124.3, x 2 = 3.05).",
        "name": "5",
        "region_bb": [
          325.44,
          204.48000000000002,
          531.36,
          358.56
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          306.72,
          389.52000000000004,
          541.44,
          452.16
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 12. RMSE for Active Learning on the real data",
        "name": "12",
        "region_bb": [
          310.32,
          76.32000000000001,
          522.0,
          231.12
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          327.6,
          246.96,
          521.28,
          255.60000000000002
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 8. MSE for Active Learning on Synthetic Dataset 2",
        "name": "8",
        "region_bb": [
          68.4,
          475.92,
          266.40000000000003,
          619.9200000000001
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          67.68,
          639.36,
          275.76,
          648.0
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "aaai11_8": {
    "figures": [
      {
        "caption": "Figure 2: The uBot-5.",
        "name": "2",
        "region_bb": [
          113.04,
          54.0,
          233.28,
          194.4
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          129.6,
          207.36,
          216.0,
          216.72
        ],
        "page": 3,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 5: The uBot\u2019s learning curve in the \ufb01rst task.",
        "name": "5",
        "region_bb": [
          352.8,
          153.36,
          516.96,
          282.24
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          335.52000000000004,
          301.68,
          541.44,
          311.04
        ],
        "page": 4,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 8: The time required for the uBot-5 to \ufb01rst complete the second task, given innate controllers or acquired skills.",
        "name": "8",
        "region_bb": [
          77.04,
          380.88,
          260.64,
          518.4
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          53.28,
          543.6,
          292.32,
          563.76
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 4: The second task in the Red Room Domain.",
        "name": "4",
        "region_bb": [
          55.440000000000005,
          315.36,
          285.84000000000003,
          480.24
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          67.68,
          491.04,
          277.92,
          500.40000000000003
        ],
        "page": 4,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: An illustration of a trajectory segmented into skills by CST. A robot executes a trajectory where it goes through a door, approaches and picks up a key, and then takes it to a lock (bottom). The robot is equipped with three possible abstractions: features describing its distance to the doorway, the key, and the lock. The value of these features change during trajectory execution (middle) as the distance to each object changes while it is in the robot\u2019s \ufb01eld of view. The robot also obtains an estimate of return for each point along the trajectory by summing the (discounted) rewards obtained from that point on (top). CST splits the trajectory into seg- ments by \ufb01nding an MAP segmentation such that the return estimate is best represented by a piecewise linear value func- tion where each segment uses a single abstraction. Segment",
        "name": "1",
        "region_bb": [
          81.36,
          207.36,
          263.52,
          437.76
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          53.28,
          452.88,
          292.32,
          613.44
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 7: A brief description of each of the skills extracted from the trajectory in Figure 6, with selected abstractions.",
        "name": "7",
        "region_bb": [
          54.0,
          54.0,
          299.52000000000004,
          185.76000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.0,
          206.64000000000001,
          292.32,
          226.8
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 6: A trajectory from the learned solution to the \ufb01rst task, segmented into skills.",
        "name": "6",
        "region_bb": [
          331.92,
          457.2,
          546.48,
          591.12
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          318.96000000000004,
          604.8000000000001,
          558.0,
          624.96
        ],
        "page": 4,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3: The \ufb01rst task in the Red Room Domain.",
        "name": "3",
        "region_bb": [
          321.12,
          334.08,
          558.0,
          492.48
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          339.12,
          503.28000000000003,
          537.12,
          512.64
        ],
        "page": 3,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6
    ]
  },
  "aaai11_2": {
    "figures": [
      {
        "caption": "Figure 2: Average runtime",
        "name": "2",
        "region_bb": [
          55.440000000000005,
          53.28,
          285.84000000000003,
          344.16
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          120.24000000000001,
          355.68,
          225.36,
          365.04
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: Simpli\ufb01ed examples of belief-states from DT ses-",
        "name": "1",
        "region_bb": [
          54.0,
          54.0,
          292.32,
          174.24
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.0,
          183.6,
          291.6,
          201.6
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3: Average plan costs and number of successful runs.",
        "name": "3",
        "region_bb": [
          319.68,
          53.28,
          547.2,
          417.6
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          318.96000000000004,
          428.40000000000003,
          557.28,
          437.76
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7
    ]
  },
  "icml13_8": {
    "figures": [
      {
        "caption": "Figure 2. A probabilistic latent component analysis factor- ization of an audio spectrogram. Solid yellow elements of the distributions explain source A (e.g. siren), while blue striped elements explain source B (e.g. speech).",
        "name": "2",
        "region_bb": [
          56.160000000000004,
          83.52,
          287.28000000000003,
          175.68
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          55.440000000000005,
          189.36,
          289.44,
          231.12
        ],
        "page": 3,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 5. Comparison of SDR (in dB) to the number of basis vectors per source. Examples include Phone (blue, circle), Drum (red, x-mark), Orchestra (black, plus), Piano (green, star), and Siren (magenta, square).",
        "name": "5",
        "region_bb": [
          316.08,
          298.8,
          539.28,
          409.68
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          307.44,
          424.8,
          541.44,
          466.56
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 2. SDR, SIR, and SAR (in dB) results for the four SiSEC rock/pop songs.",
        "name": "2",
        "region_bb": [
          317.52000000000004,
          102.96000000000001,
          527.76,
          269.28000000000003
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          308.16,
          77.04,
          540.72,
          96.48
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 1. SDR, SIR, and SAR (in dB) for the first five ex- ample recordings using 100 dictionary elements/source.",
        "name": "1",
        "region_bb": [
          55.440000000000005,
          112.32000000000001,
          295.92,
          216.72
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          56.160000000000004,
          77.04,
          288.72,
          96.48
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Figure 4. Two mixture spectrograms and the resulting sep- arated sources using the proposed method for five minutes.",
        "name": "4",
        "region_bb": [
          313.92,
          72.0,
          534.96,
          285.12
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          307.44,
          300.96000000000004,
          540.72,
          320.40000000000003
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1. (a) Given a mixture recording, a user separates distinct sounds by roughly painting on a time-frequency display. (b) Once initially separated, fine-tuning is per- formed by painting on the output results. Painting on one output track at a particular time-frequency point pushes the sound into the other track(s).",
        "name": "1",
        "region_bb": [
          307.44,
          242.64000000000001,
          547.2,
          439.92
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          307.44,
          456.48,
          540.72,
          519.84
        ],
        "page": 1,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3. Probabilistic latent component analysis with",
        "name": "3",
        "region_bb": [
          56.160000000000004,
          82.8,
          288.72,
          185.76000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          56.160000000000004,
          200.88,
          288.0,
          221.04000000000002
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "nips11_2": {
    "figures": [
      {
        "caption": "Figure 2: Histograms showing the mean percentage of edges that were correctly recovered by the algorithm for the free parameter case together with error bars showing one standard deviation.",
        "name": "2",
        "region_bb": [
          126.0,
          92.16,
          464.40000000000003,
          348.48
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.72,
          370.08000000000004,
          503.28000000000003,
          389.52000000000004
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: (a) A rooted directed tree with the root at the top. All arcs are directed downwards, i.e., away from the root. (b) An OT with probabilities associated with arcs and CNAs associated with vertices. (c) A HOT with probabilities associated with arcs (indicating the probability that the hidden variable associated with the head of the arc receives the value 1 conditioned that the hidden variable associated with the tail has this value), and CNAs as well as probabilities associated with vertices (indicating the probability that the observable variable associated with the vertex receives the value 1 conditioned that the hidden variable associated with the vertex has received this value). (d) A HOT-mixture consisting of two HOTs. The mixing probability for T 1 is 0.7 and that for T 2 is 0.3. So with probability 0.7 a synthetic tumor is generated from T 1 and otherwise one is generated from T 2 .",
        "name": "1",
        "region_bb": [
          122.4,
          376.56,
          489.6,
          474.48
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          106.56,
          495.36,
          504.0,
          583.9200000000001
        ],
        "page": 3,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 4: HOTs obtained from RCC data. (a) shows an adapted version of the pathways for CC data published in [21]. (b) is a figure adapted from [22] showing the pathways obtained from statistical analysis of RCC data. (c) and (d) are the HOTs we obtained from the RCC data using only aberrations on the left and right pathways in (b), respectively. Notice the high level of agreement between the root-to-leaf paths in the recovered HOTs with those in (b).",
        "name": "4",
        "region_bb": [
          121.68,
          84.24000000000001,
          497.52000000000004,
          274.32
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          107.28,
          295.92,
          504.72,
          343.44
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3: Histograms showing proportion of edges correctly recovered by the EM algorithm for HOT-mixtures with global parameters on two HOTs with 25 vertices each. Each bar represents 100",
        "name": "3",
        "region_bb": [
          109.44,
          419.76,
          491.04,
          573.84
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.72,
          596.88,
          504.0,
          624.96
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "icml14_8": {
    "figures": [
      {
        "caption": "Figure 1. Test problems 1, 2, 3, and 4: Speedup of multicore implementations of DW on up to 40 cores of an Intel Xeon architecture.",
        "name": "1",
        "region_bb": [
          61.92,
          68.4,
          531.36,
          162.72
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          56.160000000000004,
          173.52,
          540.0,
          192.96
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 3. Efficiency comparison between LIBSVM and A SY SCD for kernel SVM using 40 cores using homogeneous kernels (K(x i , x j ) = (x Ti x j ) 2 ). The running time and speedup are cal- culated based on the \u201cresidual\u201d 10 \u22123 . Here, to make both algo- rithms comparable, the \u201cresidual\u201d is defined by kx \u2212 P \u2126 (x \u2212 \u2207f (x))k \u221e .",
        "name": "3",
        "region_bb": [
          55.440000000000005,
          432.0,
          293.04,
          516.96
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          55.440000000000005,
          527.04,
          288.72,
          590.4
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 2. Efficiency comparison between SynGD and A SY SCD for the QP problem. The running time and speedup are based on the residual achieving a tolerance of 10 \u22125 .",
        "name": "2",
        "region_bb": [
          64.08,
          295.2,
          281.52,
          380.16
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          55.440000000000005,
          390.24,
          288.72,
          420.48
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 1. Runtimes (s) for the four test problems on 1 and 40 cores.",
        "name": "1",
        "region_bb": [
          107.28,
          204.48000000000002,
          237.6,
          265.68
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          56.160000000000004,
          275.76,
          288.0,
          284.40000000000003
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "icml11_7": {
    "figures": [
      {
        "caption": "Figure 1. Evaluation of AU C performance with respect to varied bu\ufb00er sizes (i.e., varied N +1 = N \u22121 values).",
        "name": "1",
        "region_bb": [
          59.04,
          57.6,
          541.44,
          330.48
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          77.04,
          341.28000000000003,
          518.4,
          349.92
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 2. Evaluation of average AUC performance. Algorithm Perceptron PA-I CW-full CPA PB CPA ML OAM seq OAM gra OAM inf Algorithm Perceptron PA-I CW-full CPA PB CPA ML OAM seq OAM gra OAM inf Algorithm Perceptron PA-I CW-full CPA PB CPA ML OAM seq OAM gra OAM inf Algorithm Perceptron PA-I CW-full CPA PB CPA ML OAM seq OAM gra OAM inf sonar 0.780 \u00b1 0.060 0.790 \u00b1 0.057 0.793 \u00b1 0.059 0.798 \u00b1 0.059 0.827 \u00b1 0.052 0.850 \u00b1 0.042 0.849 \u00b1 0.043 0.849 \u00b1 0.043 magic04 0.723 \u00b1 0.069 0.564 \u00b1 0.111 0.746 \u00b1 0.027 0.730 \u00b1 0.030 0.734 \u00b1 0.026 0.778 \u00b1 0.029 0.765 \u00b1 0.032 0.784 \u00b1 0.026 segment 0.852 \u00b1 0.024 0.863 \u00b1 0.021 0.896 \u00b1 0.021 0.888 \u00b1 0.018 0.886 \u00b1 0.021 0.956 \u00b1 0.013 0.955 \u00b1 0.014 0.956 \u00b1 0.013 vowel 0.859 \u00b1 0.055 0.863 \u00b1 0.063 0.870 \u00b1 0.063 0.887 \u00b1 0.049 0.923 \u00b1 0.032 0.931 \u00b1 0.046 0.928 \u00b1 0.046 0.931 \u00b1 0.041 fourclass 0.690 \u00b1 0.165 0.668 \u00b1 0.168 0.758 \u00b1 0.032 0.812 \u00b1 0.020 0.812 \u00b1 0.020 0.831 \u00b1 0.020 0.826 \u00b1 0.020 0.831 \u00b1 0.020 german 0.701 \u00b1 0.039 0.701 \u00b1 0.033 0.757 \u00b1 0.025 0.698 \u00b1 0.034 0.701 \u00b1 0.033 0.775 \u00b1 0.037 0.773 \u00b1 0.033 0.776 \u00b1 0.034 ijcnn1 0.647 \u00b1 0.088 0.531 \u00b1 0.074 0.829 \u00b1 0.021 0.908 \u00b1 0.012 0.910 \u00b1 0.011 0.920 \u00b1 0.017 0.916 \u00b1 0.018 0.928 \u00b1 0.015 letter 0.551 \u00b1 0.092 0.533 \u00b1 0.104 0.804 \u00b1 0.025 0.784 \u00b1 0.056 0.802 \u00b1 0.035 0.820 \u00b1 0.016 0.817 \u00b1 0.023 0.828 \u00b1 0.010 svmguide1",
        "name": "2",
        "region_bb": [
          309.6,
          412.56,
          539.28,
          717.84
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          323.28000000000003,
          403.2,
          524.16,
          411.84000000000003
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 1. Details of the datasets in our experiments.",
        "name": "1",
        "region_bb": [
          313.2,
          85.68,
          536.4,
          218.88
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          320.40000000000003,
          76.32000000000001,
          527.04,
          82.8
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8
    ]
  },
  "aaai13_2": {
    "figures": [
      {
        "caption": "Figure 2: An aspect-sentiment hierarchy and a magni\ufb01ed aspect-sentiment node. An aspect-sentiment node is de\ufb01ned by two latent variables: An tree T with unbounded depth and unbounded width and a topic set \u03d5, which are indepen- dently drawn from Dirichlet(\u03b2) distribution. For each aspect- sentiment node \u03a6 k , we set a two-level topic tree, having",
        "name": "2",
        "region_bb": [
          58.32,
          54.72,
          290.88,
          141.12
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.0,
          151.20000000000002,
          293.76,
          226.8
        ],
        "page": 3,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 5: Hierarchical Af\ufb01nity. The lower distance scores for Children indicate that a parent is more similar to its direct children, compared to non-children nodes at the same level. HASM and rCRP show similar patterns in the semantic dis- tances for children and non-children, while nCRP does not.",
        "name": "5",
        "region_bb": [
          326.88,
          56.160000000000004,
          551.52,
          175.68
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          318.96000000000004,
          185.76000000000002,
          558.72,
          238.32000000000002
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 3: The node specialization scores. For all three mod- els, as the level increases, the specialization scores increase, which means the model assumptions are correctly re\ufb02ected",
        "name": "3",
        "region_bb": [
          54.0,
          141.84,
          293.76,
          221.04000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          53.28,
          226.8,
          293.04,
          266.40000000000003
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 2: Statistics for the datasets used in the experiments.",
        "name": "2",
        "region_bb": [
          60.480000000000004,
          54.0,
          286.56,
          110.16000000000001
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          56.88,
          115.92,
          288.72,
          125.28
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 1: Variables and notations.",
        "name": "1",
        "region_bb": [
          65.52,
          54.0,
          280.8,
          398.16
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          403.92,
          237.6,
          411.12
        ],
        "page": 4,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Figure 4: A part of the aspect-sentiment hierarchy for L APTOPS and D IGITAL SLR S . We de\ufb01ne the root as a \ufb01rst-level node, and show the second-level aspect-sentiment nodes for which at least 15,000 sentences are assigned to each node. We also show their third-level children. We do not show stopwords or words that occur in both aspect and sentiment-polar topics. Words fast, small, and high occur in both positive and negative polarities depending on the aspect at various levels of granularity.",
        "name": "4",
        "region_bb": [
          56.88,
          56.160000000000004,
          556.5600000000001,
          428.40000000000003
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          52.56,
          439.2,
          558.72,
          480.96000000000004
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: Part of the tree structure discovered by HASM, applied to the L APTOPS review corpus. Each node itself is a two-level tree, whose root represents the aspect and the children represent the associated sentiment polarities.",
        "name": "1",
        "region_bb": [
          321.12,
          218.16,
          557.28,
          300.96000000000004
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          318.96000000000004,
          313.2,
          558.72,
          354.96000000000004
        ],
        "page": 1,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 7: Sentence level sentiment classi\ufb01cation. The Small set has reviews with 1 (negative) and 5 (positive) stars, and the Large set has reviews with 1 and 2 stars (negative) as well as 4 and 5 stars (positive). The sentiment classi\ufb01cation accuracy of HASM is comparable to the other three models",
        "name": "7",
        "region_bb": [
          329.04,
          56.160000000000004,
          550.8000000000001,
          181.44
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          318.96000000000004,
          192.24,
          558.0,
          254.16
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 6: Aspect-Sentiment Consistency. The results show that HASM achieves lower intra-node average topic dis- tance than inter-node average topic distance. In contrast, rJST shows high average topic distances for both intra- and inter- nodes. The comparison demonstrates that HASM achieves",
        "name": "6",
        "region_bb": [
          61.92,
          55.440000000000005,
          286.56,
          157.68
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.0,
          168.48000000000002,
          293.04,
          233.28
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3: Graphical representation of HASM.",
        "name": "3",
        "region_bb": [
          347.76,
          55.440000000000005,
          530.64,
          174.96
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          347.76,
          187.20000000000002,
          528.48,
          196.56
        ],
        "page": 3,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8
    ]
  },
  "aaai10_6": {
    "figures": [
      {
        "caption": "Figure 2: Plot showing user posting behavior",
        "name": "2",
        "region_bb": [
          88.56,
          59.760000000000005,
          254.88,
          149.04
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          82.8,
          162.72,
          263.52,
          172.08
        ],
        "page": 4,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 4: Effect of incorporating various priors information",
        "name": "4",
        "region_bb": [
          54.72,
          53.28,
          291.6,
          187.92000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.0,
          198.0,
          292.32,
          216.0
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 2: Some example queries used in the experiments.",
        "name": "2",
        "region_bb": [
          350.64,
          53.28,
          524.88,
          105.84
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          326.16,
          115.92,
          550.8000000000001,
          125.28
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 1: Summary statistics of different datasets used. In- terlinks refer to the number of interthread links with in a",
        "name": "1",
        "region_bb": [
          58.32,
          122.4,
          288.0,
          202.32
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.0,
          212.4,
          292.32,
          241.20000000000002
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Figure 1: Inference Network Model for Forums",
        "name": "1",
        "region_bb": [
          354.96000000000004,
          54.0,
          522.0,
          230.4
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          343.44,
          244.08,
          533.52,
          253.44
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3: Presence of user provided evidence based links in",
        "name": "3",
        "region_bb": [
          354.96000000000004,
          368.64,
          522.0,
          465.12
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          318.96000000000004,
          478.8,
          558.0,
          496.8
        ],
        "page": 4,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 3: Impact of different structural components and their combinations on retrieval performance on the two datasets. For the T+I+R model, numbers in brackets denote the opti- mal \u03b1 values for the respective components.",
        "name": "3",
        "region_bb": [
          318.96000000000004,
          138.96,
          565.2,
          273.6
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          318.96000000000004,
          282.96000000000004,
          558.0,
          325.44
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6
    ]
  },
  "nips11_6": {
    "figures": [
      {
        "caption": "Figure 2: Performance of the proposed 2SW- presence of additional weighted source data has MDA method on 20 Newsgroups dataset and Sen-",
        "name": "2",
        "region_bb": [
          321.12,
          190.08,
          501.84000000000003,
          324.72
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          306.0,
          338.40000000000003,
          503.28000000000003,
          369.36
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: Two source domains D1 and D2 and target domain data with different marginal and conditional probability differences, along with conflicting conditional probabilities (the red squares and blue triangles refer to the positive and negative classes). Many existing methods re-weight source domain data in order to minimize the marginal probability",
        "name": "1",
        "region_bb": [
          136.8,
          87.84,
          469.44,
          180.72
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          197.28,
          504.0,
          227.52
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 1: Comparison of different methods on three real-world and one toy datasets in terms of classification accuracies (%).",
        "name": "1",
        "region_bb": [
          133.20000000000002,
          288.0,
          480.96000000000004,
          519.12
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          529.9200000000001,
          504.0,
          548.64
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "icml13_6": {
    "figures": [
      {
        "caption": "Figure 2. The hierarchy of subforums in the BlackHat- World Internet forum. The number of threads in each subforum is indicated in parentheses.",
        "name": "2",
        "region_bb": [
          307.44,
          247.68,
          529.2,
          414.72
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          307.44,
          428.40000000000003,
          540.72,
          458.64000000000004
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 5. Variational approximation to the posterior dis- tribution for the graphical model in Fig. 4.",
        "name": "5",
        "region_bb": [
          92.88000000000001,
          69.84,
          249.84,
          144.72
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          56.160000000000004,
          162.0,
          288.72,
          181.44
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 2. Four examples of the K = 9 topics discovered by tiLDA on the BlackHatWorld corpus; training time was 30 minutes. Shown are the six most probable words for each topic. We replaced dollar amounts by the token DOLLAR.",
        "name": "2",
        "region_bb": [
          307.44,
          295.92,
          550.08,
          383.76
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          308.16,
          239.76000000000002,
          540.72,
          281.52
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 1. Four examples of the K = 60 topics discovered by tiLDA on the Freelancer corpus; training time was 60 hours. Shown are the six most probable words for each topic. Capitalized terms indicate project keywords.",
        "name": "1",
        "region_bb": [
          55.440000000000005,
          295.92,
          301.68,
          382.32
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          55.440000000000005,
          239.76000000000002,
          288.72,
          281.52
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Figure 4. Graphical model for tiLDA in which all docu- ments of a hierarchical corpus are attached to third-level nodes. Here C t denotes the set of indexes for the subcat- egories and documents of category t, and N d denotes the length of the document d.",
        "name": "4",
        "region_bb": [
          316.8,
          66.96000000000001,
          532.8000000000001,
          183.6
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          307.44,
          197.28,
          540.72,
          249.84
        ],
        "page": 4,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1. The hierarchy of buyers and job advertisements on Freelancer.com. The number of ads per buyer is indi- cated in parentheses. For brevity, only titles and dates of",
        "name": "1",
        "region_bb": [
          307.44,
          66.96000000000001,
          529.2,
          178.56
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          307.44,
          192.96,
          541.44,
          232.56
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 7. Predictive likelihood on the NIPS, Freelancer, and BlackHatWorld corpora from deep (multi-level) and flat (two-level) models of tiLDA, with varying numbers of topics.",
        "name": "7",
        "region_bb": [
          66.24000000000001,
          83.52,
          518.4,
          178.56
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          56.160000000000004,
          197.28,
          540.72,
          217.44
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 6. Predictive log-likelihood from two-level models of tiLDA and HDPs. See text for details.",
        "name": "6",
        "region_bb": [
          52.56,
          84.24000000000001,
          532.08,
          190.08
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          99.36,
          209.52,
          496.08000000000004,
          218.16
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3. The generative process of our topic model for hi- erarchical corpora. The process begins in the Main pro- cedure, sampling topic-word profiles and topic proportions from symmetric Dirichlet distributions. Then it recursively executes the GenerateCategory procedure for each internal node of the corpus and the GenerateDocument procedure",
        "name": "3",
        "region_bb": [
          307.44,
          68.4,
          545.04,
          258.48
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          307.44,
          273.6,
          540.72,
          345.6
        ],
        "page": 3,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "nips13_1": {
    "figures": [
      {
        "caption": "Figure 2: Results of the digits experiment. A visualization of the neuron specific weights w n (2a) and latent embedding (2b) learned by the DPP. In (2b) each blue number indicates the position of the neuron that always fires for that specific digit, and the red and green numbers indicate the neurons that respond to that digit but inhibit each other. We observe in (2b) that inhibitory pairs of neurons, the red and green pairs, are placed extremely close to each other in the DPP\u2019s learned latent space while neurons that spike simultaneously (the blue and either red or green) are distant. This scenario emphasizes the benefit of having an inhibitory dependence between neurons. The coupled GLM can not model this scenario well because both neurons of the inhibitory pair receive strong stimulus but there is no indication from past spiking behavior which neuron will spike.",
        "name": "2",
        "region_bb": [
          110.16000000000001,
          92.16,
          504.0,
          235.44
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          251.28,
          503.28000000000003,
          329.76
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 5: A visualization of the periodic component learned by our model. In 5a, the neurons share a single learned periodic frequency and offset but each learn an individual scaling factor \u03c1 n and 5b shows the average influence of the two component mixture on the high and low spike rate neurons. In 5c we provide a reproduction from (Csicsvari et al., 1999) for comparison. In 5a the neurons are colored by firing rate from light (high) to dark (low). Note that the model learns a frequency that is consistent with the approximately 4 Hz theta rhythm and there is a dichotomy in the learned amplitudes, \u03c1, that is consistent with the influence of the theta rhythm on pyramidal cells and interneurons.",
        "name": "5",
        "region_bb": [
          128.16,
          336.24,
          480.96000000000004,
          445.68
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          451.44,
          503.28000000000003,
          519.84
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 1: Model log likelihood on the held out validation set and training set for various combinations of components. We found the algorithm to be extremely stable. Each model configuration was run 5 times with",
        "name": "1",
        "region_bb": [
          123.12,
          82.08,
          488.88,
          180.72
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          190.8,
          504.0,
          216.72
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Figure 4: A visualization of the two dimensional latent embeddings, y n , learned for each neuron. Figure 4b shows 4a zoomed in on the middle of the figure. Each dot indicates the latent value of a neuron. The color of the dots represents the empirical spiking rate of the neuron, the number indicates the depth of the neuron according to its position along the shank - from 0 (shallow) to 7 (deep) - and the letter denotes which of four distinct shanks the neurons spiking was read from. We observe that the higher frequency interneurons are placed distant from each other but in a configuration such that they inhibit the low frequency pyramidal cells.",
        "name": "4",
        "region_bb": [
          128.88,
          100.08,
          478.8,
          251.28
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          259.92,
          503.28000000000003,
          318.96000000000004
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: Results of the simulated moving bar experiment (1a) compared to independent spiking behavior (1b). Note that in 1a the model puts neighboring neurons within the unit length scale while it puts others at least one length scale apart. 1c demonstrates the weights, w \u03bd , of the gain component learned if up to 5x random gain is",
        "name": "1",
        "region_bb": [
          120.96000000000001,
          91.44,
          493.20000000000005,
          182.16
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          107.28,
          198.0,
          503.28000000000003,
          234.72
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3: Visualizations of the parameters learned by the DPP on the Hippocampal data. Figure 3a shows a visualization of the kernel matrix K S . Dark colored entries of K S indicate a strong pairwise inhibition while lighter ones indicate no inhibition. The low frequency neurons, pyramidal cells, are strongly anti-correlated which is consistent with the notion that they are inhibited by a common source such as an interneuron. Figure 3b shows the (normalized) weights, w n learned from the stimulus feature vectors, which consist of concatenated (t) location and orientation bins, to each neuron\u2019s Poisson spike rate \u03bb n . An interesting observation is that the two highest frequency neurons, interneurons, have little dependence on any particular stimulus and are strongly anti-correlated with a large group of low frequency pyramidal cells. 3c shows the weights, w \u03bd to the gain control, \u03bd, and 3d shows a visualization of the stimulus weights for a single neuron n = 3 organized by location and orientation bins. In 3a and 3b the neurons are ordered by their firing rates. In 3d we see that the neuron is stimulated heavily by a specific location and orientation.",
        "name": "3",
        "region_bb": [
          112.32000000000001,
          342.72,
          502.56,
          475.92
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          107.28,
          492.48,
          504.0,
          602.64
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "aaai11_5": {
    "figures": [
      {
        "caption": "Figure 1: Syntax and semantics of EL ++ .",
        "name": "1",
        "region_bb": [
          59.760000000000005,
          54.0,
          287.28000000000003,
          169.92000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          89.28,
          180.0,
          255.60000000000002,
          190.8
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6
    ]
  },
  "aaai12_9": {
    "figures": [
      {
        "caption": "Figure 2: System of inequalities to decide if a row action a can be pruned when there is only one column action.",
        "name": "2",
        "region_bb": [
          90.0,
          238.32000000000002,
          252.72,
          336.24
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          53.28,
          351.36,
          291.6,
          371.52000000000004
        ],
        "page": 3,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 5: Computing the optimistic value \u03b2 a,b",
        "name": "5",
        "region_bb": [
          113.04,
          223.92000000000002,
          497.52000000000004,
          352.08000000000004
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          214.56,
          366.48,
          397.44,
          376.56
        ],
        "page": 4,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 4: Solving Goofspiel with SMAB.",
        "name": "4",
        "region_bb": [
          325.44,
          74.88000000000001,
          552.24,
          265.68
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          355.68,
          54.72,
          519.84,
          63.36
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 2: Solving Goofspiel with backward induction.",
        "name": "2",
        "region_bb": [
          72.72,
          74.88000000000001,
          273.6,
          141.12
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          64.8,
          54.72,
          279.36,
          63.36
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 1: Pruning in Multi-Agent Environments",
        "name": "1",
        "region_bb": [
          54.0,
          74.88000000000001,
          306.0,
          141.12
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          79.2,
          54.72,
          266.40000000000003,
          63.36
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Figure 4: Computing the pessimistic value \u03b1 a,b",
        "name": "4",
        "region_bb": [
          115.2,
          55.440000000000005,
          496.08000000000004,
          187.92000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          211.68,
          203.04000000000002,
          399.6,
          212.4
        ],
        "page": 4,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: System of inequalities for deciding whether row action a is dominated. a is dominated and can be pruned if the system of inequalities is feasible.",
        "name": "1",
        "region_bb": [
          55.440000000000005,
          62.64,
          287.28000000000003,
          177.12
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.0,
          192.24,
          292.32,
          223.20000000000002
        ],
        "page": 3,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 6: L-shaped cell ordering for 5 \u00d7 5 matrices.",
        "name": "6",
        "region_bb": [
          383.76,
          65.52,
          506.16,
          118.8
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          335.52000000000004,
          133.20000000000002,
          540.72,
          141.84
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3: System of inequalities to decide if a column ac- tion b is dominated. b is dominated and can be pruned if the system of inequalities is feasible.",
        "name": "3",
        "region_bb": [
          329.76,
          57.6,
          542.88,
          197.28
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          319.68,
          212.4,
          557.28,
          243.36
        ],
        "page": 3,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 3: Solving Goofspiel with a sequence form solver.",
        "name": "3",
        "region_bb": [
          114.48,
          174.24,
          231.84,
          229.68
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          59.04,
          154.08,
          285.84000000000003,
          162.72
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7
    ]
  },
  "icml13_7": {
    "figures": [
      {
        "caption": "Figure 2. Graphical model for hierarchical Bayesian models with global hidden variables \u03b2, local hidden variables z 1:n , and local observations x 1:n . The hyperparameter \u03b7 is fixed.",
        "name": "2",
        "region_bb": [
          356.40000000000003,
          68.4,
          491.76,
          146.88
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          307.44,
          157.68,
          542.16,
          188.64000000000001
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1. The adaptive learning rate on a run of stochastic variational inference, compared to the best Robbins-Monro and best constant learning rate. Here the data arrives non- uniformly, changing its distribution every three hours. (The algorithms do not know this.) The adaptive learning rate spikes when the data distribution changes. This leads to better predictive performance, as indicated by the held-out",
        "name": "1",
        "region_bb": [
          70.56,
          68.4,
          270.72,
          193.68
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          55.440000000000005,
          206.64000000000001,
          289.44,
          292.32
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 4. Learning rates. The adaptive learning rate has a similar shape to the best Robbins-Monro learning rate, but is obtained automatically and adapts when the data requires.",
        "name": "4",
        "region_bb": [
          62.64,
          267.12,
          521.28,
          383.76
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          55.440000000000005,
          400.32,
          540.72,
          419.76
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3. Held-out log likelihood on three large corpora. (Higher numbers are better.) The best Robbins-Monro rates from Eq. 21 were t 0 = 1000 and \u03ba = 0.7 for the New York Times and Nature copora and t 0 = 1000 and \u03ba = 0.8 for Wikipedia corpus. The best constant rate was 0.01. The adaptive learning rate performed best.",
        "name": "3",
        "region_bb": [
          62.64,
          76.32000000000001,
          521.28,
          192.96
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.72,
          210.24,
          541.44,
          240.48000000000002
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "icml13_3": {
    "figures": [
      {
        "caption": "Figure 2. We compare the efficient implementation (using incomplete Cholesky decomposition) of our conditional co- variance operator-valued KDE method with the KDE al- gorithm of Cortes et al. (2005). While the parameter n is m1 = m2 = n in the incomplete Cholesky decompo- sition, it is the number of faces randomly selected from 1,200 training faces in the KDE algorithm of Cortes et al. (2005). The right-most point is the MSE of training on the full training set of n = 1, 200 examples.",
        "name": "2",
        "region_bb": [
          318.96000000000004,
          75.60000000000001,
          522.0,
          197.28
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          306.72,
          216.0,
          540.72,
          311.76
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1. Kernel Dependency Estimation. Our general- ized formulation consists of learning the mapping g using an operator-valued kernel ridge regression rather than a scalar-valued one as in the formulations of Weston et al. (2003) and Cortes et al. (2005). Using an operator-valued kernel mapping, we construct a joint feature space from information of input and output spaces in which input- output and output correlations can be taken into account.",
        "name": "1",
        "region_bb": [
          316.08,
          57.6,
          510.48,
          200.16
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          307.44,
          213.84,
          540.72,
          298.8
        ],
        "page": 3,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 3. Mean-squared errors (MSE) of the JKM algorithm of Weston et al. (2007), KDE algorithm of Cortes et al. (2005), and our KDE method with covariance operator- valued kernels on the face-to-face mapping problem.",
        "name": "3",
        "region_bb": [
          55.440000000000005,
          129.6,
          295.92,
          191.52
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          55.440000000000005,
          77.04,
          288.72,
          118.08
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 2. (Left) Performance (mean and standard deviation of RBF loss) of the KDE algorithms of Weston et al. (2003) and Cortes et al. (2005), and our KDE method with covariance and conditional covariance operator-valued kernels on an image reconstruction problem of handwritten digits. (Right) Performance (mean and standard deviation of Well Rec- ognized word Characters (WRC)) of Max-Margin Markov Networks (M3 Ns) algorithm (Taskar et al., 2004), constrained regression version of KDE (Cortes et al., 2007), and our KDE method on an optical character recognition (OCR) task.",
        "name": "2",
        "region_bb": [
          55.440000000000005,
          141.12,
          537.84,
          203.76000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          55.440000000000005,
          77.04,
          540.72,
          129.6
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 1. This table summarizes the notations used in the paper.",
        "name": "1",
        "region_bb": [
          72.72,
          92.88000000000001,
          520.5600000000001,
          177.84
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          168.48000000000002,
          76.32000000000001,
          426.96000000000004,
          84.24000000000001
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "aaai10_4": {
    "figures": [
      {
        "caption": "Figure 2: Four Equivalent WCSPs",
        "name": "2",
        "region_bb": [
          335.52000000000004,
          54.0,
          542.16,
          231.84
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          370.08000000000004,
          247.68,
          506.16,
          257.04
        ],
        "page": 3,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: Oscillation in EDAC* enforcement",
        "name": "1",
        "region_bb": [
          95.76,
          54.0,
          248.4,
          208.8
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          82.8,
          223.92000000000002,
          262.8,
          233.28
        ],
        "page": 3,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 1: Experimental results: time (in seconds) and number",
        "name": "1",
        "region_bb": [
          321.12,
          53.28,
          555.84,
          422.64000000000004
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          318.96000000000004,
          437.04,
          558.0,
          455.76
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7
    ]
  },
  "icml12_2": {
    "figures": [
      {
        "caption": "Figure 2. Typical filters (weight vectors) of the first layer from the CAE-2 used to produce face samples.",
        "name": "2",
        "region_bb": [
          55.440000000000005,
          591.84,
          290.16,
          625.6800000000001
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          56.160000000000004,
          639.36,
          288.72,
          657.36
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 3. Test classification error of several models, trained on TFD, averaged over 5 folds (reported with standard deviation).",
        "name": "3",
        "region_bb": [
          307.44,
          485.28000000000003,
          547.2,
          504.72
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          306.72,
          437.76,
          540.0,
          468.72
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 2. Average normalized sensitivity (\u00af\u03b3 ) of last layer to affine deformations of MNIST digits. The deformations were not used in any way during training. Second column shows difference with CAE-2p together with standard er- ror of the differences. The proposed CAE-2p appears to be significantly less sensitive (more invariant) than other models.",
        "name": "2",
        "region_bb": [
          351.36,
          174.96,
          496.8,
          234.0
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          307.44,
          95.76,
          540.72,
          168.48000000000002
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 1. Log-Likelihoods from Parzen density estimator using 10000 samples from each model",
        "name": "1",
        "region_bb": [
          315.36,
          577.44,
          533.52,
          613.44
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          308.16,
          551.52,
          540.72,
          570.96
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Figure 1. Samples from models trained on MNIST (left) and TFD (right). Top row: 2-layer CAE using proposed sampling procedure (Jacobian-based). Middle row: 2-layer DBN using Gibbs sampling. Bottom row: samples obtained by adding isotropic instead of Jacobian-based Gaussian noise.",
        "name": "1",
        "region_bb": [
          97.2,
          66.96000000000001,
          499.68,
          174.24
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          55.440000000000005,
          189.36,
          540.72,
          219.6
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3. Evolution of the reconstruction error term, as we sample from CAE-2 trained on MNIST, starting from uniform random pixels (point not shown, way above the graph). Sampling chain using either Jacobian-based (blue) or isotropic (green) hidden unit perturbation. Reconstruc- tion error may be interpreted as an indirect measure of likelihood.",
        "name": "3",
        "region_bb": [
          313.92,
          231.12,
          522.72,
          382.32
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          307.44,
          403.2,
          541.44,
          475.2
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8
    ]
  },
  "icml12_10": {
    "figures": [
      {
        "caption": "Figure 2. Histograms of faces (red) vs. no faces (blue). The test set is subsampled such that the ratio between",
        "name": "2",
        "region_bb": [
          115.92,
          438.48,
          228.96,
          524.88
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.72,
          538.5600000000001,
          288.72,
          567.36
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 5. Translational invariance properties of the best feature. x-axis is in pixels",
        "name": "5",
        "region_bb": [
          65.52,
          182.88,
          279.36,
          263.52
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.72,
          277.2,
          288.72,
          295.92
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 2. Summary of classification accuracies for our method and other state-of-the-art baselines on ImageNet.",
        "name": "2",
        "region_bb": [
          86.4,
          267.12,
          501.12,
          313.2
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          74.16,
          261.36,
          521.28,
          270.0
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 1. Summary of numerical comparisons between our algorithm against other baselines. Top: Our algorithm vs. simple baselines. Here, the first three columns are results for methods that do not require training: random guess, random weights (of the network at initialization, without any training) and best linear filters selected from 100,000 examples sampled from the training set. The last three columns are results for methods that have training: the best neuron in the first layer, the best neuron in the highest layer after training, the best neuron in the network when the contrast normalization layers are removed. Bottom: Our algorithm vs. autoencoders and K-means.",
        "name": "1",
        "region_bb": [
          61.2,
          145.44,
          535.6800000000001,
          252.0
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.72,
          76.32000000000001,
          540.72,
          139.68
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Figure 4. Scale (left) and out-of-plane (3D) rotation (right)",
        "name": "4",
        "region_bb": [
          65.52,
          66.96000000000001,
          279.36,
          149.04
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.72,
          162.72,
          288.72,
          180.72
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1. The architecture and parameters in one layer of our network. The overall network replicates this structure three times. For simplicity, the images are in 1D.",
        "name": "1",
        "region_bb": [
          77.04,
          141.84,
          267.12,
          332.64
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.72,
          346.32,
          290.16,
          377.28000000000003
        ],
        "page": 3,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 6. Visualization of the cat face neuron (left) and human body neuron (right).",
        "name": "6",
        "region_bb": [
          311.04,
          66.96000000000001,
          537.12,
          154.8
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          307.44,
          168.48000000000002,
          540.72,
          187.20000000000002
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3. Top: Top 48 stimuli of the best neuron from the test set. Bottom: The optimal stimulus according to nu- merical constraint optimization.",
        "name": "3",
        "region_bb": [
          342.0,
          241.92000000000002,
          506.16,
          498.96000000000004
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          307.44,
          513.36,
          540.72,
          542.88
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8
    ]
  },
  "icml14_3": {
    "figures": [
      {
        "caption": "Figure 2. Graphical model depicting the online model learning problem, and the assumptions of PBRL, in terms of states s and actions a. Latent variables \u0393 (geometric properties) and \u03a6 (dy- namics properties) parameterize the full time-series model. \u03c0(\u00b7) denotes the policy and f (\u00b7) denotes the dynamics function. We",
        "name": "2",
        "region_bb": [
          59.04,
          66.96000000000001,
          294.48,
          202.32
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.72,
          218.88,
          288.72,
          279.36
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 5. (a) Initial state (b) expected outcome (c) actual outcome; model updated (d) final solution",
        "name": "5",
        "region_bb": [
          311.76,
          333.36,
          537.12,
          532.08
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          308.16,
          548.64,
          540.72,
          567.36
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 2. Table of the relevant algorithm parameters for each experiment. k: number of nearest neighbors (LWR,OO-LWR), \u03bb: bandwidth (LWR,OO-LWR), n s : number of sectors (OO-LWR), n tps : number of raycast collision tests per sector (OO-LWR),   c : collision radius (OO-LWR), prior: type of prior (PBRL), MCMC: sampler parameters (iterations, burn-in, thin, number of chains) (PBRL).",
        "name": "2",
        "region_bb": [
          135.36,
          67.68,
          458.64000000000004,
          100.8
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          56.160000000000004,
          113.04,
          540.72,
          144.0
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 1. Univariate distributions for each physical parameter, with",
        "name": "1",
        "region_bb": [
          307.44,
          68.4,
          547.2,
          148.32
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          308.16,
          159.84,
          540.0,
          180.0
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Figure 4. Online performance of various agents under different domain sizes and training conditions.",
        "name": "4",
        "region_bb": [
          59.04,
          161.28,
          545.76,
          294.48
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          117.36,
          309.6,
          478.08000000000004,
          318.24
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1. Detecting collision sectors for contact predicates (OO- LWR) in an apartment task.",
        "name": "1",
        "region_bb": [
          84.24000000000001,
          66.96000000000001,
          260.64,
          242.64000000000001
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          56.160000000000004,
          256.32,
          288.72,
          276.48
        ],
        "page": 3,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3. Simulated manipulation domains",
        "name": "3",
        "region_bb": [
          309.6,
          72.0,
          533.52,
          190.8
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          347.76,
          206.64000000000001,
          500.40000000000003,
          214.56
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "aaai10_1": {
    "figures": [
      {
        "caption": "Figure 2: An example pursuit-evasion scenario in the \u2018Office\u2019 environment. The big circles represent the sensors and the square represents the evader. The size of the grey circles is proportional to the value of f . The evader is captured at t = 14.",
        "name": "2",
        "region_bb": [
          61.92,
          53.28,
          550.08,
          205.92000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          53.28,
          221.76000000000002,
          558.0,
          241.92000000000002
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: The factor graph for three sensors.",
        "name": "1",
        "region_bb": [
          83.52,
          54.0,
          257.04,
          137.52
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          84.96000000000001,
          151.92000000000002,
          260.64,
          161.28
        ],
        "page": 4,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3: Empirical performance on 200 problem instances. The band near the center of the boxes is the median of the dataset; the box contains data points between the 25 th and 75 th percentile. Whiskers are drawn at 1.5 inter-quartile range (IQR).",
        "name": "3",
        "region_bb": [
          124.56,
          55.440000000000005,
          493.92,
          206.64000000000001
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.0,
          221.76000000000002,
          557.28,
          241.92000000000002
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 1: Lower bound of the 95% confidence intervals of performance increase of MS K-M compared to the four most competitive benchmarks.",
        "name": "1",
        "region_bb": [
          53.28,
          262.08,
          293.04,
          326.16
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.0,
          341.28000000000003,
          292.32,
          372.96000000000004
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7
    ]
  },
  "icml10_6": {
    "figures": [
      {
        "caption": "Figure 2. Results for an instantiation of the two moons dataset, 2000 points in 100 dimensions.",
        "name": "2",
        "region_bb": [
          71.28,
          74.16,
          524.16,
          208.08
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          103.68,
          223.92000000000002,
          487.44,
          232.56
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1. Results on two moons and CIFAR-10. Two moons is averaged over 100 instances, and CIFAR-10 is averaged over the 45 pairs of two class problems.",
        "name": "1",
        "region_bb": [
          312.48,
          113.76,
          536.4,
          319.68
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          307.44,
          77.76,
          540.72,
          108.72
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3. Results for an instantiation of the two moons dataset, 2000 points in 100 dimensions, with the modified algorithm 3.1 as in Remark 1 .",
        "name": "3",
        "region_bb": [
          347.04,
          343.44,
          501.12,
          667.44
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          306.72,
          682.5600000000001,
          540.72,
          713.52
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 1. Top: the confusion matrix for the clustering of MNIST using Algorithm 3.1 iterated as described in 4.1. Each row is a cluster; the number in the leftmost column of each row is the dominant label of that cluster. The 4\u2019s and 9\u2019s are merged, but otherwise the clustering is very accu- rate. The total computation time (not including construct- ing the weights) was 214 seconds. Middle: the confusion matrix using (B\u00a8 uhler & Hein, 2009); the total computation time is 7303 seconds. Bottom: the confusion matrix using the iterated second eigenvector cut. More classes have been merged, and the ones broken into 4 clusters. The compu-",
        "name": "1",
        "region_bb": [
          332.64,
          209.52,
          516.24,
          473.76
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          306.72,
          76.32000000000001,
          542.16,
          203.04000000000002
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8
    ]
  },
  "nips10_5": {
    "figures": [
      {
        "caption": "Figure 2: Comparison on classification accuracy",
        "name": "2",
        "region_bb": [
          109.44,
          88.56,
          505.44,
          523.44
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          209.52,
          522.0,
          401.76,
          531.36
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: An illustrative example for selecting informative and representative instances",
        "name": "1",
        "region_bb": [
          115.2,
          82.8,
          480.24,
          177.12
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          132.48000000000002,
          188.64000000000001,
          478.8,
          198.0
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 2: Win/tie/loss counts of Q UIRE versus the other methods with varied numbers of queries.",
        "name": "2",
        "region_bb": [
          115.2,
          106.56,
          496.8,
          201.6
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          113.76,
          90.72,
          496.8,
          100.08
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 1: Comparison on AUC values (mean \u00b1 std). The best performance and its comparable performances based on paired t-tests at 95% significance level are highlighted in boldface.",
        "name": "1",
        "region_bb": [
          111.60000000000001,
          117.36,
          500.40000000000003,
          684.0
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          107.28,
          90.72,
          503.28000000000003,
          111.60000000000001
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "aaai10_8": {
    "figures": [
      {
        "caption": "Figure 2. An instance of the problem of path planning for mul- tiple robots with the conjugation principle for a group of robots \ud835\udc45 \ud835\udc3a = {\ud835\udc60 1 , \ud835\udc60 2 , \ud835\udc60 3 , \ud835\udc60 4 }. The bottom line of vertices is open (not locked) only at time step 0. Dark vertices in the 4 th line (1 st line in on the top) can be entered only at time step 8; dark vertices in the 6 th line can be entered only at time step 14. The principle of con- jugation is that all the robots \ud835\udc60 1 , \ud835\udc60 2 , \ud835\udc60 3 , and \ud835\udc60 4 are located either in \ud835\udc49 \u2112 or in \ud835\udc49 \u211b at time step 1 within any optimal solution.",
        "name": "2",
        "region_bb": [
          319.68,
          408.24,
          558.72,
          604.8000000000001
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          318.24,
          610.5600000000001,
          558.0,
          692.64
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1. An illustration of vertex locking. A vertex \ud835\udc63 is to be locked at time steps 1 and 3. An augmentation \u03a3 \u2032 of an instance \u03a3 is implemented by adding a path around the locked vertex \ud835\udc63 while newly added robots are enforced to go through \ud835\udc63 exactly at time",
        "name": "1",
        "region_bb": [
          54.0,
          261.36,
          293.04,
          407.52000000000004
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.72,
          413.28000000000003,
          291.6,
          463.68
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3. A polynomial time reduction of a Boolean formula to a decision instance of multi-robot path planning. The conjugation technique is used to simulate Boolean consistency and the set locking technique is used to simulate clause satisfaction (reduc- tion of one variable/clause is shown). There exists a solution of \u03a3 of the makespan \ud835\udf02 = 11 if and only if the formula \ud835\udc39 is satisfiable.",
        "name": "3",
        "region_bb": [
          54.0,
          54.0,
          293.04,
          342.0
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          53.28,
          347.76,
          292.32,
          408.96000000000004
        ],
        "page": 3,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3
    ]
  },
  "icml14_2": {
    "figures": [
      {
        "caption": "Figure 1. Results of the experiments on three datasets. For details, see Sections 3.1 and 3.2",
        "name": "1",
        "region_bb": [
          75.60000000000001,
          82.08,
          511.92,
          343.44
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          135.36,
          359.28000000000003,
          460.08000000000004,
          367.92
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "aaai14_8": {
    "figures": [
      {
        "caption": "Figure 1: (a) derivation of B(a) from P 1 \u222a D 1 ; (b) derivation of B(a) from \u039e(P 1 ) \u222a D 1",
        "name": "1",
        "region_bb": [
          72.72,
          56.88,
          539.28,
          151.20000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          129.6,
          165.6,
          480.96000000000004,
          174.96
        ],
        "page": 4,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 2: Average query answering times",
        "name": "2",
        "region_bb": [
          318.96000000000004,
          54.0,
          556.5600000000001,
          114.48
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          358.56,
          125.28,
          518.4,
          133.92000000000002
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 1: Normalised RL (t) axioms, with A, B atomic or >, C atomic or \u22a5, R, S, T atomic roles, a an individual.",
        "name": "1",
        "region_bb": [
          318.96000000000004,
          61.92,
          558.0,
          181.44
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          318.96000000000004,
          200.16,
          557.28,
          222.48000000000002
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7
    ]
  },
  "icml12_1": {
    "figures": [
      {
        "caption": "Figure 2. The  r 1 -region-graph consisting of all the mes- sages to r 1 . The variables in each region and its counting number are shown. The upward and downward messages are passed along the edges in this  r 1 -region-graph.",
        "name": "2",
        "region_bb": [
          332.64,
          72.0,
          516.96,
          141.84
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          307.44,
          160.56,
          540.72,
          202.32
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1. Part of a factor graph, where circles are variables (circle labeled \u201ci\u201d corresponding to variable \u201cx i \u201d) and squares (with CAPITAL letters) represent factors. Note variables {x i , x k , x s } form a loop, as do {x k , x u , x t }, etc. (a) An example of absorbing short loops into overlapping regions. Here, a region includes factors around each hexagon and all its variables. Factor I and the variables x i , x j , x k appear in the three regions r 1 , r 2 , r 3 . (Figure just shows index \u03b1 for region r \u03b1 .) Region-based methods provide a way to perform inference on overlapping regions. (In general, regions do not have to involve exactly 3 variables and 3 factors.) (b) Cavity variables for x s are {x w , x j , x k , x u , x v }, shown using dotted circles. We define the cavity distribution for x s by removing all the factors around this variable, and marginalizing the remaining factor-graph on dotted circles. Even after removing factors {T, Y, W }, the variables x v , x w , and x j , x k , x u still have higher-order interactions caused by remaining factors, due to loops in the factor graph. (c) Cavity region r 1 = {j, s, k} includes variables shown in pale circles. Variables in dotted circles are the perimeter  r 1 . Removing the \u201cpale factors\u201d and marginalizing the rest of network on  r 1 , gives the cavity distribution for r 1 .",
        "name": "1",
        "region_bb": [
          55.440000000000005,
          57.6,
          558.0,
          221.04000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          55.440000000000005,
          229.68,
          540.72,
          359.28000000000003
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 4. Time vs error for 3-regular Ising models with lo- cal field and interactions sampled from a standard normal. Each method in the graph has 10 points, each representing an Ising model of different size (10 to 100 variables).",
        "name": "4",
        "region_bb": [
          338.40000000000003,
          75.60000000000001,
          494.64000000000004,
          228.24
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          307.44,
          227.52,
          540.72,
          272.88
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3. Average Run-time and accuracy for: (Left) 6x6 spinglass grids for different values of \u03b2. Variable interactions are sampled from N (0, \u03b2 2 ), local fields are sampled from N (0, 1). (Middle) various grid-sizes: [5x5, . . . , 10x10]; Factors are sampled from N (0, 1). (Right) 3-regular Ising models with local field and interactions sampled from N (0, \u03b2 2 ).",
        "name": "3",
        "region_bb": [
          59.760000000000005,
          79.92,
          527.04,
          246.96
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          56.160000000000004,
          261.36,
          540.72,
          291.6
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 1. Performance of varoius methods on Alarm",
        "name": "1",
        "region_bb": [
          55.440000000000005,
          330.48,
          273.6,
          472.32
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          63.36,
          321.12,
          277.2,
          329.76
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8
    ]
  },
  "aaai11_7": {
    "figures": [
      {
        "caption": "Figure 1: Our Model of Information Quality Assessment",
        "name": "1",
        "region_bb": [
          56.88,
          56.88,
          289.44,
          207.36
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          60.480000000000004,
          223.92000000000002,
          285.84000000000003,
          233.28
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2
    ]
  },
  "nips09_5": {
    "figures": [
      {
        "caption": "Figure 2: The SRB (spatially regularized boosting algorithms).",
        "name": "2",
        "region_bb": [
          108.0,
          389.52000000000004,
          504.0,
          671.76
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          192.96,
          683.28,
          418.32,
          691.2
        ],
        "page": 4,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 5: Experiment 2. (a-c): test classification accuracy: (a) i.i.d. Gaussian noise, (b) poisson noise, (c) spatially correlated Gaussian noise. (b,c) share the legend of (a). (d-f): pixel selection performances: (d) i.i.d. Gaussian noise, (e) poisson noise, (f) spatial correlated Gaussian noise. (e,f) share the legend of (d).",
        "name": "5",
        "region_bb": [
          110.88000000000001,
          83.52,
          501.12,
          356.40000000000003
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          373.68,
          503.28000000000003,
          401.76
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 4: Experiment 2. (a-d): example im- ages; (e): example training image with noise; (f): ground truth of discriminative pixels; (g-h): pixels selected by (g) AdaBoost and (h) SRB.",
        "name": "4",
        "region_bb": [
          333.36,
          154.08,
          504.0,
          250.56
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          333.36,
          272.88,
          503.28000000000003,
          310.32
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: Each graph is the eigenimage of size d \u00d7 d corresponding to an eigenvector of K = \u00b5I \u2212 G.",
        "name": "1",
        "region_bb": [
          117.36,
          491.76,
          497.52000000000004,
          532.8000000000001
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          120.96000000000001,
          550.08,
          490.32,
          558.72
        ],
        "page": 3,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 6: Experiment 3: an example: sets of voxels selected by (a) univariate t-test (b) AdaBoost and (c) SRB",
        "name": "6",
        "region_bb": [
          113.04,
          419.76,
          498.96000000000004,
          486.0
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          109.44,
          502.56,
          501.84000000000003,
          511.20000000000005
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3: Experiment 1. (a): an example showing annotated points; (b-c): the average component importance map \u03b2 (in- dicated by sizes of the circles) after running (b) AdaBoost and (c) SRB for 50 iterations.",
        "name": "3",
        "region_bb": [
          108.0,
          159.12,
          318.96000000000004,
          254.16
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          107.28,
          276.48,
          321.12,
          313.2
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "aaai12_8": {
    "figures": [
      {
        "caption": "Figure 2: Illustration of the advanced grouping strategy for lifted importance sampling. Here we sample indices of j i for each Y i \u2208 \u2206 y . Let the sampled indices j i be as shown. Then, we will get the same MLN as the one in Fig. 1(c).",
        "name": "2",
        "region_bb": [
          54.72,
          179.28,
          294.48,
          234.72
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.72,
          241.92000000000002,
          292.32,
          280.08
        ],
        "page": 4,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: Illustration of lifted importance sampling. (a) An example MLN. (b) Sampled groundings of R(x, y). (c) Reduced MLN obtained by instantiating the sampled groundings of R.",
        "name": "1",
        "region_bb": [
          75.60000000000001,
          64.08,
          536.4,
          130.32
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.0,
          146.16,
          557.28,
          164.88
        ],
        "page": 4,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3: Lower bound on the partition function computed using LIS, ILIS and ALIS as a function of time. (a) The example R, S, T domain with 100 objects. (b) The example R, S, T domain with 150 objects. (c) WEBKB MLN with 200 objects, (d) WEBKB MLN with 300 objects, (e) Entity Resolution MLN with 200 objects and (f) Entity resolution MLN with 300 objects. Note that for each point, we have plotted error bars showing the standard deviation. When the standard deviation is small, the error bars are not visible in the plots.",
        "name": "3",
        "region_bb": [
          71.28,
          66.96000000000001,
          536.4,
          346.32
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.0,
          362.16,
          557.28,
          400.32
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7
    ]
  },
  "icml10_10": {
    "figures": [
      {
        "caption": "Figure 2. Oblivious Roulette",
        "name": "2",
        "region_bb": [
          307.44,
          69.12,
          541.44,
          398.88
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          365.76,
          410.40000000000003,
          481.68,
          419.04
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 3. Computation time per step (second) and info. model. The results are the average of 100 iterations.",
        "name": "3",
        "region_bb": [
          103.68,
          84.96000000000001,
          493.20000000000005,
          141.84
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          79.92,
          77.04,
          516.24,
          85.68
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 2. Information model and sequence observation.",
        "name": "2",
        "region_bb": [
          63.36,
          87.84,
          281.52,
          138.96
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          55.440000000000005,
          76.32000000000001,
          272.16,
          84.24000000000001
        ],
        "page": 4,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 1. The regret bound and information model",
        "name": "1",
        "region_bb": [
          54.72,
          85.68,
          524.16,
          145.44
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          197.28,
          76.32000000000001,
          398.88,
          84.96000000000001
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Figure 1. Distributed roulette",
        "name": "1",
        "region_bb": [
          307.44,
          69.12,
          541.44,
          259.92
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          363.6,
          275.04,
          483.84000000000003,
          283.68
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3. Expectation of regret per time (avg. of 100 iter- ations, N = 10, T = 5000).",
        "name": "3",
        "region_bb": [
          64.08,
          74.16,
          280.08,
          213.12
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.72,
          234.0,
          288.72,
          253.44
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8
    ]
  },
  "nips13_5": {
    "figures": [
      {
        "caption": "Figure 2: The architecture of a single Fisher layer. Left: the arrows illustrate the data flow through\nthe layer; the dimensionality of densely computed features is shown next to the arrows. Right: spatial\npooling (the blue squares) and stacking (the red square) in sub-layers 1 and 2 respectively.",
        "name": "2",
        "region_bb": [
          241,
          171,
          1035,
          347
        ],
        "page_height": 1651,
        "page_width": 1275,
        "caption_bb": [
          225,
          377,
          1048,
          441
        ],
        "page": 4,
        "dpi": 150.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: Left: Fisher network (Sect. 4) with two Fisher layers. Right: conventional pipeline\nusing a shallow Fisher vector encoding. As shown in Sect. 6, making the conventional pipeline\nslightly deeper by injecting a single Fisher layer substantially improves the classification accuracy.",
        "name": "1",
        "region_bb": [
          327,
          171,
          943,
          583
        ],
        "page_height": 1651,
        "page_width": 1275,
        "caption_bb": [
          225,
          610,
          1049,
          678
        ],
        "page": 3,
        "dpi": 150.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 3: Performance on ILSVRC-2010 using dense SIFT and colour features. We also specify\nthe dimensionality of SIFT-based image representations.",
        "name": "3",
        "region_bb": [
          283,
          235,
          987,
          370
        ],
        "page_height": 1651,
        "page_width": 1275,
        "caption_bb": [
          225,
          191,
          1048,
          230
        ],
        "page": 8,
        "dpi": 150.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 2: Evaluation of multi-scale pooling and multi-layer image description on the subset of\nILSVRC-2010. The following configuration of Fisher layers was used: d1 = 128, K1 = 256,\nh1 = 200, d2 = 200, K2 = 256. Both Fisher layers used spatial coordinate augmentation. The\nbaseline performance of a shallow FV encoding is 59.51% and 80.50% (top-1 and top-5 accuracy).",
        "name": "2",
        "region_bb": [
          324,
          489,
          946,
          576
        ],
        "page_height": 1651,
        "page_width": 1275,
        "caption_bb": [
          225,
          399,
          1051,
          483
        ],
        "page": 7,
        "dpi": 150.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 1: Evaluation of dimensionality reduction, stacking, and normalisation sub-layers on a\n200 class subset of ILSVRC-2010. The following configuration of Fisher layers was used: d1 =\n128, K1 = 256, q1 = 5, \u03b41 = 1, h1 = 200 (number of classes), d2 = 200 , K2 = 256. The baseline\nperformance of a shallow FV encoding is 57.03% and 78.9% (top-1 and top-5 accuracy).",
        "name": "1",
        "region_bb": [
          382,
          278,
          888,
          390
        ],
        "page_height": 1651,
        "page_width": 1275,
        "caption_bb": [
          225,
          191,
          1049,
          278
        ],
        "page": 7,
        "dpi": 150.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "aaai14_5": {
    "figures": [
      {
        "caption": "Figure 2: H(\u03c0 k ) and \u2206(\u03c0 k ) for \u03c0 k\u2208[0,9] \u2208 {U, D, PL}",
        "name": "2",
        "region_bb": [
          54.72,
          54.0,
          290.88,
          162.72
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          63.36,
          172.8,
          282.24,
          183.6
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: 2\u2212dimensional utility space and its hypergraph",
        "name": "1",
        "region_bb": [
          323.28000000000003,
          494.64000000000004,
          552.96,
          577.44
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          325.44,
          588.96,
          551.52,
          597.6
        ],
        "page": 1,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 4: Duration when high degree nodes are used",
        "name": "4",
        "region_bb": [
          321.84000000000003,
          392.40000000000003,
          555.84,
          501.12
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          334.8,
          511.92,
          542.88,
          520.5600000000001
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3: AsynchMP vs. SA",
        "name": "3",
        "region_bb": [
          319.68,
          98.64,
          554.4,
          205.92000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          383.04,
          217.44,
          493.92,
          226.08
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2
    ]
  },
  "icml12_7": {
    "figures": [
      {
        "caption": "Figure 1. The pseudo-code of the CBMPI algorithm.",
        "name": "1",
        "region_bb": [
          55.440000000000005,
          72.72,
          289.44,
          323.28000000000003
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          66.24000000000001,
          339.12,
          277.92,
          347.76
        ],
        "page": 3,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8
    ]
  },
  "aaai10_9": {
    "figures": [
      {
        "caption": "Figure 2: Average precision over all trials for all methods for cold start (black), i.e., no other clusters formed yet, and warm start (grey), i.e., 50 items already in other clusters. \u201cHybrid\u201d is our method; \u201cmetric\u201d and \u201cclassifier\u201d refer to the individual learners; \u201cbest-ind\u201d represents an unimplementable upper bound on the individual methods. See Section \u201cCombining the Learners\u201d for more details about each method.",
        "name": "2",
        "region_bb": [
          321.84000000000003,
          429.84000000000003,
          556.5600000000001,
          609.84
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          319.68,
          612.72,
          558.0,
          683.28
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1. A screenshot of our assisted clustering system, showing suggested items for the selected cluster. Note that these recommendations were turned off while subjects were clustering items to provide ground-truth data for our experiments.",
        "name": "1",
        "region_bb": [
          55.440000000000005,
          206.64000000000001,
          294.48,
          423.36
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.72,
          423.36,
          294.48,
          464.40000000000003
        ],
        "page": 3,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 4: Average precision vs. number of positive examples where 50 items were already placed into other clusters (warm start condition).",
        "name": "4",
        "region_bb": [
          322.56,
          344.16,
          558.72,
          501.12
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          326.16,
          501.84000000000003,
          550.08,
          532.8000000000001
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3: Average precision vs. number of positive examples from",
        "name": "3",
        "region_bb": [
          322.56,
          152.64000000000001,
          563.04,
          310.32
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          318.96000000000004,
          313.92,
          557.28,
          329.76
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7
    ]
  },
  "icml10_1": {
    "figures": [
      {
        "caption": "Figure 2. EE trained with homotopy with a 2D spiral.",
        "name": "2",
        "region_bb": [
          78.0,
          94.0,
          748.0,
          219.0
        ],
        "page_height": 1100,
        "page_width": 850,
        "caption_bb": [
          261.0,
          233.0,
          565.0,
          245.0
        ],
        "page": 6,
        "dpi": 100.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1. Left: plot of (x(\u03bb)) 2 when the dataset has N = 2 points in 1D, for w + = w \u2212 = \u03bb \u2217 1 = 1. Right: plot of the squared diameter of X(\u03bb) for the Swiss roll example (blue",
        "name": "1",
        "region_bb": [
          70.0,
          96.0,
          402.0,
          239.0
        ],
        "page_height": 1100,
        "page_width": 850,
        "caption_bb": [
          76.0,
          250.0,
          401.0,
          310.0
        ],
        "page": 4,
        "dpi": 100.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 4. Affinities w nm = w nm \u2212 \u03bbw nm exp (\u2212 kx n \u2212 x m k 2 ) learned for a point x n near the centre of the Swiss roll for",
        "name": "4",
        "region_bb": [
          427.0,
          844.0,
          750.0,
          921.0
        ],
        "page_height": 1100,
        "page_width": 850,
        "caption_bb": [
          427.0,
          940.0,
          762.0,
          986.0
        ],
        "page": 6,
        "dpi": 100.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3. Swiss roll. Top: EE with homotopy; we show X for different \u03bb. Bottom: true X and results with other methods.",
        "name": "3",
        "region_bb": [
          77.0,
          267.0,
          752.0,
          448.0
        ],
        "page_height": 1100,
        "page_width": 850,
        "caption_bb": [
          76.0,
          468.0,
          750.0,
          480.0
        ],
        "page": 6,
        "dpi": 100.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 5. Results of EE, SNE and t-SNE with the COIL-20 dataset, all randomly initialised. Right plot: result of EE when trained on half the data, and points x = F(y) (\u2022 marks) predicted for the other half of the data (only 5 of the sequences are shown to avoid clutter). Below : images y = f (x) predicted for out-of-sample points in x-space along sequence 1.",
        "name": "5",
        "region_bb": [
          77.0,
          94.0,
          753.0,
          364.0
        ],
        "page_height": 1100,
        "page_width": 850,
        "caption_bb": [
          76.0,
          383.0,
          751.0,
          425.0
        ],
        "page": 7,
        "dpi": 100.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8
    ]
  },
  "icml11_1": {
    "figures": [],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8
    ]
  },
  "icml10_7": {
    "figures": [
      {
        "caption": "Figure 2. Example of learned graphs by different methods: (a1, b1) true component graphs of state 1 and state 2; (a2, b2) learned graphs by baseline method SUB; (a3, b3) learned graphs by hMRF; (4) learned graph by baseline method ALL",
        "name": "2",
        "region_bb": [
          62.64,
          271.44,
          288.0,
          368.64
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          55.440000000000005,
          388.08000000000004,
          288.72,
          438.48
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 3. Top 10 genes by out-degress in the learned graphs by different methods",
        "name": "3",
        "region_bb": [
          309.6,
          253.44,
          538.5600000000001,
          349.92
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          307.44,
          227.52,
          540.72,
          246.96
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 2. Percentage of overlap between bootstrap graphs and original graphs",
        "name": "2",
        "region_bb": [
          56.88,
          253.44,
          288.0,
          294.48
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          55.440000000000005,
          227.52,
          288.72,
          246.96
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 1. Comparison results of structure learning on sim- ulation data (sample size per node = 500)",
        "name": "1",
        "region_bb": [
          312.48,
          305.28000000000003,
          536.4,
          356.40000000000003
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          308.16,
          279.36,
          540.72,
          298.8
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Figure 4. Causal graph learned by ALL (A), SUB (B-C) and hMRF (D-F). The location in (B) is [30.475,-114.75] and the location in (C) is [42.975, -99.75]. Green edge: common edges shared by (D-F); red edge: additional edges unique to",
        "name": "4",
        "region_bb": [
          64.08,
          73.44,
          529.2,
          148.32
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          55.440000000000005,
          169.20000000000002,
          540.72,
          198.0
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1. Comparison results on Simulation Data II: (a) performance (in F 1 ) of structure learning by competing methods for component graph associated with state 1. x axis: number of sample size per node, y axis: F 1 score. (b) performance (in F 1 ) of structure learning by competing methods for component graph associated with state 2. (c) performance of state prediction. x axis: number of sample size per task, y axis: F 1 scores.",
        "name": "1",
        "region_bb": [
          97.2,
          77.76,
          505.44,
          187.20000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          56.160000000000004,
          208.08,
          540.72,
          249.84
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3. Predicted labels of underlying hidden states for each location. Green diamonds: state 1; Red circle: state 2; Blue square: state 3",
        "name": "3",
        "region_bb": [
          336.96000000000004,
          70.56,
          515.52,
          349.92
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          307.44,
          370.8,
          540.72,
          401.04
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8
    ]
  },
  "nips13_7": {
    "figures": [
      {
        "caption": "Figure 2: Performance comparison in the discounted setting using the distribution of D \u03b8 (x 0 ).",
        "name": "2",
        "region_bb": [
          120.96000000000001,
          91.44,
          482.40000000000003,
          218.88
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          118.8,
          227.52,
          489.6,
          238.32000000000002
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: The overall flow of our simultaneous perturbation based actor-critic algorithms.",
        "name": "1",
        "region_bb": [
          147.6,
          84.96000000000001,
          461.52000000000004,
          159.12
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          128.16,
          172.8,
          482.40000000000003,
          181.44
        ],
        "page": 4,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3: Comparison of AC vs. RS-AC in the average setting using two different metrics.",
        "name": "3",
        "region_bb": [
          120.96000000000001,
          249.84,
          488.16,
          378.0
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          126.72,
          387.36,
          484.56,
          396.0
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "aaai12_1": {
    "figures": [
      {
        "caption": "Figure 2: (a) Local solution space vs. Influence Space (b) Full vs. Local Decomposability (c) Scalability of Local Decomposability",
        "name": "2",
        "region_bb": [
          54.0,
          54.0,
          558.0,
          150.48000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.72,
          161.28,
          557.28,
          169.92000000000002
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: The minimal STN distance graphs corresponding to two feasible labelings of the problem in Table 1.",
        "name": "1",
        "region_bb": [
          64.8,
          54.0,
          281.52,
          213.84
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.72,
          226.8,
          292.32,
          246.24
        ],
        "page": 4,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 1: Summary of the example logistics problem.",
        "name": "1",
        "region_bb": [
          63.36,
          54.0,
          548.64,
          130.32
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          200.88,
          144.72,
          408.96000000000004,
          153.36
        ],
        "page": 3,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7
    ]
  },
  "icml14_9": {
    "figures": [
      {
        "caption": "Figure 2. Local dimension estimation.",
        "name": "2",
        "region_bb": [
          433.44,
          66.96000000000001,
          521.28,
          162.0
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          403.92,
          182.88,
          540.0,
          190.8
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1. A geometry of statistical manifold learning.",
        "name": "1",
        "region_bb": [
          104.4,
          66.96000000000001,
          491.76,
          183.6
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          202.32,
          203.04000000000002,
          393.12,
          211.68
        ],
        "page": 3,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 4. Internal complexity of Y and Z and their gap.",
        "name": "4",
        "region_bb": [
          342.72,
          67.68,
          504.72,
          169.92000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          323.28000000000003,
          189.36,
          524.88,
          198.0
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3. Performance measurements of different embeddings. In each sub-figure, the color-map shows the local densities vol(a, b)d\u03c3(a, b) over \u2126. From left to right (resp. bottom to up), the input (resp. output) observation radius expands from 5 to 50. The x-axis (resp. y-axis) is linear in 1/a (resp. 1/b). The number below each square shows the volume, i.e. the integration of local densities in the corresponding color-map. Note, the colors are different between the two datasets.",
        "name": "3",
        "region_bb": [
          55.440000000000005,
          57.6,
          529.9200000000001,
          291.6
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.72,
          300.96000000000004,
          540.72,
          342.72
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 1. Estimated intrinsic dimension (avg. \u00b1 std.) for each k \u2208 {5, 10, . . . , 100}.",
        "name": "1",
        "region_bb": [
          57.6,
          79.2,
          392.40000000000003,
          151.92000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          73.44,
          182.88,
          375.84000000000003,
          191.52
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "nips11_1": {
    "figures": [
      {
        "caption": "Table 2: Results on the IAM database for polynomial feature spaces of degree m \u2208 {1, 2, 3} with",
        "name": "2",
        "region_bb": [
          115.2,
          113.76,
          496.08000000000004,
          221.76000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          91.44,
          503.28000000000003,
          111.60000000000001
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 1: Results on the USPS task for different feature transformations and regularization parameters \u03b1. The columns \u201cseparation\u201d and \u201ctermination\u201d list the number of passes through the dataset until",
        "name": "1",
        "region_bb": [
          141.12,
          123.84,
          468.0,
          222.48000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          91.44,
          503.28000000000003,
          122.4
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "nips11_10": {
    "figures": [
      {
        "caption": "Figure 2: Estimated time-varying coefficients on IRVINE data. These plots suggest that there are two distinct phases of network evolution, consistent with an independent analysis of these data [15].",
        "name": "2",
        "region_bb": [
          114.48,
          206.64000000000001,
          500.40000000000003,
          285.12
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.72,
          290.88,
          503.28000000000003,
          311.04
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: (a,b) Estimated time-varying coefficients on SIM-1; (c,d) Estimated time-varying coeffi- cients on SIM-2. Ground-truth coefficients are also shown in red dashed lines.",
        "name": "1",
        "region_bb": [
          113.76,
          82.08,
          498.96000000000004,
          169.92000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          175.68,
          503.28000000000003,
          193.68
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 4: Predictive performance of the additive Aalen model, multiplicative Cox model, and logistic regression baseline on the IRVINE and METAFILTER data sets, using recall as the metric.",
        "name": "4",
        "region_bb": [
          120.96000000000001,
          82.08,
          494.64000000000004,
          223.20000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          228.96,
          502.56,
          249.12
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3: Estimated time-varying coefficients on METAFILTER. Here, the network effects continu- ously change during the observation time.",
        "name": "3",
        "region_bb": [
          115.2,
          324.72,
          498.96000000000004,
          402.48
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.72,
          408.24,
          503.28000000000003,
          428.40000000000003
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 1: Lengths of building, training, and test periods. The number of events are in parentheses.",
        "name": "1",
        "region_bb": [
          115.2,
          106.56,
          496.8,
          138.96
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          113.04,
          91.44,
          497.52000000000004,
          100.8
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "icml12_8": {
    "figures": [
      {
        "caption": "Figure 1. The performance of the proposed algorithm.",
        "name": "1",
        "region_bb": [
          83.52,
          115.2,
          515.52,
          628.5600000000001
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          201.6,
          658.08,
          394.56,
          666.72
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8
    ]
  },
  "icml11_5": {
    "figures": [
      {
        "caption": "BCDNPKL : Scalable Non-Parametric Kernel Learning Using Block Coordinate Descent",
        "name": "5",
        "region_bb": [
          61.2,
          482.40000000000003,
          532.8000000000001,
          693.36
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          59.04,
          452.16,
          536.4,
          481.68
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "BCDNPKL : Scalable Non-Parametric Kernel Learning Using Block Coordinate Descent",
        "name": "4",
        "region_bb": [
          66.24000000000001,
          257.04,
          527.76,
          437.04
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          59.04,
          226.08,
          537.12,
          255.60000000000002
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 2. The third group data sets including seven Adult",
        "name": "2",
        "region_bb": [
          307.44,
          163.44,
          532.8000000000001,
          190.8
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          307.44,
          142.56,
          540.72,
          162.0
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 1. The second group data sets.",
        "name": "1",
        "region_bb": [
          307.44,
          78.48,
          536.4,
          132.48000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          349.92,
          69.12,
          498.24,
          77.76
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Figure 1. The comparison of convergent objective values between BCDNPKL and the baselines based on SDP solver on data sets: iris (C = 10) and soybean (C = 5).",
        "name": "1",
        "region_bb": [
          66.24000000000001,
          147.6,
          277.92,
          340.56
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          55.440000000000005,
          356.40000000000003,
          288.72,
          386.64
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "BCDNPKL : Scalable Non-Parametric Kernel Learning Using Block Coordinate Descent",
        "name": "3",
        "region_bb": [
          65.52,
          86.4,
          528.48,
          210.96
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          59.04,
          66.96000000000001,
          537.12,
          83.52
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8
    ]
  },
  "aaai12_6": {
    "figures": [
      {
        "caption": "Figure 2: The percent improvement in F-measure of our ap- proach against SNMF-SS (Ma et al. 2010) as the benchmark networks are perturbed by noise, averaged over 100 trials.",
        "name": "2",
        "region_bb": [
          82.8,
          60.480000000000004,
          258.48,
          338.40000000000003
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.0,
          349.2,
          291.6,
          380.16
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: The performance of semi-supervised community detection with various amounts of guidance as the relational net- works are perturbed by noise. The noise rate refers to the proportion of random vertex pairs that have an edge either added or deleted. Figures (a)\u2013(e) show the F-measure agreement to the target communities. Figure (f) shows the modularity of the discovered communities in the Political Sentiment network. The error bars, shown in blue, depict the standard error of the mean performances, which were averaged over 100 trials.",
        "name": "1",
        "region_bb": [
          59.760000000000005,
          57.6,
          553.6800000000001,
          310.32
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.0,
          318.96000000000004,
          558.0,
          371.52000000000004
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7
    ]
  },
  "icml12_3": {
    "figures": [
      {
        "caption": "Figure 2. Diffusion of ratings, shown as deltas from the item means, for an user that has rated only the shaded items. A hand chosen subgraph of the item graph gener-",
        "name": "2",
        "region_bb": [
          318.24,
          79.92,
          529.9200000000001,
          208.08
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          308.16,
          232.56,
          540.72,
          272.16
        ],
        "page": 3,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1. Neighbourhood of \u201cBlade Runner\u201d found using",
        "name": "1",
        "region_bb": [
          68.4,
          79.92,
          276.48,
          254.88
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          56.160000000000004,
          282.24,
          288.72,
          299.52000000000004
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3. Test error as a function of the number of training iterations on the 100K MovieLens dataset. A mild over- fitting effect is visible.",
        "name": "3",
        "region_bb": [
          309.6,
          95.76,
          522.0,
          244.8
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          308.16,
          262.8,
          540.72,
          293.04
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 1. Comparison of a selection of models against the item field model with approximate maximum likelihood, exact maximum likelihood and maximum entropy approaches",
        "name": "1",
        "region_bb": [
          94.32000000000001,
          104.4,
          498.96000000000004,
          336.24
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          56.160000000000004,
          76.32000000000001,
          540.72,
          95.76
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8
    ]
  },
  "aaai13_4": {
    "figures": [
      {
        "caption": "Figure 2: Comparison between user satisfaction levels among subjects who received SAMP\u2019s advice and those who received the fully rational agent\u2019s advice",
        "name": "2",
        "region_bb": [
          324.72,
          257.76,
          551.52,
          370.8
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          318.96000000000004,
          385.2,
          557.28,
          416.16
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: A visualization of the agent\u2019s advice",
        "name": "1",
        "region_bb": [
          342.72,
          59.04,
          536.4,
          210.24
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          347.04,
          237.6,
          530.64,
          246.24
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2
    ]
  },
  "icml12_4": {
    "figures": [
      {
        "caption": "Figure 2. The directed graphical model of a 3-level MLC- HDP. The observed value is given in gray, and priors are given as squares.",
        "name": "2",
        "region_bb": [
          351.36,
          66.96000000000001,
          497.52000000000004,
          200.16
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          307.44,
          213.84,
          540.72,
          244.08
        ],
        "page": 3,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 2. The number of recorded seizures for each patient and whether all the seizures of each patient contained the same number of active channels.",
        "name": "2",
        "region_bb": [
          81.36,
          305.28000000000003,
          259.92,
          425.52000000000004
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          55.440000000000005,
          264.24,
          288.72,
          293.04
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 1. Parameters for the true distributions p T = Comp 1 i Comp 2 2 Dist w \u00b5 \u03c3 T1 T2 T3 T4 .75 .55 .40 .39 0 0 0 0 1.0 1.0 1.0 1.0 \u00b5 \u03c3 .25 .45 .30 .29 3.0 3.0 -2.0 -2.0 2.0 2.0 2.0 2.0 Sampling level hyperparameters We sample each level\u2019s hyperparameters \u03b1 and \u03b3, which can have Gamma(a, b) priors. This step is performed after sampling the other level parameters. A single sampling iteration for the full MLC-HDP proceeds through sampling the atom indicators, the level parameters (and, optionally, also the hyperpa- rameters), and the base parameters. Finally, any non- last empty base- and level-atoms are removed (and the appropriate indicator variables decremented ac- cordingly). The Supplementary Materials give explicit algorithms corresponding to these steps 8 . 4. Experiments 4.1. Simulated data To explore some of the properties of the MLC-HDP in a controlled setting and to compare it with a similar model, we ran a 2-level version of it on the same sim- ulated data presented in (Rodr\u00b4\u0131guez et al. 2008) and implemented the Nested Dirichlet Process model de- scribed in their paper. Briefly, samples were generated from one of four distributions (T1-T4), each of which is a mixture of two to four Gaussians. The parameters of the Gaussians are given in Table 1. We used a dataset with 5 samples from each of the four distributions, for 20 samples total. Each sample contained 100 observations from the particular distri- bution. We used the same hyperparameters described in (Rodr\u00b4\u0131guez et al. 2008). Posterior inference for each model was run over 25 chains, each with a 5000 sample burn in and 10 sample thinning, gathering 400 samples for each chain or 10,000 samples total. We found that the MLC-HDP gives better estimates of the four true GMM distributions than the NDP, as See http://www.seas.upenn.edu/~wulsin for supple- mentary materials, code, and a link to the EEG dataset used. w i N (\u00b5 i , \u03c3 i 2 ) used in the simulation study",
        "name": "1",
        "region_bb": [
          138.96,
          99.36,
          454.32,
          175.68
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          102.96000000000001,
          76.32000000000001,
          493.92,
          87.12
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Figure 4. (left:) The mean log-perplexity of patient B\u2019s future seizures in each of the three models: M1, a standard DP with training data from previous seizures of the patient; M2, a standard DP with training data from previous seizures of the patient as well as all the seizures from all the other patients; M3, the MLC-HDP model with its full patient-seizure- channel hierarchy and clustering. (right:) The average seizure clustering similarity between the clusters found by a DP, an MLC-HDP, and human doctor to a second human doctor\u2019s clustering for each patient individually and for all patients together. Patient A and G are excluded because they only had one seizure. Standard error values are too small to show up on these plots.",
        "name": "4",
        "region_bb": [
          55.440000000000005,
          57.6,
          540.72,
          241.92000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.72,
          257.76,
          541.44,
          331.2
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1. A schematic of the epilepsy data over a popu- lation of patients (level 0), each of whom (level 1) has a variable number of seizures (level 2) and a variable number and unique placement of electrode channels (level 3). We assume that the data cluster at each level, as denoted by",
        "name": "1",
        "region_bb": [
          55.440000000000005,
          56.88,
          287.28000000000003,
          168.48000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          55.440000000000005,
          182.88,
          288.72,
          244.08
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3. True and average estimated density functions for the T1 (left) and T2 (middle) density functions. The mean Kullback-Leibler (KL) divergence in each of the four distributions (right) for the NDP and MLC-HDP. Error bars denote",
        "name": "3",
        "region_bb": [
          77.04,
          57.6,
          492.48,
          191.52
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          55.440000000000005,
          206.64000000000001,
          540.72,
          234.72
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8
    ]
  },
  "aaai10_10": {
    "figures": [
      {
        "caption": "Table 1: Breakdown of the quoted speech usage in six annotated texts. * indicates that excerpts were used.",
        "name": "1",
        "region_bb": [
          58.32,
          54.0,
          553.6800000000001,
          186.48000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          93.60000000000001,
          197.28,
          516.96,
          205.92000000000002
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 4: Performance of both category predictions and learning tools on the test set for each syntactic category.",
        "name": "4",
        "region_bb": [
          62.64,
          54.0,
          548.64,
          222.48000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          84.24000000000001,
          233.28,
          525.6,
          241.92000000000002
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 2: The most prevalent syntactic categories found in the development corpus.",
        "name": "2",
        "region_bb": [
          54.0,
          54.0,
          558.0,
          167.04
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          141.84,
          177.84,
          468.72,
          186.48000000000002
        ],
        "page": 4,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 3: Four samples of output that show the extracted character names and nominals (in bold).",
        "name": "3",
        "region_bb": [
          58.32,
          54.0,
          550.8000000000001,
          223.92000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          113.76,
          234.72,
          496.8,
          243.36
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7
    ]
  },
  "nips09_4": {
    "figures": [
      {
        "caption": "Figure 2: (a) Empirical ROC curve of K -LPE on the banana dataset with K = 2, 4, 6, 8, 10, 12 (with n = 400 ) vs the empirical ROC curve of one class SVM developed in [9]; (b) Empirical ROC curves of K -LPE algorithm vs clairvoyant ROC curve ( f 0 is given by Equation 10) for K = 6 and for n = 40 or 160 .",
        "name": "2",
        "region_bb": [
          126.02927268891703,
          85.69990542846358,
          479.6314034903928,
          237.65519992767213
        ],
        "page_height": 842,
        "page_width": 596,
        "caption_bb": [
          108.0250908762146,
          249.1778762878017,
          504.8372580281762,
          277.2643999156175
        ],
        "page": 8,
        "dpi": 72.01672725080974,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: Left : Level sets of the nominal bivariate Gaussian mixture distribution used to illustrate the K- LPE algorithm. Middle : Results of K-LPE with K = 6 and Euclidean distance metric for m = 150 test points drawn from a equal mixture of 2D uniform and the (nominal) bivariate distributions. Scores for the test points are based on 200 nominal training samples. Scores falling below a threshold level 0.05 are declared as anomalies. The dotted contour corresponds to the exact bivariate Gaussian density level set at level \u03b1 = 0.05 . Right : The empirical distribution of the test point scores associated with the bivariate Gaussian appear to be uniform while scores for the test points drawn from 2D uniform distribution cluster around zero.",
        "name": "1",
        "region_bb": [
          104.42425451367411,
          364.40463988909727,
          505.5574253006843,
          499.0759198481115
        ],
        "page_height": 842,
        "page_width": 596,
        "caption_bb": [
          107.3049236037065,
          513.4792652982734,
          504.11709075566813,
          581.8951561865426
        ],
        "page": 3,
        "dpi": 72.01672725080974,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3: ROC curves on real datasets via LPE ; (a) Wine dataset with D = 13, n = 39, \u00b2 = 0.9 ; (b)",
        "name": "3",
        "region_bb": [
          105.1444217861822,
          425.61885805228553,
          499.7960871206196,
          545.1666252886297
        ],
        "page_height": 842,
        "page_width": 596,
        "caption_bb": [
          108.0250908762146,
          552.3682980137106,
          503.39692348316004,
          570.3724798264132
        ],
        "page": 8,
        "dpi": 72.01672725080974,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "nips11_3": {
    "figures": [
      {
        "caption": "Figure 2: Shared weight artificial neural network architecture for pairwise ranking. The goal is to determine a productivity order between the (source, sink) A and (source, sink) B pairs. This is done with a pair of shared-weight artificial neural networks with sigmoidal hidden nodes and a linear output node. The output of these internal networks are tied to a single sigmoidal output node with fixed weights. The final output will approach 1 if the (source, sink) A pair is predicted to be relatively more productive than the (source, sink) B pair, and 0 otherwise.",
        "name": "2",
        "region_bb": [
          170.64000000000001,
          222.48000000000002,
          441.36,
          372.24
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          107.28,
          387.36,
          504.0,
          451.44
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: Overall transformation of an alkene (hydrocarbon with double bond) with hydro- bromic acid (HBr) and corresponding mechanistic reactions. (a) shows the overall transform as a SMIRKS[14] string pattern and as a graph representation. In a molecular graph, vertices represent atoms, with carbons at unlabeled vertices. The number of edges between two vertices represents bond order. +/\u2212 symbols represent formal charge. Standard valences are filled using implicit hydrogens. (b) shows the two mechanistic reactions composing the overall transformation as arrow- pushing diagrams[15, 16]. Dots represent non-bonded (lone pair) electrons, while arrows represent concerted electron movement. In the first step, electrons in the electron-rich carbon-carbon double bond attack the hydrogen and break the electron-poor hydrogen-bromine single bond, producing an anionic bromide (Br\u2212) and a carbocation (C+). In the second step, electrons from the charged, electron-rich bromide attack the electron-poor carbocation, yielding the final alkyl halide.",
        "name": "1",
        "region_bb": [
          157.68,
          332.64,
          453.6,
          493.20000000000005
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          107.28,
          507.6,
          504.0,
          626.4
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 3: Chemical reactions of interest. The first row shows an example of full multi-step reaction prediction by the ranking system, a three step intramolecular Claisen condensation (room temp., polar aprotic). At each stage, the reaction shown is the top ranked when all possible reactions are considered by the two stage machine learning system. The second row shows two macrocyclizations which the rule-based system (Reaction Explorer) is unable to predict, but the machine learning approach effectively generalizes and ranks correctly. These reactions lead to the formation of a seven homo-cycle (7 carbons) on the left and seven hetero-cycle (6 carbons, 1 oxygen) on the right. The third row shows an intelligible error of the machine learning approach (see text).",
        "name": "3",
        "region_bb": [
          107.28,
          522.72,
          504.72,
          692.64
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          430.56,
          504.0,
          516.96
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 2: Reaction ranking results. We show Normalized Discounted Cumulative Gain at different list sizes i (NDCG@i) and Percent Within-n. See text for description of the measures. We report mean (standard deviation) results over CV folds.",
        "name": "2",
        "region_bb": [
          185.04000000000002,
          228.24,
          426.24,
          307.44
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          107.28,
          198.72,
          503.28000000000003,
          228.96
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 1: Reactive site classification results. Source reactive and sink reactive rows show results on the respective classification problems. The reaction row shows results of using the two atom classifiers for an initial reaction filtering. CV columns indicate results of full 10-fold cross-validation over (m, c) tuples. CV results show the mean and standard deviation over folds. The best TNR",
        "name": "1",
        "region_bb": [
          154.08,
          518.4,
          457.92,
          567.36
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          461.52000000000004,
          503.28000000000003,
          519.12
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "aaai12_4": {
    "figures": [
      {
        "caption": "Table 1: Summary of our results. For each parameter combination the entries indicate whether L OBBYING is fixed-parameter tractable (FPT), FPT based on a formulation as integer linear program (ILP-FPT), W[2]-hard, W[2]-complete (meaning that it is W[2]-hard and contained in W[2] \u2286XP), or NP-hard even for a constant parameter value (para-NP-hard). Entries on the main diagonal represent results for the single parameters. Furthermore, for each of the parameter combinations above, under some reasonable complexity-theoretic assumptions (see Section 3), there cannot be a polynomial-size problem kernel.",
        "name": "1",
        "region_bb": [
          54.0,
          54.0,
          494.64000000000004,
          136.8
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          53.28,
          149.76000000000002,
          558.0,
          202.32
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7
    ]
  },
  "nips09_1": {
    "figures": [
      {
        "caption": "Figure 2: Example inferred latent properties associated with MSRC dataset. Left: Posterior distribution on the mixture-weights u, quantifying the probability of scene classes (10 classes are inferred). Middle and Right: Example probability of objects for a given class, w i (probability of object/words); here we only give the top 5 words for each class.",
        "name": "2",
        "region_bb": [
          125.28,
          172.8,
          475.2,
          234.72
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          107.28,
          241.92000000000002,
          502.56,
          278.64
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: Depiction of the generative process. (i) A scene-class indicator zm \u2208 {1, . . . , I} is drawn to define the image class; (ii) conditioned on zm , and using the LSBP, contiguous segmented blocks are constituted, with associated words defined by object indicator cml \u2208 {1, \u00b7 \u00b7 \u00b7 , K}, where w i defines the probability of observing each object type for image class i; (iii) conditioned on cml , image-feature atoms are drawn from appropriate mixture models Gcml , linked to over-segmented regions within each of the object clusters; (iv) the image-feature model parameters are responsible for generating the image features, via the model F (\u03b8), where \u03b8 is the image-feature parameter.",
        "name": "1",
        "region_bb": [
          164.16,
          125.28,
          441.36,
          260.64
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          263.52,
          504.0,
          332.64
        ],
        "page": 4,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 4: Example segmentation and labeling results. First row: original images; second row: Corr-LDA [6]; third row: proposed model without LSBP; fourth row: proposed model with LSBP. Columns 1-3 from MSRC dataset; Columns 4-6 from UIUC-Sport dataset. The name of original images are inferred by scene-level classification via our model. The UIUC-Sport results are based on the words inferred by our model.",
        "name": "4",
        "region_bb": [
          115.92,
          181.44,
          505.44,
          398.16
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          401.76,
          503.28000000000003,
          439.92
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3: Comparisons using confusion matrices for all images in each dataset (all of the annotated and non- annotated images in MSRC; all the non-annotated images in UIUC-Sport). The left two results are for MSRC, and the right two for UIUC-Sport. In each pair, the result is without LSBP, and the right is with LSBP. Average performance, left to right: 82.90%, 86.80%, 60.50% and 63.50%.",
        "name": "3",
        "region_bb": [
          105.84,
          281.52,
          509.76,
          355.68
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          107.28,
          362.88,
          503.28000000000003,
          401.04
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 1: Comparison of precision and recall values for annotation and segmentation with Corr-LDA [6], our model without LSBP (Simp. Model) and the extended models with KSBP (Ext. with KSBP) and LSBP (Ext. with LSBP) on MSRC datasets. To evaluate annotation performance, the results are just calculated based on non-annotated images; while for segmentation, the results are based on all images.",
        "name": "1",
        "region_bb": [
          107.28,
          530.64,
          514.08,
          636.48
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          107.28,
          491.04,
          503.28000000000003,
          528.48
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "icml11_8": {
    "figures": [
      {
        "caption": "Figure 1. The posterior distribution P ( w|Z) ~ is often non-smooth and non-Gaussian. Both MAP and Gaussian approxi- mations can lead to errors when estimating the Bayes Point. a) A plot of the P ( w|Z) ~ (Eq. 5) for the simple Gaussian training data shown in subfigure c. Each point in this plot corresponds to a different decision boundary (described by its angle \u03b8 and offset \u03c1). The margin width | w| ~ is fixed at the width that minimizes error. The green dot represents the ground-truth optimal decision boundary for this data. The red x shows the maximum a posteriori (MAP) estimate over P ( w|Z). ~ The blue line shows the decision boundary chosen by our knapsack-based algorithm, using values of \u03b2 ranging from 5 to 50. b) A plot of the ground-truth test-set performance of each decision boundary, computed using the data\u2019s known distribu- tion. Here, theoretical P ( w|Z) ~ is Gaussian only because Z is Gaussian; non-Gaussian data leads to non-Gaussian P ( w|Z). ~ c) Simple toy training data. 50 points each are generated from unit-variance Gaussians centered at (1, 1) and (\u22121, \u22121). Ground-truth optimal (solid) and MAP decision boundaries (dashed) are drawn in. d) A factor graph for inferring the optimal decision hyperplane given four sample points of two-dimensional data. Variable node \u03c1 corresponds to affine bias (\u03c8 0 \u2261 1). e-g) Three example max-marginals from the Heart and Liver UCI datasets. Often, marginals are sufficiently skewed so that a Gaussian approximation is not accurate, and the MAP estimate (red x) does not lie near the mean (blue line, \u03b2 = 1). In other cases, the marginal is flat or even bimodal near its peak, which may also place the MAP far from the mean. These examples illustrate the potential advantages of estimating the mean without making a Gaussian approximation.",
        "name": "1",
        "region_bb": [
          55.440000000000005,
          74.16,
          542.16,
          512.64
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.72,
          528.48,
          542.16,
          712.8000000000001
        ],
        "page": 3,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 1. Error rates for linear classification over test sets \u00b1 the standard deviation of the mean over 500 test/training splits for soft-margin (hinge loss) linear SVMs, Expectation Propagation (source code from (Minka, 2001)), and our knapsack programming approach (Knap). Entries in bold show statistically significant improvement over the other two methods (p < 0.01). The Size row lists the number of samples S and the number of features D, respectively.",
        "name": "1",
        "region_bb": [
          55.440000000000005,
          116.64,
          537.84,
          187.92000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          55.440000000000005,
          77.04,
          541.44,
          118.08
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8
    ]
  },
  "aaai11_6": {
    "figures": [
      {
        "caption": "Figure 2: A simpli\ufb01ed version of the ARA* algorithm.",
        "name": "2",
        "region_bb": [
          53.28,
          54.0,
          281.52,
          277.2
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          64.08,
          285.84000000000003,
          281.52,
          295.2
        ],
        "page": 4,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 5: Experimental results of the 5000x5000 grid-world path-planning experiments with random cost.",
        "name": "5",
        "region_bb": [
          353.52000000000004,
          59.760000000000005,
          521.28,
          360.72
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          318.96000000000004,
          370.8,
          558.0,
          391.68
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 4: Experimental results of the 100x1200 grid-world path-planning experiment with obstacles and uniform cost.",
        "name": "4",
        "region_bb": [
          92.16,
          57.6,
          248.4,
          190.8
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.0,
          201.6,
          292.32,
          221.76000000000002
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: The Anytime Nonparametric A* algorithm.",
        "name": "1",
        "region_bb": [
          53.28,
          54.0,
          280.8,
          287.28000000000003
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          66.24000000000001,
          295.2,
          278.64,
          304.56
        ],
        "page": 3,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 6: Results for the multiple sequence alignment prob- lem with \ufb01ve protein sequences.",
        "name": "6",
        "region_bb": [
          92.16,
          57.6,
          246.24,
          192.24
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.0,
          202.32,
          291.6,
          222.48000000000002
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3: Experimental results of the robot motion planning problem; illustrating performance over time for 6 DOF and",
        "name": "3",
        "region_bb": [
          351.36,
          58.32,
          524.88,
          661.6800000000001
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          318.96000000000004,
          672.48,
          558.0,
          701.28
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7
    ]
  },
  "aaai13_9": {
    "figures": [
      {
        "caption": "Figure 2: Message passing scheme",
        "name": "2",
        "region_bb": [
          329.04,
          56.88,
          550.08,
          141.12
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          370.08000000000004,
          159.84,
          507.6,
          168.48000000000002
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: Radar data from KBGM station in Binghamton, NY during a heavy period of bird migration on 9/11/20100. Top row: (a) reflectivity data, (b) correctly dealiased radial velocity data, (c) original radial velocity data with clear aliasing. Bottom row: (d) radial velocity vs. azimuth at fixed range of 25 km (charcoal circle in panel (c)), together with predictions from models fitted by our method (EP) and GVAD, (e) the GVAD response variable \u2206 \u03b3 (\u03c6) for \u03b3 = 0.2, together with the fitted model predictions.",
        "name": "1",
        "region_bb": [
          61.2,
          60.480000000000004,
          531.36,
          322.56
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.0,
          335.52000000000004,
          558.0,
          378.0
        ],
        "page": 3,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3: Performance comparison on test scans.",
        "name": "3",
        "region_bb": [
          318.96000000000004,
          57.6,
          550.08,
          149.04
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          342.0,
          164.88,
          534.24,
          173.52
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7
    ]
  },
  "nips12_9": {
    "figures": [
      {
        "caption": "Figure 2: Max-pooling vs. LWTA. (a) In max-pooling, each group of neurons in a layer has a single set of output weights that transmits the winning unit\u2019s activation (0.8 in this case) to the next layer, i.e. the layer activations are subsampled. (b) In an LWTA block, there is no subsampling. The activations flow into subsequent units via a different set of connections depending on the winning unit.",
        "name": "2",
        "region_bb": [
          110.16000000000001,
          82.08,
          501.84000000000003,
          221.04000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.72,
          232.56,
          504.0,
          285.12
        ],
        "page": 4,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 4: LWTA networks outperform sigmoid and ReLU activation in remembering dataset P1 after training on dataset P2.",
        "name": "4",
        "region_bb": [
          141.84,
          113.76,
          466.56,
          148.32
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.72,
          82.08,
          503.28000000000003,
          102.24000000000001
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 2: Test set errors on the permutation invariant MNIST dataset for methods without data augmentation or unsupervised pre-training",
        "name": "2",
        "region_bb": [
          186.48000000000002,
          113.76,
          421.92,
          169.92000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.72,
          82.08,
          503.28000000000003,
          102.24000000000001
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 1: Comparison of rectified linear activation and LWTA-2.",
        "name": "1",
        "region_bb": [
          177.84,
          102.96000000000001,
          431.28000000000003,
          215.28
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          167.04,
          82.08,
          444.24,
          91.44
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Figure 1: A Local Winner-Take-All (LWTA) network with blocks of size two showing the winning neuron in each block (shaded) for a given input example. Activations flow forward only through the winning neurons, errors are backpropagated through the active neurons. Greyed out connections do not propagate activations. The active neurons form a subnetwork of the full network which changes depending on the inputs.",
        "name": "1",
        "region_bb": [
          175.68,
          82.08,
          436.32,
          270.72
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          280.8,
          503.28000000000003,
          334.08
        ],
        "page": 3,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3: (a) Each entry in the matrix denotes the fraction of neurons that a pair of MNIST digits has in common, on average, in the subnetworks that are most active for each of the two digit classes. (b) The fraction of neurons in common in the subnetworks of each of the 55 possible digit pairs, before and after training.",
        "name": "3",
        "region_bb": [
          112.32000000000001,
          82.08,
          500.40000000000003,
          234.72
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.72,
          246.24,
          503.28000000000003,
          288.0
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 3: Test set errors on MNIST dataset for convolutional architectures with no data augmentation. Results marked with an asterisk use layer-wise unsupervised feature learning to pre-train the network and global fine tuning.",
        "name": "3",
        "region_bb": [
          172.08,
          230.4,
          436.32,
          308.88
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          187.92000000000002,
          504.0,
          218.88
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "aaai12_5": {
    "figures": [
      {
        "caption": "Figure 2: A trace of Alg. 1 for the case where |S| = 2. At each step (from top to bottom), configurations are an- notated with their expected likelihood scores in bold. The witness set, min-witness, and the generated sets are shown in columns on the right of the search graphs.",
        "name": "2",
        "region_bb": [
          59.760000000000005,
          54.72,
          287.28000000000003,
          232.56
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.0,
          246.24,
          292.32,
          299.52000000000004
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: A trace of Alg. 1 for the case where there is only one size variable, i.e. S = {S}. The witness set, min- witness, and the generated configurations are listed for every step of the search.",
        "name": "1",
        "region_bb": [
          370.08000000000004,
          303.84000000000003,
          506.88,
          362.16
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          318.96000000000004,
          376.56,
          557.28,
          418.32
        ],
        "page": 4,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 4: Bounds on log p(K | D train , \u03b8) (Lem. 2), where K is the number of LDA topics. Different Poisson priors over K are used, corresponding to Poisson parameters \u03bb = 5, 10, 40. The values of K in this experiment range from 1 to 150, but this figure only shows up to K = 100 topics.",
        "name": "4",
        "region_bb": [
          319.68,
          54.0,
          557.28,
          231.84
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          318.96000000000004,
          244.8,
          557.28,
          298.08
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3: Bounds on log p(D train | \u03b8) (Lem. 1), evaluated over K = 1, . . . 150 LDA topics, using a Poisson prior over K with parameters \u03bb = 5, 10, 40.",
        "name": "3",
        "region_bb": [
          54.72,
          54.0,
          292.32,
          227.52
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.0,
          240.48000000000002,
          292.32,
          271.44
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 5: Comparison of Bounds on log p(D test | D train , \u03b8), shown against p(D test | D train , k \u2217 , \u03b8 k \u2217 ) where K is the number of LDA topics, and where k \u2217 is the num- ber of topics for the best LDA model. The sub-plots corre- spond to different Poisson priors over K, for parameter val- ues \u03bb = 5, 10, 40. Higher log-likelihood indicates greater accuracy.",
        "name": "5",
        "region_bb": [
          54.72,
          54.0,
          292.32,
          237.6
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.72,
          250.56,
          292.32,
          326.16
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7
    ]
  },
  "aaai14_1": {
    "figures": [
      {
        "caption": "Figure 1: Cyclic dependencies of persistent condi-",
        "name": "1",
        "region_bb": [
          54.72,
          53.28,
          292.32,
          157.68
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.72,
          164.88,
          291.6,
          182.16
        ],
        "page": 4,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 1: Results on FF domains. Node expansions (# Exp), (generated) actions (# Act) and search time in seconds.",
        "name": "1",
        "region_bb": [
          321.12,
          54.0,
          556.5600000000001,
          177.12
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          318.96000000000004,
          183.6,
          557.28,
          203.04000000000002
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 4: Object transportation task with need for co- operation.",
        "name": "4",
        "region_bb": [
          331.92,
          205.20000000000002,
          543.6,
          311.04
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          318.96000000000004,
          317.52000000000004,
          557.28,
          336.96000000000004
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 2: Results for the object transportation task.",
        "name": "2",
        "region_bb": [
          67.68,
          54.0,
          279.36,
          146.16
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          73.44,
          152.64000000000001,
          270.72,
          161.28
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 3: Result set 2 for object transportation task.",
        "name": "3",
        "region_bb": [
          333.36,
          54.0,
          544.32,
          165.6
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          337.68,
          172.08,
          537.84,
          180.72
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7
    ]
  },
  "aaai13_5": {
    "figures": [
      {
        "caption": "Figure 2: 2\u2013agent game (left) and its Pareto frontier (right).",
        "name": "2",
        "region_bb": [
          321.12,
          144.0,
          546.48,
          227.52
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          320.40000000000003,
          238.32000000000002,
          556.5600000000001,
          247.68
        ],
        "page": 4,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 5: 2\u2013agent game (left) and its Pareto frontier (right).",
        "name": "5",
        "region_bb": [
          340.56,
          527.76,
          532.8000000000001,
          614.88
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          320.40000000000003,
          626.4,
          556.5600000000001,
          635.76
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 2: Performance to find an SNE (or state that none ex-",
        "name": "2",
        "region_bb": [
          61.2,
          352.08000000000004,
          283.68,
          414.0
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.0,
          424.08000000000004,
          291.6,
          444.24
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 1: SNOPT performance in finding an NE.",
        "name": "1",
        "region_bb": [
          59.04,
          132.48000000000002,
          285.84000000000003,
          186.48000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          77.76,
          196.56,
          267.84000000000003,
          205.92000000000002
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Figure 4: 2\u2013agent game (left) and its Pareto frontier (right).",
        "name": "4",
        "region_bb": [
          334.08,
          200.16,
          545.04,
          298.08
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          320.40000000000003,
          309.6,
          556.5600000000001,
          318.96000000000004
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: 2\u2013agent game (left) and its Pareto frontier (right).",
        "name": "1",
        "region_bb": [
          334.08,
          426.96000000000004,
          545.76,
          526.32
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          320.40000000000003,
          537.84,
          556.5600000000001,
          547.2
        ],
        "page": 3,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 6: 2\u2013agent game (left) and its Pareto frontier (right).",
        "name": "6",
        "region_bb": [
          65.52,
          54.72,
          271.44,
          141.84
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.72,
          153.36,
          290.88,
          162.72
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3: Example game (prisoner\u2019s dilemma) without any SNE (left) and Pareto frontier (right).",
        "name": "3",
        "region_bb": [
          344.16,
          411.12,
          535.6800000000001,
          504.0
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          318.96000000000004,
          515.52,
          558.0,
          536.4
        ],
        "page": 4,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8
    ]
  },
  "aaai10_2": {
    "figures": [
      {
        "caption": "Figure 2: An illustration of bird filtering. The motion se- quence of the bird can be used to extract a set of moving line segments that correspond to the body axis of the bird. The segmentation error of the end of body axis is uniformed distributed in the u-v image plane and can be represented as an inverse pyramid when the error range is back-projected from the camera center to the 3D space.",
        "name": "2",
        "region_bb": [
          113.04,
          55.440000000000005,
          238.32000000000002,
          173.52
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          53.28,
          194.4,
          292.32,
          270.0
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: An example of a short video sequence of a flying bird that is captured in Bayou DeView in eastern Arkansas. It superimposes the segmented bird images from consecutive video frames on the top of the background frame.",
        "name": "1",
        "region_bb": [
          348.48,
          216.0,
          528.48,
          350.64
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          318.96000000000004,
          364.32,
          558.0,
          406.8
        ],
        "page": 1,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 4: (a) Physical experiment for rock pigeons. (b) The ROC curves for both the simulation and the physical exper- iment. The corresponding areas under the ROC curves are 91.5% and 95.0%, respectively.",
        "name": "4",
        "region_bb": [
          329.76,
          59.04,
          542.88,
          154.08
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          318.96000000000004,
          159.84,
          558.0,
          202.32
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3: (a) Convergence for different EKF configurations based on simulated rock pigeon data. (b) False positive (FP) and false negative (FN) rates with different \u03b4 in simulation.",
        "name": "3",
        "region_bb": [
          324.0,
          325.44,
          545.04,
          425.52000000000004
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          318.96000000000004,
          431.28000000000003,
          558.0,
          462.24
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 1: Species used in the experiments",
        "name": "1",
        "region_bb": [
          346.32,
          72.0,
          531.36,
          118.8
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          357.12,
          63.36,
          519.84,
          71.28
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6
    ]
  },
  "icml11_2": {
    "figures": [],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8
    ]
  },
  "icml12_6": {
    "figures": [
      {
        "caption": "Figure 2. Comparing CBP with Balaton and FeedExp on the easy game",
        "name": "2",
        "region_bb": [
          131.04,
          72.72,
          465.12,
          190.8
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          149.04,
          206.64000000000001,
          447.84000000000003,
          215.28
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1. An example of cell decomposition (M = 3).",
        "name": "1",
        "region_bb": [
          67.68,
          72.0,
          280.8,
          185.76000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          64.8,
          202.32,
          278.64,
          210.96
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 4. Comparing CBP and FeedExp on \u201charsh\u201d setting of the Dynamic Pricing game.",
        "name": "4",
        "region_bb": [
          131.04,
          393.12,
          465.12,
          511.20000000000005
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          115.92,
          527.04,
          480.24,
          535.6800000000001
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3. Comparing CBP and FeedExp on \u201cbenign\u201d setting of the Dynamic Pricing game.",
        "name": "3",
        "region_bb": [
          131.04,
          232.56,
          465.12,
          351.36
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          113.04,
          367.2,
          482.40000000000003,
          375.84000000000003
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 1. List of basic symbols",
        "name": "1",
        "region_bb": [
          114.48,
          85.68,
          482.40000000000003,
          300.24
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          238.32000000000002,
          76.32000000000001,
          357.84000000000003,
          82.8
        ],
        "page": 4,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8
    ]
  },
  "aaai11_10": {
    "figures": [
      {
        "caption": "Figure 2: The interface",
        "name": "2",
        "region_bb": [
          61.2,
          64.8,
          154.08,
          130.32
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          61.92,
          139.68,
          152.64000000000001,
          148.32
        ],
        "page": 4,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 5: Differences between humans and machines (aggre- gated over CS and CI). (a) The \ufb01rst unlabeled items (black dots) chosen by the \ufb01rst-view partners. (b) Same, but for the second-view. (c) Per-item average labels.",
        "name": "5",
        "region_bb": [
          325.44,
          53.28,
          551.52,
          127.44
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          318.96000000000004,
          140.4,
          558.0,
          182.16
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 1: The fraction of patterns in cluster-level majority classi\ufb01cation. \u201cOther\u201d includes the remaining 16 \u2212 4 = 12 possible patterns. Boldface indicates the largest fraction",
        "name": "1",
        "region_bb": [
          71.28,
          105.12,
          275.04,
          213.84
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.0,
          63.36,
          292.32,
          106.56
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Figure 4: In Experiment 1 each participant worked with only one view of the dataset. There were four labeled items. Points dithered to show overlap.",
        "name": "4",
        "region_bb": [
          369.36,
          54.0,
          506.88,
          127.44
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          318.96000000000004,
          139.68,
          558.0,
          171.36
        ],
        "page": 4,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: On this \u201cdiamond\u201d dataset, supervised learning and Co-Training, both with 1NN classi\ufb01ers, produce dras- tically different outcomes.",
        "name": "1",
        "region_bb": [
          335.52000000000004,
          383.04,
          541.44,
          455.04
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          318.96000000000004,
          467.28000000000003,
          558.0,
          498.24
        ],
        "page": 1,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 6: The counter-example",
        "name": "6",
        "region_bb": [
          82.08,
          53.28,
          262.8,
          128.16
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          110.88000000000001,
          140.4,
          234.72,
          149.76000000000002
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3: Sample stimuli",
        "name": "3",
        "region_bb": [
          169.20000000000002,
          56.88,
          281.52,
          128.16
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          167.76000000000002,
          139.68,
          265.68,
          149.04
        ],
        "page": 4,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6
    ]
  },
  "nips12_4": {
    "figures": [
      {
        "caption": "Figure 2: A summary of 10 times 10-fold cross-validation selection of regularizer index i into \u0398 for different privacy levels \u03b1. Each point in the figure represents a summary of 100 data points. The error bars indiciate a boot-strap sample estimate of the 95% confidence interval of the mean. A small amount of jitter was added to positions on the x-axes to avoid over-plotting.",
        "name": "2",
        "region_bb": [
          118.8,
          225.36,
          493.20000000000005,
          454.32
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          479.52000000000004,
          503.28000000000003,
          522.0
        ],
        "page": 19,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: A summary of 10 times 10-fold cross-validation experiments for different privacy levels \u03b1. Each point in the figure represents a summary of 100 data points. The error bars indiciate a boot-strap sample estimate of the 95% confidence interval of the mean. A small amount of jitter was added to positions on the x-axes to avoid over-plotting.",
        "name": "1",
        "region_bb": [
          113.04,
          245.52,
          498.96000000000004,
          381.6
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.72,
          393.84000000000003,
          503.28000000000003,
          435.6
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9,
      10,
      11,
      12,
      13,
      14,
      15,
      16,
      17,
      18,
      19
    ]
  },
  "nips13_9": {
    "figures": [
      {
        "caption": "Figure 2: For each synthetic and real dataset: (top) histogram of data overlaid with actual Gaussian mixture generating the synthetic data, and posterior mean mixture model for (middle) IID and (bottom) DPP. Red dashed lines indicate resulting density estimate.",
        "name": "2",
        "region_bb": [
          117.36,
          80.64,
          480.24,
          244.8
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          249.84,
          503.28000000000003,
          278.64
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: Estimates of total variational distance for Nystr\u00a8om and RFF approximation methods to a DPP with Gaussian quality and similarity with covariances \u0393 = diag(\u03c1 2 , . . . , \u03c1 2 ) and \u03a3 = diag(\u03c3 2 , . . . , \u03c3 2 ), respectively. (a)-(c) For dimensions d=1, 5 and 10, each plot considers \u03c1 2 = 1 and varies \u03c3 2 . (d) Eigenvalues for the Gaussian kernels with \u03c3 2 = \u03c1 2 = 1 and varying dimension d.",
        "name": "1",
        "region_bb": [
          124.56,
          84.24000000000001,
          478.08000000000004,
          151.20000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          156.96,
          504.72,
          195.12
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 2: For IID and DPP, mean (stdev) of (left) mixture membership entropy and held-out log-likelihood for",
        "name": "2",
        "region_bb": [
          102.96000000000001,
          109.44,
          495.36,
          166.32
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          107.28,
          91.44,
          504.0,
          109.44
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Figure 3: Left: Diverse set of human poses relative to an original pose by sampling from an RFF (top) and Nystr\u00a8om (bottom) approximations with kernel based on MoCap of the activity dance. Right: Fraction of data having a DPP/i.i.d. sample within an   neighborhood.",
        "name": "3",
        "region_bb": [
          123.84,
          172.08,
          483.12,
          257.76
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.72,
          264.24,
          504.0,
          292.32
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 1: For IID and DPP on synthetic datasets: mean (stdev) for mixture membership entropy, cluster assignment error rate and held-out log-likelihood of 100 observations under the posterior mean density estimate.",
        "name": "1",
        "region_bb": [
          108.0,
          323.28000000000003,
          509.04,
          365.76
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          107.28,
          288.72,
          504.72,
          307.44
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "aaai12_2": {
    "figures": [],
    "pages_annotated": [
      1,
      2
    ]
  },
  "icml13_9": {
    "figures": [
      {
        "caption": "Figure 2. Geometry of ASE. Analogy constraints for the semantic category embedding: The analogy quadruplet (p, q, r, s) forms a parallelogram in the semantic embedding space, cf. eq. (1). Data embedding W : At the same time, when projected onto the semantic space by W , the data point xi from class q should be closer to its semantic category embedding uq , compared to any other category embedding, by a large margin (see dotted circles).",
        "name": "2",
        "region_bb": [
          340.56,
          68.4,
          509.04,
          193.68
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          306.72,
          198.72,
          541.44,
          284.40000000000003
        ],
        "page": 4,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 5. AWA-50 categories projected to 2D using ASE- C. We show only three analogies for ease of viewing: 1) dalmatian:siamese cat=killer whale:dolphin, 2) lion:chihuahua = humpbackwhale:bluewhale, and 3) chimp:gorilla = persian cat:walrus.",
        "name": "5",
        "region_bb": [
          111.60000000000001,
          226.8,
          235.44,
          321.84000000000003
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.72,
          327.6,
          288.72,
          378.0
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 3. Sample analogy completion results",
        "name": "3",
        "region_bb": [
          309.6,
          168.48000000000002,
          546.48,
          249.84
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          332.64,
          248.4,
          522.72,
          257.04
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 2. Top-k class prediction accuracy, given an analogy with an unknown class in the form p:q=r:?",
        "name": "2",
        "region_bb": [
          310.32,
          67.68,
          545.76,
          144.0
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          306.72,
          143.28,
          545.76,
          163.44
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 1. Multiclass classification accuracy. The numbers denote mean and the standard error over 5 runs.",
        "name": "1",
        "region_bb": [
          310.32,
          96.48,
          548.64,
          174.96
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          306.72,
          66.24000000000001,
          541.44,
          84.24000000000001
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Figure 4. Confusion reduction. Left: CLME \u2212 CASE-A , Right: CLME \u2212 CASE-C . The numbers and colors at each entry show the reduction in confusion (red:\u2191, blue:\u2193). Out- lined entries are pairs that appear in the training analo- gies. Positive off-diagonal entries indicate reduced confu- sion. ASE-C focuses on initially confused classes.",
        "name": "4",
        "region_bb": [
          56.88,
          66.96000000000001,
          287.28000000000003,
          154.8
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.72,
          160.56,
          288.72,
          222.48000000000002
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1. We introduce analogical parallelogram con- straints to regularize a semantic embedding. By learning from both labeled instances and analogies, our method pre- serves structural similarities between category pairs.",
        "name": "1",
        "region_bb": [
          346.32,
          61.2,
          498.96000000000004,
          185.76000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          306.72,
          192.96,
          540.72,
          234.72
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3. Example analogies discovered from attributes.",
        "name": "3",
        "region_bb": [
          73.44,
          66.96000000000001,
          282.96000000000004,
          256.32
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          59.04,
          261.36,
          284.40000000000003,
          270.0
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "nips10_3": {
    "figures": [
      {
        "caption": "Figure 2: Denoising results: Gaussian (left) and Bimodal (right) noise.",
        "name": "2",
        "region_bb": [
          123.12,
          64.8,
          488.88,
          159.12
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          164.16,
          163.44,
          447.12,
          172.08
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: Gaussian and bimodal noise: Comparison of our approach to loopy belief propaga- tion and mean field approximations when optimizing using BFGS, SGD and SMD. Note that our approach significantly outperforms all the baselines. MF-SMD did not work for Bimodal noise.",
        "name": "1",
        "region_bb": [
          118.8,
          64.8,
          493.20000000000005,
          160.56
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.72,
          159.12,
          504.0,
          195.84
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3: Convergence. Primal and dual train errors for I 1 .",
        "name": "3",
        "region_bb": [
          185.76000000000002,
          61.2,
          425.52000000000004,
          157.68
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          186.48000000000002,
          156.24,
          421.92,
          167.04
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "aaai11_1": {
    "figures": [
      {
        "caption": "Figure 2: The \ufb01rst-order derivatives of Log, MCP, SCAD and ETP",
        "name": "2",
        "region_bb": [
          68.4,
          335.52000000000004,
          530.64,
          442.08000000000004
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          173.52,
          453.6,
          438.48,
          462.96000000000004
        ],
        "page": 3,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 5: Sparsity on the eight datasets (%)",
        "name": "5",
        "region_bb": [
          346.32,
          84.96000000000001,
          528.48,
          175.68
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          354.24,
          63.36,
          522.0,
          72.72
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 4: Classi\ufb01cation accuracies on the eight datasets (%)",
        "name": "4",
        "region_bb": [
          81.36,
          383.76,
          262.8,
          475.2
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          56.160000000000004,
          362.88,
          289.44,
          372.24
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 2: Simulation results from the linear regression meth- ods with MCP, SCAD and ETP, respectively. Here \u201cC\u201d is for",
        "name": "2",
        "region_bb": [
          329.04,
          104.4,
          545.76,
          276.48
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          318.96000000000004,
          63.36,
          558.0,
          92.16
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 1: The log-penalty, MCP and SCAD (P \u03bb,\u03b3 (|\u03b8|)) as well as their \ufb01rst-order derivatives (P \u03bb,\u03b3 (|\u03b8|)).",
        "name": "1",
        "region_bb": [
          76.32000000000001,
          87.12,
          533.52,
          170.64000000000001
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          98.64,
          63.36,
          511.92,
          75.60000000000001
        ],
        "page": 3,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Figure 1: Penalty functions: MCP, SCAD and ETP.",
        "name": "1",
        "region_bb": [
          66.96000000000001,
          200.88,
          530.64,
          307.44
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          203.76000000000002,
          319.68,
          406.8,
          329.04
        ],
        "page": 3,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 6: Computational times on the eight datasets (s).",
        "name": "6",
        "region_bb": [
          344.16,
          216.0,
          530.64,
          307.44
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          329.76,
          194.4,
          546.48,
          203.76000000000002
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Figure 3: Box-and-whisker plots of regression errors, which was conducted on the data from 1000 independent runs us- ing MCP, SCAD and ETP.",
        "name": "3",
        "region_bb": [
          89.28,
          68.4,
          253.44,
          292.32
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.0,
          308.16,
          292.32,
          339.12
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 3: Classi\ufb01cation dataset description (%).",
        "name": "3",
        "region_bb": [
          327.6,
          320.40000000000003,
          547.2,
          411.84000000000003
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          344.16,
          298.8,
          529.9200000000001,
          308.16
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6
    ]
  },
  "nips12_1": {
    "figures": [
      {
        "caption": "Figure 2: Synaptic models that maximize initial SNR. (a) For potentiation, all transitions starting from a weak state lead to a strong state, and the probabilities for all transitions leaving a given weak state sum to 1. (a) Depression is similar to potentiation, but with strong and weak interchanged. (c) The equivalent two state model, with transition probabilities under potentiation and depression equal to one.",
        "name": "2",
        "region_bb": [
          200.16,
          81.36,
          412.56,
          126.72
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          140.4,
          504.0,
          195.12
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: Models of complex synapses. (a) The cascade model of [10], showing transitions between states of high/low synaptic weight (red/blue circles) due to potentiation/depression (solid red/dashed blue arrows). (b) The serial model of [12]. (c) The memory curves of these two models, showing the decay of the signal-to-noise ratio (to be defined in \u00a72) as subsequent memories are stored.",
        "name": "1",
        "region_bb": [
          124.56,
          81.36,
          486.0,
          154.8
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.72,
          169.92000000000002,
          504.0,
          216.72
        ],
        "page": 3,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 4: The memory curve envelope for N = 100, M = 12. (a) An upper bound on the SNR at any time is shown in green. The red dashed curve shows the result of numerical optimization of synaptic models with random initialization. The solid red curve shows the highest SNR we have found with hand designed models. At early times these models are of the form shown in (b) with different numbers of states, and all transition probabilities equal to 1. At late times they are of the form shown in (c) with different values of \u03b5. The model shown in (c) also saturates the area bound (16) in the limit \u03b5 \u2192 0.",
        "name": "4",
        "region_bb": [
          123.12,
          81.36,
          468.0,
          201.6
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          218.16,
          504.0,
          296.64
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3: Perturbations that increase the area. (a) Perturbations that increase elements of M pot above the diagonal and decrease the corresponding elements of M dep . It can no longer be used when M dep is lower triangular, i.e. depression must move synapses to \u201cmore depressed\u201d states. (b) Perturbations that decrease elements of M pot below the diagonal and increase the corresponding elements of M dep . It can no longer be used when M pot is upper triangular, i.e. potentiation must move synapses to \u201cmore potentiated\u201d states. (c) Perturbation that decreases \u201cshortcut\u201d transitions and increases the bypassed \u201cdirect\u201d transitions. It can no longer be used when there are only nearest- neighbor \u201cdirect\u201d transitions.",
        "name": "3",
        "region_bb": [
          216.0,
          81.36,
          396.72,
          113.04
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          126.0,
          504.0,
          213.84
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "icml13_1": {
    "figures": [
      {
        "caption": "Table 2. Examples of some factorized matrix norms on R m\u00d7n , each induced by two atomic norms (last two rows giving ~ denotes the entries non-negative factorizations). Here kM k 2,\u221e is the length of the ` 2 -longest row of the matrix M , and M of M written in a single large vector.",
        "name": "2",
        "region_bb": [
          63.36,
          66.96000000000001,
          533.52,
          149.76000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          55.440000000000005,
          155.52,
          540.72,
          186.48000000000002
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 1. Some examples of atomic domains suitable for optimization using the Frank-Wolfe algorithm. Here SVD refers to the complexity of computing a singular value decomposition, which is O(min{mn 2 , m 2 n}). N f is the number of non-zero \u03b4C f entries in the gradient of the objective function f , and \u03b5 0 = k+2 is the required accuracy for the linear subproblems. For 1 1 any p \u2208 [1, \u221e], the conjugate value q is meant to satisfy p + q = 1, allowing q = \u221e for p = 1 and vice versa.",
        "name": "1",
        "region_bb": [
          72.72,
          66.96000000000001,
          524.16,
          243.36
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          56.160000000000004,
          249.12,
          541.44,
          295.92
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "aaai11_9": {
    "figures": [
      {
        "caption": "Figure 2: Decomposition and combination",
        "name": "2",
        "region_bb": [
          77.76,
          124.56,
          268.56,
          220.32000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          88.56,
          236.16,
          257.76,
          245.52
        ],
        "page": 3,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 5: P @n and N DCG@n on MQ2007",
        "name": "5",
        "region_bb": [
          57.6,
          112.32000000000001,
          288.72,
          189.36
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          84.24000000000001,
          204.48000000000002,
          261.36,
          213.84
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 1: Accuracy in M AP",
        "name": "1",
        "region_bb": [
          53.28,
          69.84,
          293.76,
          104.4
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          117.36,
          53.28,
          228.96,
          61.92
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Figure 4: Fitness calculation",
        "name": "4",
        "region_bb": [
          78.48,
          54.72,
          267.84000000000003,
          171.36
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          116.64,
          185.76000000000002,
          229.68,
          195.12
        ],
        "page": 4,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: Tree representation",
        "name": "1",
        "region_bb": [
          101.52000000000001,
          54.0,
          244.8,
          97.2
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          115.2,
          110.16000000000001,
          231.12,
          119.52000000000001
        ],
        "page": 3,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 7: Ef\ufb01ciency of CCRank",
        "name": "7",
        "region_bb": [
          323.28000000000003,
          157.68,
          556.5600000000001,
          247.68
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          375.12,
          262.8,
          501.84000000000003,
          272.16
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 6: P @n and N DCG@n on MQ2008",
        "name": "6",
        "region_bb": [
          322.56,
          54.0,
          554.4,
          131.04
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          349.2,
          146.16,
          527.04,
          154.8
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3: Evolution",
        "name": "3",
        "region_bb": [
          343.44,
          54.72,
          533.52,
          218.88
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          399.6,
          232.56,
          477.36,
          241.92000000000002
        ],
        "page": 3,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6
    ]
  },
  "nips10_1": {
    "figures": [
      {
        "caption": "Figure 2: Graphical illustration of our model. An input image is segmented into several regions. The annotation of the image is represented as a 0-1 vector indicating the presence/absence of each possible annotation term. Our model captures the unobserved mapping that translate image regions to annotation terms associated with the image (e.g. horse, athlete). For annotation terms not associated with the image (e.g. car, dog), there are no mapped image regions. Our model also captures relationship between the unobserved scene label (e.g. polo) and image regions/annotations.",
        "name": "2",
        "region_bb": [
          162.0,
          31.68,
          449.28000000000003,
          124.56
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          107.28,
          136.08,
          503.28000000000003,
          192.96
        ],
        "page": 3,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 5: (Best viewed in color) Results of annotation and segmentation on the UIUC sport dataset. Different annotation terms are shown in different colors. Image regions mapped to an annotation term are overlayed with the color corresponding to that annotation term.",
        "name": "5",
        "region_bb": [
          156.24,
          137.52,
          453.6,
          436.32
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          447.12,
          503.28000000000003,
          475.2
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 1: Comparison of image annotation (a) and scene clustering (b). The number of clusters is set to be eight for all methods. See the text for more descriptions.",
        "name": "1",
        "region_bb": [
          166.32,
          31.68,
          445.68,
          93.60000000000001
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          105.84,
          503.28000000000003,
          124.56
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Figure 4: Visualization of the \u201cposition\u201d components of the \u03b1 parameters for some annotation terms. Bright areas correspond to high values.",
        "name": "4",
        "region_bb": [
          108.0,
          182.88,
          501.12,
          224.64000000000001
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          107.28,
          236.88,
          502.56,
          255.60000000000002
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: Our goal is to learn a model using images and their associated unaligned textual object annotations (a) as the training data. Given a new image (b), we can use the model to predict its textual annotations and roughly localize image regions corresponding to each of the annotation terms (c).",
        "name": "1",
        "region_bb": [
          109.44,
          31.68,
          503.28000000000003,
          115.92
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          129.6,
          503.28000000000003,
          157.68
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3: Visualization of \u03b3 parameters. Each plot corresponds to a scene label s, we show the weights of top s five components of \u03b3 j,1 of all j \u2208 {1..V } (y-axis) and the corresponding annotation terms (x-axis).",
        "name": "3",
        "region_bb": [
          118.08,
          37.440000000000005,
          479.52000000000004,
          138.96
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          107.28,
          144.72,
          503.28000000000003,
          164.16
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "aaai13_1": {
    "figures": [
      {
        "caption": "Figure 2: The \ufb01rst-order derivative of MCP for different val- ues of \u03b3.",
        "name": "2",
        "region_bb": [
          86.4,
          69.12,
          262.08,
          178.56
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.0,
          202.32,
          291.6,
          222.48000000000002
        ],
        "page": 3,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: The MCP function for different values of \u03b3.",
        "name": "1",
        "region_bb": [
          350.64,
          429.84000000000003,
          526.32,
          539.28
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          330.48,
          563.04,
          543.6,
          572.4
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 4: Prediction Errors. Here \u201c1,\u201d \u201c2,\u201d and \u201c3\u201d are for SparseNet R, DC and ALM respectively.",
        "name": "4",
        "region_bb": [
          73.44,
          224.64000000000001,
          554.4,
          333.36
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          348.48,
          503.28000000000003,
          357.84000000000003
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3: Feature Selection Error. Here \u201c1,\u201d \u201c2,\u201d and \u201c3\u201d are for SparseNet R, DC and ALM respectively.",
        "name": "3",
        "region_bb": [
          72.0,
          66.24000000000001,
          554.4,
          174.96
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          95.76,
          190.8,
          515.52,
          200.16
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 1: Simulated results of SparseNet, DC and ALM.",
        "name": "1",
        "region_bb": [
          319.68,
          84.96000000000001,
          555.12,
          296.64
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          328.32,
          63.36,
          548.64,
          72.72
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7
    ]
  },
  "nips10_9": {
    "figures": [
      {
        "caption": "Figure 2: Two-level relation trees for the P erson type (left) and the \u27e8P erson, P erson\u27e9 type (right).",
        "name": "2",
        "region_bb": [
          108.7452581487227,
          206.68800720982395,
          504.11709075566813,
          276.5442326431094
        ],
        "page_height": 842,
        "page_width": 596,
        "caption_bb": [
          108.0250908762146,
          274.3837308255851,
          503.39692348316004,
          288.78707627574704
        ],
        "page": 4,
        "dpi": 72.01672725080974,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 4: Overall performance. Bold identifies the best performance, and \u00b1 marks the standard deviations. Experiments are conducted with Intel Xeon 2.33GHz CPU (E5410). \u22c6 These results were started with a treeRMN that only has unary features. \u00a7 The CLL of kinship data is not comparable to previous approaches, because we treat each of its labels as one variable with 26 categories instead of 26 binary variables. \u2020 The results of existing methods were run on different machines (Intel Xeon 2.8GHz CPU), and their 10-fold data splits are independent to those used for the RMN models. They were allowed to run up to 10-24 hours, and here we assumes that these methods cannot achieve similar accuracy when the amount of training time is significantly reduced.",
        "name": "4",
        "region_bb": [
          115.22676360129557,
          82.0990690659231,
          496.91541803058715,
          274.3837308255851
        ],
        "page_height": 842,
        "page_width": 596,
        "caption_bb": [
          107.3049236037065,
          281.58540355066606,
          504.11709075566813,
          367.28530897912964
        ],
        "page": 8,
        "dpi": 72.01672725080974,
        "figure_type": "Table"
      },
      {
        "caption": "Table 2: The associated entities and features (sorted by decreasing magnitude of feature weights) for the first four categories \u2227 of the induced hidden variable a.H 0 on the Animal data. The features are in the form a.H 0 = C i a.A = 1, where A is any of the variables in the last two columns.",
        "name": "2",
        "region_bb": [
          108.0250908762146,
          77.05789815836641,
          509.15826166322483,
          179.32165085451624
        ],
        "page_height": 842,
        "page_width": 596,
        "caption_bb": [
          108.0250908762146,
          186.52332357959722,
          503.39692348316004,
          218.2106835699535
        ],
        "page": 7,
        "dpi": 72.01672725080974,
        "figure_type": "Table"
      },
      {
        "caption": "Table 1: Number of entities tional dataset with no relational structure, but is useful as a base case (#E) and attributes (#A) for for comparison. The Nation dataset contains attributes of nations four datasets. \u2217 The kinship and relations among them. The binary predicates are of the form data has only one attribute R(n 1 , n 2 ), where n 1 , n 2 are nations and R is a relation between which has 26 possible values.",
        "name": "1",
        "region_bb": [
          385.28949079183207,
          432.8205307773665,
          508.43809439071674,
          494.0347489405548
        ],
        "page_height": 842,
        "page_width": 596,
        "caption_bb": [
          384.569323519324,
          501.23642166563576,
          504.11709075566813,
          554.528799831235
        ],
        "page": 6,
        "dpi": 72.01672725080974,
        "figure_type": "Table"
      },
      {
        "caption": "Figure 1: (Left) is a schema, where round and rectangular boxes represent basic and composite entity types respectively. (Right) is a corresponding entity relation graph with three basic entities: p1, p2, c1. For clarity we only show one direction of the relations and omit their labels.",
        "name": "1",
        "region_bb": [
          141.15278541158708,
          82.8192363384312,
          469.54906167527946,
          167.79897449438667
        ],
        "page_height": 842,
        "page_width": 596,
        "caption_bb": [
          108.0250908762146,
          163.4779708593381,
          503.39692348316004,
          201.64683630226725
        ],
        "page": 4,
        "dpi": 72.01672725080974,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3: change of the con- ditional log likelihood during are similar to those reported by Kok and Domingos [11]. training for the UML data.",
        "name": "3",
        "region_bb": [
          378.0878180667511,
          369.4458107966539,
          500.51625439312767,
          489.7137453055062
        ],
        "page_height": 842,
        "page_width": 596,
        "caption_bb": [
          377.367650794243,
          494.75491621306287,
          504.11709075566813,
          525.7221089309111
        ],
        "page": 7,
        "dpi": 72.01672725080974,
        "figure_type": "Figure"
      },
      {
        "caption": "C2 Amphibian Animal Bird Invertebrate c \u2212\u2212\u2212\u2212\u2212\u2192cc.InteractsWith c \u2212\u2212\u2212\u2212\u2212\u2192cc.PropertyOf CC2 \u22121 CC2 \u22121 Fish Mammal Reptile Vertebrate c \u2212\u2212\u2212\u2212\u2212\u2192cc.InteractsWith c \u2212\u2212\u2212\u2212\u2212\u2192cc.PartOf ... Table 3: The associated entities and features (sorted by decreasing magnitude of feature weights) for the first three categories \u2227 of the induced hidden variable c.H 0 on the UML data. The features are in the form c.H 0 = C i A = 1, where A is any of the variables in the last column.",
        "name": "3",
        "region_bb": [
          110.90575996624699,
          231.89386174760736,
          500.51625439312767,
          316.8735999035628
        ],
        "page_height": 842,
        "page_width": 596,
        "caption_bb": [
          108.0250908762146,
          316.1534326310547,
          503.39692348316004,
          352.8819635289677
        ],
        "page": 7,
        "dpi": 72.01672725080974,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "aaai10_3": {
    "figures": [
      {
        "caption": "Figure 2: F1-values for ca, ca+co, and ca+co+sm averaged over the 21 OAEI reference alignments for thresholds rang- ing from 0.45 to 0.95. AgreementMaker was the best per- forming system on the conference dataset of the latest ontol- ogy evaluation initiative in 2009.",
        "name": "2",
        "region_bb": [
          321.12,
          57.6,
          552.96,
          211.68
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          318.96000000000004,
          226.08,
          558.0,
          278.64
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: Example ontology fragments.",
        "name": "1",
        "region_bb": [
          318.96000000000004,
          54.72,
          561.6,
          228.96
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          360.0,
          236.16,
          516.96,
          245.52
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 1: Average F1-values over the 21 OAEI reference alignments for manual weights (ca+co+sm) vs. learned weights (ca+co+sl) vs. formulation without stability con- straints (ca+co); thresholds range from 0.6 to 0.95.",
        "name": "1",
        "region_bb": [
          321.12,
          296.64,
          556.5600000000001,
          341.28000000000003
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          318.96000000000004,
          350.64,
          558.0,
          393.12
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6
    ]
  },
  "nips12_2": {
    "figures": [
      {
        "caption": "Figure 2: (a) Left: the phase diagram for sequences generated by the time-rescaled renewal process with the gamma ISI distribution. The ordinate represents the amplitude of rate fluctuation \u03c3, and abscissa represents CV of the gamma ISI distribution. The dots represent the result of numerical simulations in which the empirical Bayes decoder provides a fluctuating rate estimation (\u02c6  \u03b3 > 0). Each dot is plotted if \u03b3\u02c6 > 0 in more than 20 out of 40 identical trials. The solid line represents the theoretical lower bound obtained by the formula (21). Right: raster plots of sample spike trains and the estimated rates. The dotted lines and the solid lines represent the underlying rates and the estimated rates, respectively. The parameters (C V , \u03c3) of top (\u02c6 \u03b3 > 0) and bottom (\u02c6  \u03b3 = 0) are (0.6, 0.3) and (1.5, 0.15), respectively. (b) The optimal hyperparameter \u03b3\u02c6 as a function of \u03c3 for CV = 0.6. The solid line represents the theoretical value, and the error bars represent the average and standard deviation of \u03b3\u02c6 determined by applying the empirical Bayes algorithm to 40 trials. (c, d) The phase diagrams for the IG and lognormal ISI distributions. (e) Comparison of the lower bounds among the three models. (f-h) The phase diagrams for the gamma, IG and lognormal ISI distributions, when the rate process is given by \u03bb(t) = \u00b5 + \u03c3 sin t/\u03c4 with \u00b5 = 1 and \u03c4 = 10. ",
        "name": "2",
        "region_bb": [
          162.0,
          80.64,
          452.15999999999997,
          339.12
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          106.56,
          357.12,
          503.28,
          511.2
        ],
        "page": 7,
        "dpi": 72,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: (a) The gamma (blue), IG (green) and lognormal (red) ISI distribution functions for CV =0.6, 1 and 1.5. (b) The KL divergence as a function of \u03c3 for C V =0.6, 1 and 1.5, when the rate fluctuates according to the Ornstein-Uhlenbeck process (17) with \u00b5 = 1 and \u03c4 = 10. The blue, green and red indicate the KL divergence for the gamma, IG and lognormal distribution, respectively. The lines represent the theoretical values obtained by Eqs. (13), (14) and (15), and the error bars repre- sent the average and standard deviation numerically computed according to Eq. (5) with n = 50, 000 and 10 trials. (c) The KL divergence for the sinusoidally modulated rate, \u03bb(t) = \u00b5 + \u03c3 sin t/\u03c4 , with \u00b5 = 1 and \u03c4 = 10. ",
        "name": "1",
        "region_bb": [
          156.23999999999998,
          81.36,
          455.76,
          256.32
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          106.56,
          273.59999999999997,
          503.28,
          361.44
        ],
        "page": 5,
        "dpi": 72,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "icml10_5": {
    "figures": [
      {
        "caption": "Figure 2. Results on synthetic dataset with varying noise features",
        "name": "2",
        "region_bb": [
          67.68,
          70.56,
          527.04,
          180.72
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          180.72,
          185.76000000000002,
          414.72,
          194.4
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 5. Training time on various data sets",
        "name": "5",
        "region_bb": [
          57.6,
          398.16,
          283.68,
          699.84
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          93.60000000000001,
          702.72,
          249.84,
          711.36
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 4. Training time (in s) of various methods on small datasets",
        "name": "4",
        "region_bb": [
          119.52000000000001,
          86.4,
          475.2,
          232.56
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          180.72,
          76.32000000000001,
          415.44,
          84.24000000000001
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 2. Sparsity ratio of various methods on small datasets",
        "name": "2",
        "region_bb": [
          55.440000000000005,
          380.16,
          299.52000000000004,
          526.32
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          56.160000000000004,
          362.88,
          267.84000000000003,
          371.52000000000004
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 1. Datasets used in the experiments",
        "name": "1",
        "region_bb": [
          59.760000000000005,
          84.24000000000001,
          282.96000000000004,
          187.20000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          97.92,
          76.32000000000001,
          246.24,
          83.52
        ],
        "page": 4,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Figure 4. Testing accuracy on different data sets",
        "name": "4",
        "region_bb": [
          58.32,
          72.0,
          285.12,
          369.36
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          85.68,
          372.24,
          257.76,
          380.88
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1. Detailed information of the synthetic dataset",
        "name": "1",
        "region_bb": [
          311.04,
          67.68,
          534.96,
          159.12
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          326.88,
          167.76000000000002,
          521.28,
          176.4
        ],
        "page": 4,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3. Results on synthetic dataset with varying training samples",
        "name": "3",
        "region_bb": [
          68.4,
          210.24,
          527.04,
          319.68
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          176.4,
          324.72,
          419.04,
          333.36
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 3. Testing accuracies (in %) of various methods on small datasets",
        "name": "3",
        "region_bb": [
          92.88000000000001,
          84.24000000000001,
          501.84000000000003,
          232.56
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          170.64000000000001,
          76.32000000000001,
          425.52000000000004,
          84.24000000000001
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8
    ]
  },
  "nips13_10": {
    "figures": [
      {
        "caption": "Figure 2: Type I errors on the distributions shown in Figure 3 for \u03b1 = 5%: (a) MMD, single kernel, \u03c3 = 1, (b) MMD, single kernel, \u03c3 set to the median pairwise distance, and (c) MMD, non-negative linear combination of multiple kernels. The experiment was repeated 30000 times. Error bars are not visible at this scale.",
        "name": "2",
        "region_bb": [
          113.76,
          316.08,
          497.52000000000004,
          422.64000000000004
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          437.76,
          504.0,
          466.56
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 5: Empirical Type I error rate for \u03b1 = 5% on the music data (Section 3.2). (a) A single kernel test with \u03c3 = 1, (b) A single kernel test with \u03c3 = median, and (c) for multiple kernels. Error bars are not visible at this scale. The results broadly follow the trend visible from the synthetic experiments.",
        "name": "5",
        "region_bb": [
          113.76,
          262.08,
          497.52000000000004,
          367.92
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          383.76,
          503.28000000000003,
          412.56
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 2: A comparison of consistent tests on the music experiment described in Section 3.2. Here computation",
        "name": "2",
        "region_bb": [
          110.16000000000001,
          82.08,
          501.84000000000003,
          210.24
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          220.32000000000002,
          502.56,
          239.04000000000002
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 1: Sample complexity for tests on the distributions described in Figure 3. The fourth column indicates the minimum number of samples necessary to achieve Type I and Type II errors of 5%. The fifth column is the computation time required for 2000 samples, and is not presented for settings that have unsatisfactory sample complexity.",
        "name": "1",
        "region_bb": [
          109.44,
          82.08,
          501.84000000000003,
          249.12
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          259.2,
          503.28000000000003,
          298.08
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Figure 4: Synthetic experiment: number of Type II er- In the first experiment we set the Type I error to rors vs B, given a fixed probability \u03b1 of Type I er- be 5%, and we recorded the Type II error. We rors. As B grows, the Type II error drops quickly when conducted these experiments on 2000 samples the kernel is appropriately chosen. The kernel selec- over 1000 repetitions, with varying block size, tion method is described in [9], and closely approx- B. Figure 4 presents results for different kernel imates the baseline performance of the well-informed",
        "name": "4",
        "region_bb": [
          306.0,
          85.68,
          496.08000000000004,
          234.72
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          306.0,
          245.52,
          503.28000000000003,
          311.76
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: Empirical distributions under H 0 and H A for different regimes of B for the music experiment (Section 3.2). In both plots, the number of samples is fixed at 500. As we vary B, we trade off the quality of the finite sample Gaussian approximation to the null distribution, as in Theorem 2.3, with the variances of the H 0 and H A distributions, as outlined in Section 2.1. In (b) the distribution under H 0 does not resemble a Gaussian (it does not pass a level 0.05 Kolmogorov-Smirnov (KS) normality test [16, 20]), and a Gaussian approximation results in a conservative test threshold (vertical green line). The remaining empirical distributions all pass a KS normality test.",
        "name": "1",
        "region_bb": [
          108.72,
          87.12,
          501.12,
          262.08
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          107.28,
          277.2,
          503.28000000000003,
          345.6
        ],
        "page": 3,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3: Synthetic data distributions P and which is a standard way to choose the Gaussian kernel Q. Samples belonging to these classes are bandwidth [17], although it is likewise arbitrary in this difficult to distinguish. context. Finally, we applied a kernel learning strategy, in which the kernel was optimized to maximize the test power for the alternative P 6 = Q [9]. This",
        "name": "3",
        "region_bb": [
          348.48,
          547.9200000000001,
          499.68,
          616.32
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          345.6,
          631.44,
          503.28000000000003,
          660.24
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "nips09_2": {
    "figures": [
      {
        "caption": "Table 2: The classification performance of Transductive SVMs on benchmark data sets.",
        "name": "2",
        "region_bb": [
          115.2,
          228.96,
          493.20000000000005,
          334.8
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          114.48,
          218.88,
          496.08000000000004,
          227.52
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 1: Datasets used in our experiments. d represents the data dimensionality, and n denotes the total number of examples.",
        "name": "1",
        "region_bb": [
          179.28,
          467.28000000000003,
          429.12,
          529.2
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          448.56,
          503.28000000000003,
          466.56
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "aaai10_7": {
    "figures": [
      {
        "caption": "Figure 1: A simple imperfect graph (\u03c7(G)=3 and \u03c9(G)=2)",
        "name": "1",
        "region_bb": [
          135.36,
          561.6,
          267.84000000000003,
          653.0400000000001
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          64.8,
          661.6800000000001,
          280.8,
          671.0400000000001
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 3: Runtimes [sec] for DIMACS benchmarks. \u201cd\u201d stands for the density. For Cliquer, MCQdyn and MaxCLQ \u2212 , \u201c-\u201d means that an instance cannot be solved in 24 hours, except the instances keller5, p hat1500-2 and p hat1000-3 that cannot be solved in 5 days; for Regin and MCR, \u201c-\u201d means that the runtime is not avail- able. s is the average upper bound improvement by MaxSAT, and Rate is the success rate of MaxSAT technology in MaxCLQ to prune subtrees (explained later).",
        "name": "3",
        "region_bb": [
          53.28,
          142.56,
          319.68,
          503.28000000000003
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.0,
          57.6,
          292.32,
          136.8
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 2: Runtimes [sec] for random graphs. For Cliquer, MCQdyn and MaxCLQ \u2212 , \u201c-\u201d means that an instance cannot be solved in 3 hours; for MCR, \u201c-\u201d means that the runtime is not available. The runtimes of Regin for random graphs are not available. s is the average upper bound improvement by MaxSAT, and Rate is the success rate of MaxSAT technology in MaxCLQ to prune subtrees (explained later), averaged for 50 graphs at each point.",
        "name": "2",
        "region_bb": [
          329.04,
          411.12,
          544.32,
          563.76
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          318.96000000000004,
          336.24,
          557.28,
          405.36
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "ative occurrences but only one positive occurrence (in a soft clause). Instead, we detect if every literal in a soft clause is failed. In fact, given a soft clause c=x 1 \u2228 x 2 \u2228 ... \u2228 x t , if every x i (1 \u2264 i \u2264 t) is a failed literal, the union of all the soft clauses used to produce an empty clause for every x i , together with c, constitute an inconsistent subset. For example, in the independent set based MaxSAT en- coding of the graph in Figure 1 presented in the last section, there are three soft clauses {x 1 \u2228x 4 \u2228x 6 , x 2 \u2228x 3 , x 5 } corre- sponding to the three independent sets in an optimal partition of the graph. The hard clauses are: {\u00af x 1 \u2228 x \u00af 4 , x \u00af 1 \u2228 x \u00af 5 , x \u00af 2 \u2228 x \u00af 3 , x \u00af 2 \u2228 x \u00af 5 , x \u00af 3 \u2228 x \u00af 4 , x \u00af 1 \u2228 x \u00af 6 , x \u00af 2 \u2228 x \u00af 6 , x \u00af 4 \u2228 x \u00af 6 , x \u00af 5 \u2228 x \u00af 6 }. The initial upper bound for a maximum clique is 3, which is the tightest upper bound that can be obtained by using a color- ing process. Let us see that x 5 is a failed literal, and allows us to improve the upper bound. Set x 5 =1 to satisfy the third soft clause, x \u00af 5 is removed from the hard clauses x \u00af 1 \u2228 x \u00af 5 , x \u00af 2 \u2228 x \u00af 5 , and x \u00af 5 \u2228 x \u00af 6 , the three new unit clauses imply that x 1 =0, x 2 =0 and x 6 =0, then x 1 and x 6 are removed from the first soft clause, and x 2 from the second, which become unit. So, x 4 and x 3 should be assigned 1, making the hard clause x \u00af 3 \u2228 x \u00af 4 empty. So, x 5 is a failed literal and the three soft clauses used in the propagation to produce the empty clause constitute an inconsistent subset, because this subset, in con- junction with hard clauses, allows to derive a contradiction. Since at most two soft clauses can be satisfied by any assign- ment satisfying all the hard clauses, we improve the upper bound for a maximum clique from 3 to 2. In general, we have Table 1: Run time [sec.] of Maxsatz (version 2009, MSZ in the table) and Minimaxsat (Mini in the table) on a Macpro 2.8 Ghz with 4Gb of memory for different MaxSAT encodings of Maxclique, k is the number of soft clauses in an encoding",
        "name": "1",
        "region_bb": [
          54.72,
          102.24000000000001,
          288.0,
          224.64000000000001
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.0,
          57.6,
          292.32,
          96.48
        ],
        "page": 4,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6
    ]
  },
  "icml10_2": {
    "figures": [
      {
        "caption": "Figure 2. Results on the USPS benchmark.",
        "name": "2",
        "region_bb": [
          348.48,
          64.8,
          501.12,
          185.76000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          342.72,
          199.44,
          506.16,
          207.36
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1. An illustration of a Haar-like basis.",
        "name": "1",
        "region_bb": [
          139.68,
          65.52,
          236.16,
          365.04
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          102.24000000000001,
          383.04,
          272.88,
          390.96000000000004
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 4. Coefficient decay of target function of USPS benchmark, in the Laplacian eigenbasis and in a Haar-like basis. Note that in the Haar-like basis 930 coefficients out of 1500 are identically zero.",
        "name": "4",
        "region_bb": [
          346.32,
          228.96,
          502.56,
          354.96000000000004
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          314.64,
          369.36,
          534.96,
          408.24
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3. Results on the MNIST subset.",
        "name": "3",
        "region_bb": [
          349.2,
          64.8,
          501.12,
          185.04000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          348.48,
          198.72,
          500.40000000000003,
          206.64000000000001
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 1. Test classification errors for USPS benchmark",
        "name": "1",
        "region_bb": [
          87.12,
          86.4,
          286.56,
          250.56
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          78.48,
          71.28,
          285.12,
          77.76
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8
    ]
  },
  "nips12_7": {
    "figures": [
      {
        "caption": "Figure 2: Figure 2(a) and 2(b) show the effect of changing the underlying data size or number of dimension. Figure 2(c) shows that our DPMM proposal for children sets is comparable to an exhaustive enumeration of all possible children sets (enum).",
        "name": "2",
        "region_bb": [
          120.24000000000001,
          93.60000000000001,
          487.44,
          205.20000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          219.6,
          504.0,
          251.28
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: The beta coalescent can merge four simi- 17: if ESS < S/2 then lar nodes at once, while Kingman\u2019s coalescent only",
        "name": "1",
        "region_bb": [
          323.28000000000003,
          92.16,
          510.48,
          257.04
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          316.08,
          264.96000000000004,
          514.08,
          294.48
        ],
        "page": 3,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 4: One sample hierarchy of the 20news- groups from beta. Each small square is a docu- ment colored by its class label. Large rectangles represent a subtree with all the enclosed docu- ments as leaf nodes. Most of the documents from the same group are clustered together; the three \u201crec\u201d groups are merged together first, and then merged with the religion and space groups.",
        "name": "4",
        "region_bb": [
          311.04,
          91.44,
          491.04,
          185.04000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          305.28000000000003,
          215.28,
          496.8,
          300.96000000000004
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3: One sample hierarchy of human tissue from beta. Color indicates germ layer origin of tissue. Dashed border indicates secretory func- tion. While neural tissues from the ectoderm were clustered correctly, some mesoderm and endoderm tissues were commingled. The cluster also preferred placing secretory tissues together and higher in the tree.",
        "name": "3",
        "region_bb": [
          110.88000000000001,
          84.96000000000001,
          295.2,
          187.92000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          215.28,
          298.8,
          300.96000000000004
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 1: Comparing the three models: beta performs best on all three scores.",
        "name": "1",
        "region_bb": [
          112.32000000000001,
          313.2,
          497.52000000000004,
          365.04
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          150.48000000000002,
          375.84000000000003,
          456.48,
          384.48
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "aaai14_10": {
    "figures": [
      {
        "caption": "Table 5: Portfolio turnovers (%)",
        "name": "5",
        "region_bb": [
          318.96000000000004,
          73.44,
          558.72,
          138.96
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          374.40000000000003,
          64.08,
          501.84000000000003,
          72.0
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 4: Cumulative wealth",
        "name": "4",
        "region_bb": [
          54.0,
          72.0,
          290.16,
          159.84
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          118.08,
          64.08,
          226.8,
          70.56
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 2: Summary of the tested datasets",
        "name": "2",
        "region_bb": [
          54.0,
          74.16,
          289.44,
          162.0
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          93.60000000000001,
          64.08,
          252.0,
          72.72
        ],
        "page": 4,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 1: Summary of key notations",
        "name": "1",
        "region_bb": [
          321.84000000000003,
          72.0,
          550.8000000000001,
          211.68
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          368.64,
          64.08,
          508.32,
          71.28
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Figure 1: The curves of cumulative wealth across the investment periods for different portfolios on a) FF25, b) FF48, c) FF100, d) ETF, e) INDEX, and f) EQUITY datasets.",
        "name": "1",
        "region_bb": [
          59.04,
          54.0,
          553.6800000000001,
          285.84000000000003
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.0,
          293.76,
          557.28,
          312.48
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 6: Portfolio volatilities (%) with p-values",
        "name": "6",
        "region_bb": [
          324.0,
          344.16,
          548.64,
          480.96000000000004
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          344.16,
          334.08,
          532.08,
          342.72
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 3: Portfolio Sharpe ratios (%) with p-values",
        "name": "3",
        "region_bb": [
          321.12,
          74.16,
          551.52,
          221.76000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          339.12,
          64.08,
          537.12,
          72.72
        ],
        "page": 4,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7
    ]
  },
  "nips13_6": {
    "figures": [
      {
        "caption": "Figure 1: An example of a non-adaptive policy tree (left) and an adaptive policy tree (right).",
        "name": "1",
        "region_bb": [
          157.68,
          82.08,
          452.88,
          168.48000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          123.12,
          181.44,
          487.44,
          190.08
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 1: AUC of different learning algorithms with batch size s = 10.",
        "name": "1",
        "region_bb": [
          108.0,
          109.44,
          505.44,
          212.4
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          166.32,
          91.44,
          443.52000000000004,
          100.08
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "aaai11_3": {
    "figures": [
      {
        "caption": "Figure 2: (a) Performance as the number of link types in- creases. (b) AUC and inference variance for a hybrid model that only uses CEC on a limited number of models.",
        "name": "2",
        "region_bb": [
          314.64,
          353.52000000000004,
          559.44,
          457.2
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          318.96000000000004,
          468.72,
          557.28,
          499.68
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: AUC on real and synthetic datasets for varying proportions of labeled test data.",
        "name": "1",
        "region_bb": [
          50.4,
          65.52,
          540.0,
          198.72
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          128.88,
          208.08,
          482.40000000000003,
          217.44
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 1: AUCs for varying autocorrelation and linkage.",
        "name": "1",
        "region_bb": [
          347.04,
          237.6,
          530.64,
          316.08
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          328.32,
          326.16,
          547.9200000000001,
          335.52000000000004
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7
    ]
  },
  "nips09_9": {
    "figures": [],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "nips13_3": {
    "figures": [
      {
        "caption": "Figure 2: The averaged ROC curves of the neighborhood pursuit when combined with different correlation estimators. \u201cSKEPTIC\u201d represents the indefinite nonparanormal SKEPTIC estimator, and \u201cProjection\u201d represents our proposed projection approach.",
        "name": "2",
        "region_bb": [
          238.5722508330809,
          582.1763102090275,
          1039.8149045743714,
          780.2362920327173
        ],
        "page_height": 1651,
        "page_width": 1275,
        "caption_bb": [
          225.06816116328383,
          814.7467434110876,
          1050.318085428658,
          877.765828536807
        ],
        "page": 7,
        "dpi": 150.04544077552256,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: The empirical performance using different smoothing parameters. \u00b5 = 1 has a similar performance to the smaller \u00b5\u2019s in terms of the estimation error.",
        "name": "1",
        "region_bb": [
          241.57315964859131,
          171.05180248409573,
          1030.81217812784,
          405.12269009391093
        ],
        "page_height": 1651,
        "page_width": 1275,
        "caption_bb": [
          225.06816116328383,
          439.6331414722811,
          1050.318085428658,
          480.1454104816722
        ],
        "page": 7,
        "dpi": 150.04544077552256,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 4: Two topic graphs illustrating the difference of the estimated topic graphs. The smooth- projected neighborhood pursuit (subfigure (a)) generates 6 mid-size modules and 6 small modules while the naive neighborhood pursuit (subfigure (b)) generates 1 large module, 2 mid-size modules and 6 small modules.",
        "name": "4",
        "region_bb": [
          243.07361405634654,
          862.7612844592547,
          1035.3135413511056,
          1096.8321720690699
        ],
        "page_height": 1651,
        "page_width": 1275,
        "caption_bb": [
          223.5677067555286,
          1129.842169039685,
          1048.8176310209028,
          1212.3671614662223
        ],
        "page": 8,
        "dpi": 150.04544077552256,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3: The averaged ROC curves of the neighborhood pursuit when combined with different correlation estimators. \u201cSNP\u201d represents our proposed estimator and \u201cNNP\u201d represents the Pearson estimator. The SNP uniformly outperforms the NNP for all four graphs.",
        "name": "3",
        "region_bb": [
          238.5722508330809,
          1107.3353529233566,
          1039.8149045743714,
          1303.8948803392911
        ],
        "page_height": 1651,
        "page_width": 1275,
        "caption_bb": [
          225.06816116328383,
          1336.904877309906,
          1050.318085428658,
          1401.4244168433806
        ],
        "page": 7,
        "dpi": 150.04544077552256,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "nips11_9": {
    "figures": [
      {
        "caption": "Figure 1: The hidden Markov process (21\u201322) for   = 0. Gray circles and arrows indicate on the realization and transitions of the internal Markov process; see (21). The light circles and black arrows indicate on the realizations of the observed process.",
        "name": "1",
        "region_bb": [
          239.76000000000002,
          84.96000000000001,
          372.24,
          177.84
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          107.28,
          195.12,
          503.28000000000003,
          226.08
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "nips12_5": {
    "figures": [
      {
        "caption": "Figure 2: Comparison of different planning strategies (on the same problem as in figure 1). The \u201cSafe\u201d strategy is to use uniform planning in the individual trees, the \u201cOptimistic\u201d strategy is to use OPD. ASOP corresponds to the \u201cSafe+Optimistic\u201d strategy.",
        "name": "2",
        "region_bb": [
          110.88000000000001,
          81.36,
          498.96000000000004,
          234.0
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          107.28,
          252.0,
          503.28000000000003,
          282.24
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: Comparison of ASOP to OP-MDP, UCT, and FSSS on the inverted pendulum benchmark problem, showing the sum of discounted rewards for simulations of 50 time steps.",
        "name": "1",
        "region_bb": [
          110.88000000000001,
          81.36,
          500.40000000000003,
          290.88
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          308.16,
          503.28000000000003,
          327.6
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3: The middle branch (II) of this MDP is never explored deep enough if only the node with the largest b-value is sampled in each iteration. Transition probabilities are given in gray where not equal to one.",
        "name": "3",
        "region_bb": [
          107.28,
          606.24,
          485.28000000000003,
          676.08
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          693.36,
          504.0,
          724.32
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "nips10_2": {
    "figures": [
      {
        "caption": "Figure 2: Gamma distributed renewal process with shape parameter \u03b8 = 3 (H 0 ) and \u03b8 = 0.5 (H 1 ). The mean number of action potential is fixed to 10. (Left) Spike trains from the null and alternate hypothesis. (Right) Comparison of the power of each method. The error bars are standard deviation",
        "name": "2",
        "region_bb": [
          125.92352946245703,
          88.5062521364698,
          481.3876640593357,
          270.5556975879077
        ],
        "page_height": 842,
        "page_width": 595,
        "caption_bb": [
          107.93445382496316,
          285.6665211234025,
          502.9745548243283,
          325.96205055138876
        ],
        "page": 6,
        "dpi": 71.95630254997545,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: (Left) Illustration of how the point process space is stratified. (Right) Example of spike trains stratified by their respective spike count.",
        "name": "1",
        "region_bb": [
          126.64309248795678,
          81.31062188147224,
          478.5094119573367,
          277.7513278429052
        ],
        "page_height": 842,
        "page_width": 595,
        "caption_bb": [
          108.65401685046292,
          290.7034623019008,
          502.9745548243283,
          310.8512270158939
        ],
        "page": 3,
        "dpi": 71.95630254997545,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 4: (Left) Dissimilarity/divergences from tactile response across parameter sets. The values of each measure are shifted and scaled to be in the range of 0 to 1. \u03bb L2 uses 2.5 ms bins with no smoothing. (Right) Responses from the tactile response (left), stimulation settings selected by \u03bb L2 (center), and the realizations selected by K-S and C-M (right). Top row shows the spike trains stratified into number of spikes and then sorted by spike times. Bottom row shows the average response binned at 2.5 ms; the variance is shown as a thin green line.",
        "name": "4",
        "region_bb": [
          123.76484038595775,
          85.62800003447077,
          502.9745548243283,
          264.79919338390965
        ],
        "page_height": 842,
        "page_width": 595,
        "caption_bb": [
          107.2148907994634,
          284.227395072403,
          502.9745548243283,
          348.2685043418811
        ],
        "page": 8,
        "dpi": 71.95630254997545,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3: [Top] Precisely timed spike train model (H 0 ) versus equi-intensity Poisson process (H 1 ). Spike trains from the null and alternate hypothesis for L = 4. [Bottom] Comparison of the power of each method for L = 1, 2, 3, 4 on precisely timed spike train model (H 0 ) versus equi-intensity Poisson process (H 1 ). (Left) Power comparison for methods except for N . The rate statistic \u03bb L2 are not labeled, since they are not able to detect the difference. (Right) Wilcoxon test on the number of action potentials. The error bars are standard deviation over 10 Monte Carlo runs.",
        "name": "3",
        "region_bb": [
          129.5213445899558,
          87.06712608547028,
          481.3876640593357,
          367.6967060303745
        ],
        "page_height": 842,
        "page_width": 595,
        "caption_bb": [
          107.2148907994634,
          382.80752956586934,
          504.4136808753278,
          446.8486388353475
        ],
        "page": 7,
        "dpi": 71.95630254997545,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "nips10_6": {
    "figures": [
      {
        "caption": "Figure 2: a) Classification Accuracy b) Std. deviation as an indicator for confidence",
        "name": "2",
        "region_bb": [
          115.2,
          389.52000000000004,
          500.40000000000003,
          441.36
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          119.52000000000001,
          440.64000000000004,
          487.44,
          454.32
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: Constrained continuous MRF sampling algorithm",
        "name": "1",
        "region_bb": [
          109.44,
          433.44,
          461.52000000000004,
          696.24
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          187.92000000000002,
          700.5600000000001,
          424.08000000000004,
          709.2
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3: a) KL Divergence by sample size 0 \u00a0 2000 \u00a0 4000 \u00a0 6000 \u00a0 8000 \u00a0 Number \u00a0of \u00a0Poten1al \u00a0Func1ons \u00a0 10000 \u00a0 b) Runtime for 1000 samples",
        "name": "3",
        "region_bb": [
          116.64,
          76.32000000000001,
          495.36,
          192.96
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          141.12,
          201.6,
          470.88,
          210.24
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 1: Example PSL program for collective classification.",
        "name": "1",
        "region_bb": [
          205.92000000000002,
          76.32000000000001,
          406.08000000000004,
          108.0
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          186.48000000000002,
          118.08,
          424.08000000000004,
          126.72
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "nips13_8": {
    "figures": [
      {
        "caption": "Figure 1: Performance of various algorithms and strategies for HPQ on May 8 and 9, 2013. For clarity, the total value every 100 periods is shown. Top row: On May 8, MMMW outperforms the best strategy, and on May 9 the reverse happens. Bottom row: performance of different strategies. On May 8, b = 100 performs best, while on May 9, b = 40 performs best.",
        "name": "1",
        "region_bb": [
          110.16000000000001,
          244.08,
          502.56,
          478.08000000000004
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          498.96000000000004,
          504.0,
          540.0
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 1: Final performance of various algorithms in cents. Bolded values indicate best performance.",
        "name": "1",
        "region_bb": [
          174.96,
          82.08,
          437.04,
          195.12
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          210.24,
          502.56,
          230.4
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "nips13_2": {
    "figures": [
      {
        "caption": "Figure 1: Example of original graph G (left) and new graph G 0 (right) after collapsing cycle C = (1, 2, 3, 4, 5). In the new graph G 0 , edge weight w 1C = 1/2(w 12 \u2212 w 23 + w 34 \u2212 w 45 + w 15 ). Associate a binary variable with each new edge and consider the new probability distribution on",
        "name": "1",
        "region_bb": [
          233.28,
          369.36,
          379.44,
          433.44
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.72,
          443.52000000000004,
          503.28000000000003,
          462.96000000000004
        ],
        "page": 4,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 1: Evaluation of CP-BP and CP-LP on random MWM instances. Columns # CP-BP and # CP-LP indicate the percentage of instances in which the cutting plane methods found a MWM. The column # Tight LPs indicates the percentage for which the initial MWM-LP is tight (i.e. C = \u2205). # Correct and # Converged indicate the number of correct matchings and number of instances in which CP-BP converged upon",
        "name": "1",
        "region_bb": [
          136.08,
          557.28,
          475.92,
          655.2
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          666.0,
          504.0,
          707.0400000000001
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "nips09_6": {
    "figures": [
      {
        "caption": "Figure 2: Performance when trained via self-play starting from random initial weights. 95% confi- dence intervals are marked at each data point. The x-axis uses a logarithmic scale.",
        "name": "2",
        "region_bb": [
          151.82779838044817,
          87.06712608547028,
          447.56820186084724,
          307.9729749138949
        ],
        "page_height": 842,
        "page_width": 595,
        "caption_bb": [
          107.93445382496316,
          326.6816135768885,
          502.9745548243283,
          346.1098152653819
        ],
        "page": 7,
        "dpi": 71.95630254997545,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: Left: TD, TD-Root and TD-Leaf backups. Right: RootStrap(minimax) and TreeStrap(minimax).",
        "name": "1",
        "region_bb": [
          113.6909580289612,
          89.22581516196955,
          497.2180506203303,
          223.78410093042362
        ],
        "page_height": 842,
        "page_width": 595,
        "caption_bb": [
          110.09314290146243,
          244.6514286699165,
          495.059361543831,
          253.28618497591356
        ],
        "page": 2,
        "dpi": 71.95630254997545,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 3: Blitz performance at the Internet Chess Club",
        "name": "3",
        "region_bb": [
          198.59939503793223,
          82.03018490697201,
          413.02917663685906,
          115.84964710546046
        ],
        "page_height": 842,
        "page_width": 595,
        "caption_bb": [
          198.59939503793223,
          130.9604706409553,
          411.5900505858595,
          139.59522694695235
        ],
        "page": 8,
        "dpi": 71.95630254997545,
        "figure_type": "Table"
      },
      {
        "caption": "Table 2: Best performance when trained by self play. 95% confidence intervals given.",
        "name": "2",
        "region_bb": [
          228.82104210892192,
          361.9402018263765,
          382.0879665403696,
          428.8595631978536
        ],
        "page_height": 842,
        "page_width": 595,
        "caption_bb": [
          135.27784879395384,
          438.9334455548502,
          474.9115968298379,
          447.56820186084724
        ],
        "page": 7,
        "dpi": 71.95630254997545,
        "figure_type": "Table"
      },
      {
        "caption": "Table 1: Backups for various learning algorithms.",
        "name": "1",
        "region_bb": [
          189.96463873193517,
          82.03018490697201,
          421.6639329428561,
          173.4146891454408
        ],
        "page_height": 842,
        "page_width": 595,
        "caption_bb": [
          206.51458831842953,
          184.20813452793712,
          403.67485730536225,
          192.84289083393418
        ],
        "page": 4,
        "dpi": 71.95630254997545,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "aaai13_3": {
    "figures": [
      {
        "caption": "Figure 2: [top] The environment from Figure 1 with each connected region intersected by a particular set of obstacles assigned a graph vertex. Since movement within a single region does not intersect any new obstacles, the minimum constraint removal problem for this obstacle set can be trans- formed into a discrete problem. [bottom] The graph for the discrete minimum constraint removal problem. The num- bers next to a vertex denote the obstacles intersecting that",
        "name": "2",
        "region_bb": [
          338.40000000000003,
          54.72,
          538.5600000000001,
          393.84000000000003
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          318.96000000000004,
          412.56,
          558.0,
          506.88
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 5: A clause gadget for x i \u2228 x j . A single low-weight obstacle blocks the right path of the x i variable loop and the left path of the clause loop. Another low-weight obstacle blocks the right path of the x j variable loop and the right path of the clause loop. If x i is true, then the left path of the clause loop can be taken without intersecting an additional obstacle. If x j is false, then the right path of the clause loop can be taken without intersecting an additional obstacle.",
        "name": "5",
        "region_bb": [
          325.44,
          257.76,
          552.24,
          462.96000000000004
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          318.96000000000004,
          482.40000000000003,
          558.0,
          567.36
        ],
        "page": 4,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 4: A clause gadget for x i \u2228 x j . A single low-weight obstacle blocks the left path of the x i variable loop and the left path of the clause loop. Another low-weight obstacle blocks the right path of the x j variable loop and the right path of the clause loop. If x i is true, then the left path of the clause loop can be taken without intersecting an additional obstacle. If x j is false, then the right path of the clause loop can be taken without intersecting an additional obstacle.",
        "name": "4",
        "region_bb": [
          59.760000000000005,
          237.6,
          286.56,
          475.92
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          53.28,
          496.08000000000004,
          292.32,
          581.0400000000001
        ],
        "page": 4,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: An instance of the minimum constraint removal problem. A path from the start (q s ) to the goal (q g ) that intersects the minimum number of obstacles (O 2 and O 5 ) is",
        "name": "1",
        "region_bb": [
          338.40000000000003,
          216.72,
          539.28,
          393.12
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          318.96000000000004,
          412.56,
          558.0,
          452.16
        ],
        "page": 1,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 6: A MAX2HORNSAT problem with four variables and five clauses represented as a minimum constraint re- moval problem. High-weight obstacles surround the solid black path between q s and q g . Medium-weight obstacles are represented by dotted black lines. Low-weight obstacles are represented by grey lines and grey squares.",
        "name": "6",
        "region_bb": [
          325.44,
          54.0,
          551.52,
          361.44
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          318.96000000000004,
          380.88,
          558.0,
          444.24
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3: A variable gadget for x i . Taking the left path sets x i to true, and taking the right path sets x i to false. Medium-weight obstacles (the dotted lines) ensure that the same assignment path is taken in both the upper and lower loops. The upper loop of the gadget appears before the pos- itive/negative clause gadgets, and the lower loop appears af- ter the positive/negative clause gadgets.",
        "name": "3",
        "region_bb": [
          325.44,
          398.88,
          552.24,
          574.5600000000001
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          318.96000000000004,
          595.44,
          558.0,
          669.6
        ],
        "page": 3,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6
    ]
  },
  "nips10_10": {
    "figures": [
      {
        "caption": "Table 1: Trade-off between quantities in our learning model and effectiveness of different criteria.",
        "name": "1",
        "region_bb": [
          115.2,
          82.08,
          496.8,
          128.88
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          144.72,
          502.56,
          164.16
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "icml14_7": {
    "figures": [
      {
        "caption": "Figure 1. Solving an ill-conditioned \u2113 1 -LS problem. AdapAPG1 starts with \u00b5 0 = L 0 /10, and AdapAPG2 uses \u00b5 0 = L 0 /100.",
        "name": "1",
        "region_bb": [
          51.120000000000005,
          68.4,
          537.12,
          321.12
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          55.440000000000005,
          332.64,
          501.12,
          342.0
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "aaai14_4": {
    "figures": [
      {
        "caption": "the or direct R ij , a particular Figure 2(B), where the graph at is directed Figure 3: of Illustrates the that notion of and a the smooth constraint (left of the end-points). In this scenario, is side). easy to constraint note that by Figure 2: R Shows general geometry a CRC there exists an instantiation X k such R linear. it Moreover, there are weights asso ik kj The right side illustrates the general pattern of the required pair of Preliminaries and Background and the pattern of the directions at the symmetry, T enforcing (L, R) required = T (R, pairs L). of Further, using the infea- property CSP is an assignment of values all the variables from their path-consistency are to also satisfied. Conceptually, which are interpreted as probabilities in directions for \u20180\u2019s in sible a CRC constraint (shaded indicate \u20181\u2019s). points. Shaded areas areas represent feasible of random walks stated above, are combinations n nodes the A CSP domains is defined by that a triplet , D, Ci, by where X = \u201ctightening\u201d respective such all the hX constraints are satisfied. algorithms work iteratively the binary con- \u27e9",
        "name": "2",
        "region_bb": [
          386.64,
          277.92,
          489.6,
          331.2
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          318.96000000000004,
          347.76,
          558.0,
          386.64
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 3: Varying Graph Density p 1 : 21 agents; domain size",
        "name": "3",
        "region_bb": [
          357.84000000000003,
          54.0,
          519.84,
          106.56
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          318.96000000000004,
          116.64,
          557.28,
          134.64000000000001
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 2: Varying Domain Size |D i |: 21 agents; 0.5 density.",
        "name": "2",
        "region_bb": [
          93.60000000000001,
          182.16,
          252.72,
          255.60000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          55.440000000000005,
          267.12,
          289.44,
          275.76
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 1: Varying Number of Agents |X |: 0.5 density; do-",
        "name": "1",
        "region_bb": [
          92.16,
          54.0,
          254.88,
          148.32
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.0,
          159.12,
          291.6,
          177.12
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Figure 1: Illustrates the idea of smoothness. The left side, of a smooth END ALGORITHM ing knowledge between agents in an exchangeable format The right side illustrates the general pattern o (a), is a binary smooth constraint. It satisfies the property could also be very expensive. Furthermore, in many appli- directions for \u20180\u2019s in a CRC constraint (shade that at each \u20180\u2019, there exist two directions, as indicated, such cation having for agents exchange information in about Figure 1: domains, Basic algorithm enforcing path-consistency a bi- that with respect to any \u20181\u2019, as encircled, moving along at nary constraint constraints network. \u03a0 is indicates the projection \u201cinferred\u201d highly undesirable for operation, purposes and of least one of these two directions decreases the manhattan \u25c3\u25b9 indicates the join operation (similar to that in database theory). security and privacy. T (R, L), not then T (R, + T (L, R) is O distance to it. The right side, (b), does satisfy the L) prop- In contrast, the randomized algorithm that works on Figure 3: Illustrates the notion of a smooth constraint (left side). the number of edges, and H(L, R) is th erty of smoothness. The required pair of directions does not smooth constraints follows a simple random walk strategy. ITHM: PATH-CONSISTENCY The right side illustrates the general pattern of the required pair of weights tween L and R, when the unit exist at the \u20180\u2019 marked with a cross. In some sense, this is It is very simple to implement and does not introduce any A binary constraint network \u27e8X , D, C\u27e9. Figure 2: Shows 3 scenarios in which because random the walks are directions for \u20181\u2019s \u20180\u2019s in per- a on CRC constraint (shaded areas indicate \u20181\u2019s). lie three or more sides of it, as shown preted as electrical resistance values (Doy newly inferred constraints formed. between In the original variables. T: A path-consistent network. an undirected graph ((A) and (B)), for Binary any 2 nodes encircled. smooth constraints are 2(B) the same as a CRC Figure shows particular case o This power of randomization is especially well-suited for L and R, T (R, L) + T (L, R) is related to constraints, the \u201cresistance\u201d between at until no constraint is changed: as proved in (Kumar 2005c). (a) therefore satis- in the graph generalization to distributed settings. In this paper, we ex- 2(A), in which the nodes them. In (C) (p \u2264 q at every node), T (R, L) is the less definitional than that in conditions (B) r k = 1, 2 . . . N : fies CRC stated T (R, L), then T (R, L) + T for (L, a R) is constraint O(mH(L, as the R)); m is a of simple polynomial-time linear fashion\u2014i.e. probabilities of because an increased \u201cattraction\u201d towards L at every node. For i, j = 1, 2 ploit . . . N : this intuition and present in the running text as well. the number of edges, and H(L, R) is the \u201cresistance\u201d be- randomized algorithm that solves CRC constraints in dis- or to the right from a particular",
        "name": "1",
        "region_bb": [
          321.84000000000003,
          56.160000000000004,
          553.6800000000001,
          115.2
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          318.96000000000004,
          131.04,
          558.0,
          270.72
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure three scenarios in which random walks are formed. In an undirected graph ((A) and (B)), for any 2 nodes performed. In an graph, as \u201cresistance\u201d in (a) and between (b), for L and R, T (R, L) + T undirected (L, R) is related to the any two nodes R, node), T (R, T L) is that related them. In (C) (p \u2264 L q and at every (R, + L) T is (L, less R) than in (B) to the \u201cresistance\u201d between them. In (c), p L \u2264 q at every because of an increased \u201cattraction\u201d towards at every node. node, and T (R, L) is less than that in (b) because of an increased \u201cattraction\u201d towards L at every node. CSP is an assignment of values to all the variables from their",
        "name": "3",
        "region_bb": [
          54.72,
          56.88,
          290.16,
          123.12
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.0,
          139.68,
          292.32,
          203.04000000000002
        ],
        "page": 3,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7
    ]
  },
  "nips10_8": {
    "figures": [
      {
        "caption": "Table 3: Comparison of classification accuracy (odd rows) and F-score (even rows) on Proteins",
        "name": "3",
        "region_bb": [
          110.16000000000001,
          267.12,
          498.96000000000004,
          470.88
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          115.2,
          246.96,
          493.92,
          256.32
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 2: Comparison of classification accuracy (odd rows) and F-score (even rows) on real data sets",
        "name": "2",
        "region_bb": [
          110.16000000000001,
          100.08,
          498.96000000000004,
          226.08
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          91.44,
          503.28000000000003,
          100.8
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 1: Comparison of classification accuracy (odd rows) and F-score (even rows) on UCI data sets",
        "name": "1",
        "region_bb": [
          108.0,
          496.08000000000004,
          501.12,
          633.6
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          107.28,
          475.92,
          503.28000000000003,
          485.28000000000003
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "nips11_7": {
    "figures": [
      {
        "caption": "Figure 2: Classification accuracy versus run- To further test the effectiveness of our active selec- ning time for the baseline, active classifica-",
        "name": "2",
        "region_bb": [
          331.92,
          289.44,
          498.24,
          426.96000000000004
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          325.44,
          440.64000000000004,
          503.28000000000003,
          471.6
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: (Best viewed magnified and in colors) Performance comparisons on UCI datasets. From the left to right are the results on satimage, pendigits, vowel and letter (in log-scale) datasets. Note that the error bars for pendigits and letter datasets are very small (around 0.5% on average).",
        "name": "1",
        "region_bb": [
          108.72,
          82.8,
          503.28000000000003,
          164.16
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.72,
          177.84,
          504.0,
          208.8
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 1: Detailed performance comparisons on Scene15 dataset with various feature types. For our methods, we show the speedup factors with respective to using all the features in a static way.",
        "name": "1",
        "region_bb": [
          107.28,
          531.36,
          511.92,
          620.64
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          629.28,
          504.0,
          649.44
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "nips10_7": {
    "figures": [
      {
        "caption": "Figure 2: The experiment results on real data. (a) left: the timelines of the top 20 flows; right: illustration of first two flows. (Illustrations of larger sizes are in the supplement.) (b) left: the timelines of the top 10 topics; right: the two leading keywords for these topics. (A list with more keywords is in the supplement.)",
        "name": "2",
        "region_bb": [
          114.48,
          82.08,
          491.76,
          188.64000000000001
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          204.48000000000002,
          504.0,
          233.28
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: The simulation results: (a) compares the performance between D-DPMM and D-FMM with differing numbers of components. The upper graph shows the median of distance between the resulting clusters and the ground truth at each phase. The lower graph shows the actual numbers of clusters. (b) shows the performance of D-DPMM with different values of acceptance probability, under different data sizes. (c) shows the performance of D-DPMM with different values of diffusion variance, under different data sizes.",
        "name": "1",
        "region_bb": [
          115.92,
          82.8,
          494.64000000000004,
          188.64000000000001
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          204.48000000000002,
          504.0,
          252.0
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "aaai12_7": {
    "figures": [
      {
        "caption": "Figure 1: Proving framework for GS-type algorithm convergence",
        "name": "1",
        "region_bb": [
          359.28000000000003,
          163.44,
          518.4,
          300.96000000000004
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          320.40000000000003,
          314.64,
          556.5600000000001,
          323.28000000000003
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 1: GS Algorithm and Zangwill\u2019s Theorem\u2019s similarities",
        "name": "1",
        "region_bb": [
          74.16,
          54.0,
          272.16,
          105.12
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          60.480000000000004,
          115.92,
          284.40000000000003,
          124.56
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2
    ]
  },
  "nips11_4": {
    "figures": [
      {
        "caption": "Figure 1: Value functions obtained by the evaluated methods. The black lines show the reward function. The blue lines show the value function computed from the trajectories of 50,000 uniformly sampled points. The LSTD, KTD, DSDP, and NPDP methods evaluated the policy using only 500 points. The presentation was divided into two plots for improved clarity",
        "name": "1",
        "region_bb": [
          109.44,
          82.8,
          502.56,
          232.56
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.72,
          246.96,
          504.0,
          300.24
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 1: Each row corresponds to one of the four tested algorithms for policy evaluation. The columns indicate the performance of the approaches during the experiment. The per- formance indexes include the mean squared error evaluated uniformly over the zero to one range, the mean squared error evaluated at the 500 sampled points, and the maximum error. The results are averaged over 500 trials. The standard errors of the means are also given.",
        "name": "1",
        "region_bb": [
          160.56,
          609.84,
          451.44,
          666.72
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.72,
          676.8000000000001,
          503.28000000000003,
          730.08
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "nips09_7": {
    "figures": [
      {
        "caption": "Figure 2: Comparison of various selective selection schemes (best viewed in color).",
        "name": "2",
        "region_bb": [
          94.32000000000001,
          72.0,
          491.04,
          180.0
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          139.68,
          190.08,
          471.6,
          198.72
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: Illustration of induction-time and diagnosis-time active information acquisition. Induction-time active learning focuses on acquiring information for the pool of data used to train a diagnostic model; diagnosis-time information acquisition focuses on the next best observations to acquire from the test case at hand.",
        "name": "1",
        "region_bb": [
          166.32,
          86.4,
          445.68,
          221.04000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.72,
          246.24,
          503.28000000000003,
          287.28000000000003
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3: Statistics of different information selected in active learning.",
        "name": "3",
        "region_bb": [
          98.64,
          69.84,
          509.76,
          205.92000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          166.32,
          216.0,
          444.96000000000004,
          224.64000000000001
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "nips12_8": {
    "figures": [
      {
        "caption": "{C(r) : r \u2208 [0, \u221e)} and return C. Figure 1: Robust Single Linkage (RSL) Algorithm",
        "name": "1",
        "region_bb": [
          108.0,
          82.08,
          498.24,
          203.04000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          203.04000000000002,
          188.64000000000001,
          403.2,
          198.0
        ],
        "page": 4,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "icml11_9": {
    "figures": [
      {
        "caption": "Figure 1. Root Mean Squared Error on the test set as a function of the rank. The horizontal line corresponds to the minimal error achieved by JS. Left: MovieLens100k, Middle: MovieLens1M, Right: MovieLens10M.",
        "name": "1",
        "region_bb": [
          67.68,
          68.4,
          528.48,
          185.04000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          56.160000000000004,
          206.64000000000001,
          540.72,
          226.08
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8
    ]
  },
  "nips11_5": {
    "figures": [
      {
        "caption": "Figure 2: Performance landscapes and estimated gradient fields for the examples in Figure 1.",
        "name": "2",
        "region_bb": [
          110.88000000000001,
          213.12,
          500.40000000000003,
          297.36
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          120.96000000000001,
          312.48,
          489.6,
          321.12
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: Oscillatory examples. Boxes marked with y k denote observations (aggregated states). Circles marked with w k illustrate receptive fields of the basis functions. Only non-zero rewards are shown. Start states: s 1 (1.1), s 0 (1.2), and s 1 (1.3). Arrows leading out indicate termination.",
        "name": "1",
        "region_bb": [
          165.6,
          163.44,
          448.56,
          269.28000000000003
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.72,
          286.56,
          502.56,
          316.8
        ],
        "page": 4,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3: Empirical results for the Tetris problem. See the text for details.",
        "name": "3",
        "region_bb": [
          115.2,
          82.8,
          493.92,
          311.76
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          159.84,
          329.04,
          451.44,
          337.68
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "icml10_3": {
    "figures": [
      {
        "caption": "Figure 2. The two-moon problem of 1200 2D points. (a) 100 anchor points by k-means clus- tering (m = 100); (b) 10NN graph built on original points; (c) the proposed AnchorGraph with s = 2 built on original points.",
        "name": "2",
        "region_bb": [
          182.88,
          87.12,
          542.88,
          178.56
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          172.8,
          202.32,
          537.12,
          233.28
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 4. Classification error rates (%) on Extended MNIST (630,000 samples) with l = 100 labeled samples. m = 500 for two versions of AGR. The running time of k-means clustering is 195.16 seconds.",
        "name": "4",
        "region_bb": [
          63.36,
          124.56,
          280.8,
          196.56
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          55.440000000000005,
          76.32000000000001,
          290.16,
          118.08
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 2. Classification error rates (%) on USPS-Train (7,291 samples) with l = 100 labeled samples. m = 1000 for four versions of AGR. The running time of k-means clustering is 7.65 seconds.",
        "name": "2",
        "region_bb": [
          54.72,
          124.56,
          301.68,
          217.44
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.72,
          76.32000000000001,
          288.72,
          118.08
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 1. Time complexity analysis of the proposed scalable SSL approach. n is the data size, m is # of anchor points, s",
        "name": "1",
        "region_bb": [
          71.28,
          97.2,
          525.6,
          119.52000000000001
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          55.440000000000005,
          76.32000000000001,
          540.72,
          95.76
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Figure 1. A bipartite graph representation of data points v 1 , \u00b7 \u00b7 \u00b7 , v 6 and anchor points u 1 , u 2 . Z ik captures the data-to-anchor relation- ship (Z 21 + Z 22 = 1).",
        "name": "1",
        "region_bb": [
          59.04,
          69.12,
          127.44,
          170.64000000000001
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          59.04,
          176.4,
          156.96,
          250.56
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3. Average classification error rates vs. numbers of anchor points.",
        "name": "3",
        "region_bb": [
          79.92,
          222.48000000000002,
          258.48,
          370.08000000000004
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.72,
          377.28000000000003,
          290.16,
          396.0
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 3. Classification error rates (%) on MNIST (70,000 samples). m = 1000 for two versions of AGR.",
        "name": "3",
        "region_bb": [
          321.84000000000003,
          102.96000000000001,
          526.32,
          195.12
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          307.44,
          76.32000000000001,
          540.72,
          96.48
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8
    ]
  },
  "aaai12_3": {
    "figures": [
      {
        "caption": "Figure 1: The Wumpus domain, plan tree, and linearized plan",
        "name": "1",
        "region_bb": [
          318.96000000000004,
          54.0,
          560.88,
          390.24
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          318.96000000000004,
          406.8,
          558.0,
          426.24
        ],
        "page": 3,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 2: Wumpus domains with deadends. TF denotes that the CLG translation did not complete in 30 minutes. Name Wumpus 4 Wumpus 8 Wumpus 16 Wumpus 20 Wumpus 24 MPSR Time (secs) #Actions 1.3 (0.01) 17.5 (0.1) 16.9 (1.5) 27.5 (1.1) 93.5 (4.5) 38.3 (1.2) 216.8 (6.7) 48.28 (1.2) 285.5 (5.5) 71.5 (0.7) CLG Time (secs) #Actions",
        "name": "2",
        "region_bb": [
          323.28000000000003,
          306.0,
          552.96,
          378.0
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          318.96000000000004,
          286.56,
          557.28,
          306.72
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Table 1: Comparing MPSR to CLG (execution mode) and SDR* \u2014 the best SDR variation in each domain. For domains with conditional actions (localize) CLG execution cannot be simulated. We denote TF when the CLG translation failed, CSU when CLG cannot run a simulation with a uniform distribution, and PF where the CLG planner failed, either due to too many predicates or due to timeout. Domains were FF was unable to solve the translation are denoted by FFF. Values in parentheses denote standard error. Maximum time allowed was 30 minutes. Best time and plan length for each domain are bolded.",
        "name": "1",
        "region_bb": [
          74.88000000000001,
          111.60000000000001,
          535.6800000000001,
          426.24
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.0,
          64.08,
          558.0,
          111.60000000000001
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7
    ]
  },
  "aaai14_6": {
    "figures": [
      {
        "caption": "Figure 1: Platformer level showing a playtrace using a gen- erated mechanic set. Arrows indicate generated mechanics, dotted arrows indicate gravity.",
        "name": "1",
        "region_bb": [
          342.72,
          54.0,
          534.24,
          149.76000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          318.96000000000004,
          163.44,
          557.28,
          193.68
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7
    ]
  },
  "icml12_9": {
    "figures": [
      {
        "caption": "Figure 2. Final error of logistic regression at time T versus mixing rate for the mean (top) and covariance (bottom) estimates after 100 (left) and 3000 (right) seconds of computation. See main text for detailed explanation.",
        "name": "2",
        "region_bb": [
          48.96,
          67.68,
          287.28000000000003,
          247.68
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          55.440000000000005,
          262.8,
          288.72,
          304.56
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 5. Final error for DRBM at time T versus mixing rate for the mean (left) and covariance (right) estimates after 6790 sec- onds of computation on a subset of KDD99.",
        "name": "5",
        "region_bb": [
          302.40000000000003,
          70.56,
          529.9200000000001,
          166.32
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          306.72,
          180.72,
          540.72,
          210.96
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 1. Final test error rate on the KDD99 dataset.",
        "name": "1",
        "region_bb": [
          333.36,
          223.92000000000002,
          515.52,
          249.84
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          332.64,
          264.96000000000004,
          515.52,
          270.72
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Figure 4. 2-d marginal posterior distributions of DRBM. Grey colors correspond to samples from SGFS/SGLD. Thick red solid lines correspond to iso-probability contours at two standard de- viations away from the mean computed from HMC samples. Thin red solid lines correspond to HMC results based on sub- sets of the samples. The thick blue dashed lines correspond to SGFS-f (top) and SGLD (bottom) runs. Plots on the left repre- sent the 2-d marginals with the smallest difference between HMC and SGFS/SGLD while the plots on the right represent the 2-d marginals with the largest difference.",
        "name": "4",
        "region_bb": [
          61.92,
          72.0,
          280.8,
          248.4
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.72,
          262.8,
          288.72,
          370.08000000000004
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1. 2-d marginal posterior distributions for logistic regres- sion. Grey colors correspond to samples from SGFS. Red solid and blue dotted ellipses represent iso-probability contours at two standard deviations away from the mean computed from HMC and SGFS, respectively. Top plots are the results for SGFS-f and bottom plots represent SGFS-d. Plots on the left represent the 2-d marginals with the smallest difference between HMC and SGFS while the plots on the right represent the 2-d marginals with the largest difference. Value for \u03b1 is 0 meaning that no additional",
        "name": "1",
        "region_bb": [
          306.0,
          68.4,
          534.96,
          254.88
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          306.72,
          270.0,
          540.72,
          375.12
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3. Test-set classification error of NNs trained with SGFS- f, SGFS-d, SGLD and SGD on the HHP dataset (left) and the MNIST dataset (right)",
        "name": "3",
        "region_bb": [
          301.68,
          67.68,
          541.44,
          162.0
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          306.72,
          176.4,
          540.72,
          206.64000000000001
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8
    ]
  },
  "nips11_8": {
    "figures": [
      {
        "caption": "Figure 2: In (a) we show two variable nodes, V 1 and V 2 , that share a factor of degree two P 1 . In (b), V 1 heirs the connections of V 2 (solid lines). In (c), we show the graph once P 1 and V 2 have been removed. If P 1 is parity one, the parities of P 2 , P 3 are reversed.",
        "name": "2",
        "region_bb": [
          111.60000000000001,
          230.4,
          500.40000000000003,
          301.68
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          107.28,
          309.6,
          503.28000000000003,
          339.84000000000003
        ],
        "page": 4,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 5: For the regular (3, 6) ensemble and   BP \u2248 0.4294, in (a) we plot the expected evolution of",
        "name": "5",
        "region_bb": [
          118.08,
          416.88,
          488.16,
          588.96
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          107.28,
          601.9200000000001,
          504.0,
          622.8000000000001
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 4: In (a), for a regular (3, 6) code with n = 2 17 and   = 0.415, we compare the solution of the system of differential equations for R 1 (\u03c4 ) = r 1 (\u03c4 )E (C) and R 2 (\u03c4 ) = r 2 (\u03c4 )E ( ) (thick solid lines) with 30 simulated decoding trajectories (thin dashed lines). In (b), we reproduce the same experiment for the irregular LDPC in (3) and (4) for   = 0.47.",
        "name": "4",
        "region_bb": [
          117.36,
          165.6,
          493.92,
          339.12
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          352.08000000000004,
          504.0,
          395.28000000000003
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: Example of the PD algorithm for LDPC channel decoding in the erasure channel.",
        "name": "1",
        "region_bb": [
          110.16000000000001,
          259.92,
          501.12,
          344.16
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          124.56,
          352.08000000000004,
          486.0,
          360.72
        ],
        "page": 3,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 7: In (a), we plot the solution for r 1 (\u03c4 ) with respect to e(t) for a (3, 6) regular code at   =   BP =   TEP . In (b), we compare the TEP performance for the regular (3, 6) ensemble (solid lines) with the approximation in (8) (dashed lines), using the approximation \u03b1 TEP \u2248 \u03b1 BP = 0.56036, \u03b4(\u03c4 \u2217 ) \u2248 0.0526 and \u03b3 TEP \u2248 0.3198. We have plot the results for code lengths of n = 2 9 (\u25e6), n = 2 10 ( ), n = 2 11 (\u00d7) and n = 2 12 (B).",
        "name": "7",
        "region_bb": [
          117.36,
          270.72,
          491.04,
          437.04
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.72,
          450.0,
          504.0,
          503.28000000000003
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 6: TEP (solid line) and BP (dashed line) decoding performance for a regular LDPC (3,6) code in (a), and the irregular LDPC in (3) and (4) in (b), with code lengths n = 2 9 (\u25e6), n = 2 10 ( ), n = 2 11 (\u00d7) and 2 12 (B).",
        "name": "6",
        "region_bb": [
          108.72,
          130.32,
          497.52000000000004,
          299.52000000000004
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.0,
          307.44,
          503.28000000000003,
          338.40000000000003
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3: In (a), the variables V 1 and V 2 are connected to a degree two factor, P 3 , and they also share a factor of degree three, P 4 . In (b) we show the graph once the TEP has removed P 3 and V 2 .",
        "name": "3",
        "region_bb": [
          194.4,
          502.56,
          417.6,
          637.2
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          108.72,
          645.12,
          503.28000000000003,
          664.5600000000001
        ],
        "page": 4,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "icml11_10": {
    "figures": [
      {
        "caption": "Figure 2. Noisy interactive set cover as a circuit.",
        "name": "2",
        "region_bb": [
          336.24,
          66.96000000000001,
          512.64,
          150.48000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          326.16,
          181.44,
          521.28,
          190.08
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1. Related problems",
        "name": "1",
        "region_bb": [
          331.92,
          66.96000000000001,
          516.96,
          166.32
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          369.36,
          199.44,
          479.52000000000004,
          208.08
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 1. Average survey responses. 5 is Strongly Agree, 1",
        "name": "1",
        "region_bb": [
          63.36,
          102.24000000000001,
          282.24,
          153.36
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          56.160000000000004,
          76.32000000000001,
          287.28000000000003,
          95.76
        ],
        "page": 8,
        "dpi": 72.0,
        "figure_type": "Table"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8
    ]
  },
  "icml14_6": {
    "figures": [
      {
        "caption": "Figure 2: The densities of three 2D Poisson MRFs that show possible dependency structures between two words. Negative dependencies (left) suggest that two words rarely co-occur whereas positive dependencies (right) suggest that two words often co-occur.",
        "name": "2",
        "region_bb": [
          314.64,
          316.08,
          536.4,
          397.44
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          307.44,
          403.92,
          541.44,
          460.08000000000004
        ],
        "page": 2,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1: A Poisson MRF can provide interesting insights into a text corpus including multiple word senses (hubs of graph) and semantic concepts (coherent subgraphs).",
        "name": "1",
        "region_bb": [
          308.16,
          511.20000000000005,
          540.72,
          664.5600000000001
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          307.44,
          676.08,
          541.44,
          708.48
        ],
        "page": 1,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 4: These APM topic visualizations illustrate that PMRFs are much more intuitive than multinomials (as in LDA/PLSA), which can only be represented as a list of words. Word size signifies relative word frequency and edge width signifies the strength of word dependency (only positive dependencies shown).",
        "name": "4",
        "region_bb": [
          75.60000000000001,
          74.88000000000001,
          519.84,
          241.20000000000002
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          55.440000000000005,
          254.16,
          540.72,
          286.56
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3: (Left) In mixtures, documents are drawn from exactly one component distribution. (Right) In admixtures, documents are drawn from a distribution whose parameters are a convex combination of component parameters.",
        "name": "3",
        "region_bb": [
          311.76,
          69.12,
          540.0,
          154.08
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          307.44,
          159.12,
          540.72,
          203.76000000000002
        ],
        "page": 3,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 5: APM seems to outperform LDA in a simple 200- word experiment when the number of topics is small but is only comparable to LDA for a larger number of topics. (Median score is shown to reduce the effect of outliers.)",
        "name": "5",
        "region_bb": [
          308.16,
          337.68,
          541.44,
          451.44
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          307.44,
          462.96000000000004,
          541.44,
          506.88
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9
    ]
  },
  "icml10_8": {
    "figures": [
      {
        "caption": "Figure 2 The OSFS algorithm",
        "name": "2",
        "region_bb": [
          305.28000000000003,
          228.96,
          540.0,
          365.04
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          365.76,
          373.68,
          479.52000000000004,
          382.32
        ],
        "page": 3,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 5 The compactness and predictive accuracy of 4 algorithms (alpha=0.05)",
        "name": "5",
        "region_bb": [
          305.28000000000003,
          400.32,
          538.5600000000001,
          684.0
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          305.28000000000003,
          694.8000000000001,
          539.28,
          714.96
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 8 The compactness and prediction accuracy (%) of four algorithms (alpha=0.01)",
        "name": "8",
        "region_bb": [
          54.0,
          101.52000000000001,
          292.32,
          460.8
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          53.28,
          465.84000000000003,
          287.28000000000003,
          486.0
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 2. Summary of challenge datasets",
        "name": "2",
        "region_bb": [
          58.32,
          467.28000000000003,
          288.0,
          697.6800000000001
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.72,
          449.28000000000003,
          197.28,
          457.92
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Figure 9 Normalized time results",
        "name": "9",
        "region_bb": [
          305.28000000000003,
          279.36,
          533.52,
          381.6
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          306.72,
          386.64,
          424.08000000000004,
          395.28000000000003
        ],
        "page": 7,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 4 The compactness and predictive accuracy of 4 algorithms (alpha=0.01)",
        "name": "4",
        "region_bb": [
          306.0,
          73.44,
          539.28,
          354.24
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          306.0,
          365.04,
          539.28,
          384.48
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 1 A framework for streaming feature selection",
        "name": "1",
        "region_bb": [
          54.72,
          476.64000000000004,
          287.28000000000003,
          625.6800000000001
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.72,
          632.88,
          279.36,
          641.52
        ],
        "page": 3,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Table 1. Summary of UCI benchmark data sets",
        "name": "1",
        "region_bb": [
          58.32,
          211.68,
          288.0,
          385.92
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          54.72,
          193.68,
          222.48000000000002,
          202.32
        ],
        "page": 5,
        "dpi": 72.0,
        "figure_type": "Table"
      },
      {
        "caption": "Figure 7 Fast-OSFS performance of with different alpha values",
        "name": "7",
        "region_bb": [
          306.0,
          403.2,
          534.96,
          601.9200000000001
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          306.72,
          613.44,
          534.96,
          621.36
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 6 OSFS performance with different alpha values",
        "name": "6",
        "region_bb": [
          306.72,
          177.84,
          536.4,
          377.28000000000003
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          306.72,
          388.08000000000004,
          505.44,
          396.72
        ],
        "page": 6,
        "dpi": 72.0,
        "figure_type": "Figure"
      },
      {
        "caption": "Figure 3 The Fast-OSFS algorithm",
        "name": "3",
        "region_bb": [
          305.28000000000003,
          174.24,
          540.0,
          324.0
        ],
        "page_height": 792,
        "page_width": 612,
        "caption_bb": [
          360.72,
          332.64,
          484.56,
          341.28000000000003
        ],
        "page": 4,
        "dpi": 72.0,
        "figure_type": "Figure"
      }
    ],
    "pages_annotated": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8
    ]
  }
}